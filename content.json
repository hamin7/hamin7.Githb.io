{"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Reactive Design Pattern Actors Cluster Cassandra Event Sourcing &amp; CQRS Persistence(Event Sourcing) Lagom core concepts Docker &amp; Kubernetes Cassandra, Zookeeper, Kafka on Docker Akka, Lagom on Kubernetes Programming in Scala Webflux","link":"/2020/03/21/hello-world/"},{"title":"권한 관리 - chmod란","text":"유닉스 및 유닉스 유사 운영 체제에서 chmod는 파일 시스템 개체(파일 및 디렉토리)의 액세스 권한을 변경하는 데 사용되는 명령 및 system call이다.특수 모드 플래그를 변경할 때도 사용된다. 요청은 umask로 필터링된다. 이름은 Change mode의 약칭이다.","link":"/2020/04/08/chmod-types/"},{"title":"머신러닝을 위한 데이터 다루기","text":"머신러닝(machine learning)이란 규칙을 일일이 프로그래밍하지 않아도 자동으로 데이터에서 규칙을 학습하는 알고리즘을 연구하는 분야이다.최근 머신러닝의 발전은 통계나 수학 이론보다 경험을 바탕으로 발전하는 경우도 많다. 컴퓨터 과학 분야가 이런 발전을 주도하고 있다. 컴퓨터 과학 분야의 대표적인 머신러닝 라이브러리는 사이킷런(scikit-learn)이다. 사이킷런 라이브러리는 파이썬 API를 사용하는데 파이썬 언어는 배우기 쉽고 컴파일하지 않아도 되기 때문에 사용하기 편리하다.연구자들은 새로운 알고리즘을 끊임없이 개발하여 발표한다. 많은 사람이 이를 검증하고 사용해 본 다음 장단점을 파악하게 된다. 어느정도 시간이 지나서 이런 알고리즘이 유익하다고 증명되어 널리 사용하게 되면 사이킷런 라이브러리 개발자들이 이 알고리즘을 라이브러리에 추가한다. 그러므로 머신러닝 라이브러리에 포함된 알고리즘들은 안정적이며 성능이 검증되어 있다. 머신러닝에 쓰이는 용어 특성 : 데이터를 표현하는 하나의 성질ex) 생선 데이터 각각의 길이와 무게 훈련 : 머신러닝 알고리즘이 데이터에서 규칙을 찾는 과정, 사이킷런에서는 fit() 메서드가 하는 역할 K-최근접 이웃 알고리즘 : 가장 간단한 머신러닝 알고리즘 중 하나. 사실 어떤 규칙을 찾기보다는 전체 데이터를 메모리에 가지고 있는 것이 전부 모델 : 머신러닝 프로그램에서 알고리즘이 구현된 객체를 모델이라고 부름. 종종 알고리즘 자체를 모델이라고 부르기도 함 정확도 : 정확한 답을 몇 개 맞췄는지를 백분율로 나타낸 값. 사이킷런에서는 0~1 사이의 값으로 출력 됨. matplotlib scatter()는 산점도를 그리는 맷플롯십 함수이다. 처음 2개의 매개변수로 x축과 y축 값을 전달한다. 이 값은 파이썬 list 또는 numpy 배열입니다. c 매개변수로 색깔을 지정합니다. RGB를 16진수로 지저하거나 색깔코드 ‘b’, ‘g’, ‘r’, ‘c’, ‘m’, ‘y’, ‘k’, ‘w’ 중 하나를 지정합니다. 지저하지 않을 경우 10개의 기본 색깔을 사용해 그래프를 그립니다. maker 매개변수로 마커 스타일을 지정합니다. maker의 기본값은 o(circle, 원)입니다. scikit-learn KNeighborsClassifier()는 k-최근접 이웃 분류 모델을 만드는 사이킷런 클래스이다. n_neighbors 매개변수로 이웃의 개수를 지정한다. 기본값은 5이다. p 매개변수로 거리 재는 방법을 지정한다.(기본값 : 2) 1일 경우 : 맨해튼 거리 2일 경우 : 유클리디안 거리 n_jobs 매개변수로 사용할 CPU 코어를 지정한다. -1로 설정하면 모든 CPU 코어를 사용한다. 이웃 간의 거리 계산 속도를 높일 수 있지만 fit() 메서드에는 영향이 없다. 기본값은 1 fit() : 사이킷런 모델을 훈련할 때 사용하는 메서드이다. 처음 두 매개변수로 훈련에 사용할 특성과 정답 데이터를 전달한다. predict() : 사이킷런 모델을 훈련하고 예측할 때 사용하는 메서드이다. 특성 데이터 하나만 매개변수로 받는다. score() : 훈련된 사이킷런 모델의 성능을 측정한다. 처음 두 매개변수로 특성과 정답 데이터를 전달한다. 이 메서드는 먼저 predict() 메서드로 예측을 수행한 다음 분류 모델일 경우 정답과 비교하여 올바르게 예측한 개수의 비율을 반환한다. 지도학습과 비지도학습지도학습 (supervised learning) 지도학습에서는 데이터와 정답을 입력(input)과 타깃(target)이라고 하고, 이 둘을 합쳐 훈련 데이터(training data)라고 부른다. 입력으로 사용된 길이와 무게를 특성(feature)이라고 한다. 훈련세트와 테스트세트 훈련 세트 : 모델을 훈련할 때 사용하는 데이터. 보통 훈련 세트가 크면 클수록 좋다. 따라서 테스트 세트를 제외한 모든 데이터를 사용한다. 테스트 세트 : 전체 데이터에서 20~30%를 테스트 세트로 사용하는 경우가 많다. 전체 데이터가 아주 크다면 1%만 덜어내도 충분할 수 있다. 샘플링 편향(sampling bias) 훈련 세트와 테스트 세트에 샘플이 골고루 섞여 있지 않은 경우 넘파이 (numpy) numpy는 파이썬의 대표적인 배열 라이브러리이다. 파이썬의 리스트로 2차원 리스트를 표현할 수 있지만 고차원 리스트를 표현하려면 번거롭다. 넘파이는 고차원의 배열을 손쉽게 만들고 조작할 수 있는 간편한 도구를 많이 제공한다. seed() : 넘파이에서 난수를 생성하기 위한 정수 초깃값을 지정한다. 초깃값이 같으면 동일한 난수를 뽑을 수 있다. 따라서 랜덤 함수의 결과를 동일하게 재현하고 싶을 때 사용한다. arrange() : 일정한 간격의 정수 또는 실수 배열을 만든다. 기본 간격은 1이다. 매개변수가 하나이면 종료 숫자를 의미한다. 0에서 종료 숫자까지 배열을 만든다. 종료 숫자는 배열에 포함되지 않는다. 12&gt;&gt;&gt; print(np.arange(3))[0 1 2] 매개변수가 2개이면 시작 숫자, 종료 숫자를 의미한다. 12&gt;&gt;&gt; print(np.arange(1, 3))[1 2] 매개변수가 3개면 마지막 매개변수가 간격을 나타낸다. 12&gt;&gt;&gt; print(np.arange(1, 3, 0.2))[1. 1.2 1.4 1.6 1.8 2. 2.2 2.4 2.6 2.8] shuffle() : 주어진 배열을 랜덤하게 섞는다. 다차원 배열일 경우 첫 번째 축(행)에 대하여만 섞는다.123456&gt;&gt;&gt; arr = np.array([[1, 2], [3, 4], [5, 6]])&gt;&gt;&gt; np.random.shuffle(arr)&gt;&gt;&gt; print(arr)[[1 2] [5 6] [3 4]] 파이썬 리스트를 넘파이 배열로 바꾸는것은 쉽다. 넘파이 array() 함수에 파이썬 리스트를 전달하면 끝이다. 1input_arr = np.array(python_list) 넘파이는 슬라이싱 외에 배열 인덱싱(array indexing)이란 기능을 제공한다. 배열 인덱싱은 1개의 인덱스가 아닌 여러개의 인덱스로 한 번에 여러 개의 원소를 선택할 수 있다.예를 들면 아래처럼 input_arr에서 두 번째와 네 번째 샘플을 선택하여 출력 가능하다. 123print(input_arr[[1,3]])&gt;&gt; [[ 26.3 290. ] [ 29. 363. ]] 비지도학습 (unsupervised learning)타깃 데이터가 없다. 따라서 무엇을 예측하는것이 아니라 입력 데이터에서 어떤 특징을 찾는 데 주로 활용한다. 데이터 전처리데이터를 표현하는 기준이 다르면 알고리즘이 올바르게 예측할 수 없다. (두 특성의 스케일이 다른 경우 등)알고리즘이 거리기반일 때 특히 그렇다. 여기에는 k-최근접 이웃도 포함된다. 이런 알고리듬들은 샘플 간의 거리에 영향을 많이 받으므로 제대로 사용하려면 특성값을 일정한 기준으로 맞춰 주어야 한다. 이런 작업을 데이터 전처리(data preprocessing)라고 부른다. 표준점수데이터 전처리 방법 중 하는 표준점수이다(혹은 z 점수라고도 부른다). 표준점수는 각 특성값이 0에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다. 이를 통해 실제 특성값의 크기와 상관없이 동일한 조건으로 비교할 수 있다. numpy의 std() 함수를 이용하여 표준점수를 계산할 수 있다.12mean = np.mean(train_input, axis=0)std = np.std(train_input, axis=0) 브로드캐스팅크기가 다른 넘파이 배열에서 자동으로 사칙 연산을 모든 행이나 열로 확장하여 수행하는 기능이다. scikit-learntrain_test_split()훈련 데이터를 훈련 세트와 테스트 세트로 나누는 함수다. 여러 개의 배열을 전달할 수 있다. 테스트 세트로 나눌 비율은 test_size 매개변수에서 지정할 수 있으며 기본값은 0.25(25%)이다.shuffle 매개변수로 훈련 세트와 테스트 세트로 나누기 전에 무작위로 섞을지 여부를 결정할 수 있다. 기본값은 true이다. stratify 매개변수에 클래스 레이블이 담긴 배열(일반적으로 타깃 데이터)을 전달하면 클래스 비율에 맞게 훈련 세트와 테스트 세트를 나눈다. kneighbors()k-최근접 이웃 객체의 메서드이다. 이 메서드는 입력한 데이터에 가장 가까운 이웃을 찾아 거리와 이웃 샘플의 인덱스를 반환한다. 기본적으로 이웃의 개수는 KNeighborClassifier 클래스의 객체를 생성할 때 지정한 개수를 사용한다. 하지만 n_neighbors 매개변수에서 다르게 지정할 수도 있다. return_distance 매개변수를 False로 지정하면 이웃 샘플의 인덱스만 반환하고 거리는 반환하지 않는다. 이 매개변수의 기본값은 True이다. 튜플이란? 파이썬 튜플은 리스트와 매우 비슷하다. 리스트처럼 원소에 순서가 있지만 한 번 만들어진 튜플은 수정할 수 없다. 튜플을 사용하면 함수로 전달한 값이 바뀌지 않는다는 것을 믿을 수 있기 때문에 매개변수 값으로 많이 사용된다. column_stack : 전달받은 리스트를 일렬로 세운 다음 차례대로 나란히 연결. 1234&gt;&gt;&gt; np.column_stack(([1,2,3],[4,5,6]))array([[1, 4], [2, 5], [3, 6]])","link":"/2024/03/25/machineLearning/"},{"title":"RAG란 무엇인가?","text":"RAG는 “Retrieve, Generate, and Rank”의 약자로, 주로 자연어 처리(NLP)와 관련된 작업에서 사용되는 기술방법론이다. 이 접근 방식은 정보검색(IR: Information Retrieval)과 생성적 모델(Generative Models)을 결합하여, 복잡한 질문에 대해 더 정확하고 관련성 높은 답변을 생성하는데 사용된다. RAG 모델은 특히 대규모 텍스트 데이터셋에서 정확한 정보를 검색하고, 이를 기반으로 새로운 텍스트를 생성하여 사용자의 질문에 답변하는 데 효과적이다. 작동방식RAG 모델의 작동 방식은 크게 세 단계로 나눌 수 있다. 검색(Retrieve) : 사용자의 질문이 주어지면, 모델은 대규모 데이터셋에서 질문과 관련된 정보나 문서를 검색한다. 이 과정에서는 주로 효율적인 검색 알고리즘이나 기술이 사용된다. 생성(Generate) : 검색된 정보를 기반으로, 모델은 관련성 높은 답변을 생성한다. 이 단계에서는 생성적 딥러닝 모델, 특히 변형자(Transformer) 기반의 언어 모델이 사용될 수 있다. 순위 매기기(Rank) : 모델이 생성한 여러 답변 중에서 최종 사용자에게 제시될 가장 적합한 답변을 선택하기 위해, 답변들의 순위를 매긴다. 순위 매기기는 답변의 정확성, 관련성, 유용성 등 여러 기준을 고려하여 수행된다. 응용분야RAG 모델은 다양한 NLP 작업에 활용될 수 있으며, 특히 정보가 풍부한 오픈 도메인 질의응답(Open-Domain Question Answering), 자연어 이해(Natural Language Understanding), 챗봇(Chatbots) 등의 분야에서 유용하다. 장점 정확성과 관련성 향상 : RAG 모델은 관련 문서를 검색하여 정보를 기반으로 답변을 생성하기 때문에, 답변의 정확성과 관련성이 향상될 수 있다. 유연성 : 다양한 종류의 데이터와 질문에 적용될 수 있는 높은 유연성을 가진다. 지식 기반 학습 : RAG 모델은 대규모 데이터셋에서 검색한 정보를 활용하여 학습히기 때문에, 지식 기반 학습에 강점을 보인다. 한계 리소스 요구량 : 대규모 데이터셋에서 효율적인 검색과 순위 매기기를 수행하려면 상당한 계산 리소스가 필요할 수 있다. 검색 품질 : 최종 답변의 품질은 검색 단계에서 검색된 문서의 품질에 크게 의존한다. 검색 알고리즘의 성능이 중요하다. RAG는 AI와 NLP 분야에서 중요한 연구주제이며, 지속적인 기술 발전으로 인해 응용 가능성이 확대되고 있다.","link":"/2024/03/26/WhatIsRAG/"},{"title":"선형회귀 알고리즘","text":"지도 학습 알고리즘은 크게 분류와 회귀(regression)으로 나뉜다. 분류는 말 그대로 샘플을 몇 개의 클래스 중 하나로 분류하는 문제이다. 회귀는 클래스 중 하나로 분류하는 것이 아니라 임의의 어떤 숫자를 예측하는 문제이다. 예를 들면 내년도 경제 성장률을 예측하거나 배달이 도착할 시간을 예측하는 것이 회귀 문제이다. 회귀는 정해진 클래스가 없고 임의의 수치를 출력한다. [출처 : 혼자 공부하는 머신러닝+딥러닝 3장. 회귀알고리즘과 모델규제] K-최근접 이웃 회귀(출처 : kNN 최근접 이웃 알고리즘) 녹색 영화는 액션영화일까? 로맨틱 영화일까?녹색 영화는 액션 영화와 로맨틱 영화 가운데 있다.그래서 상당히 답을하기 곤란한 상황이다.현실 세계에서 이게 액션 영화다 로맨틱 영화다 라고 딱 부러지게 얘기하기는 어렵다.이럴 경우에 머신러닝을 사용해 예측갑을 가지고 이야기 할 수가 있다. 그래서 기존의 데이터, 녹색 별을 제외한 기존의 데이터를 중심으로 이 녹색영화가 액션 영화다, 로맨틱 영화다라고 이야기 하는 방법이 knn 알고리즘이다. y축에 보이는 것처럼 발차기 횟수가 많을 경우에는 액션 영화의 가능성이 크고x축의 키스 횟수가 많을때는 로맨틱 영화다라고 볼 수가 있다. 그렇다면 여기서 knn 알고리즘을 간단하게 살펴보도록 하겠다.일단은 k를 정해줘야 한다.k는 최근접점을 우리가 몇개까지 볼것인지 정하는 것이다.일단 k=3으로 쓰겠다. 이것으로 한번 예측값을 내보도록 하겠다. k는 기본적으로 홀수를 쓴다. 왜냐하면 짝수로 쓰면 2:2와 같은 상황이 되어 답을 할 수 없는 상황이 되기 때문이다. 위 그래프에서 보이는 것처럼 최근접 거리에 있어 써클안에 액션 영화가 2개가 있고로맨틱 영화가 하나가 있다.그래서 원 안에 로맨틱 영화보다 액션 영화가 더 많기 때문에 녹색 영화는 액션 영화에요 라고 예측 값을 리턴할 수 있다.이것이 바로 knn 알고리즘의 핵심이다. 그렇다면 최근접점을 프로그램상에서 어떻게 구하는지 보자.피타고라스의 정리를 이용해서 구한다.두 정점의 거리를 구해서 가장 작은 거리의 점들부터 비교를 해나가는 것이다. 결정계수 (R^2)과대적합선형회귀모델 파라미터다항 회귀특성 공학과 규제다중 회귀특성 공학릿지라쏘하이퍼파라미터","link":"/2024/03/27/linearRegression/"},{"title":"비지도학습","text":"지도 학습과는 달리 정답 라벨이 없는 데이터를 비슷한 특징끼리 군집화하여 새로운 데이터에 대한 결과를 예측하는 방법을 비지도학습이라고 한다.라벨링 되어있지 않은 데이터로부터 패턴이나 형태를 찾아야 하기 때문에 지도학습보다는 조금 더 난이도가 있다고 할 수 있다.실제로 지도 학습에서 적절한 피처를 찾아내기 위한 전처리 방법으로 비지도 학습을 이용하기도 한다. [출처 : 혼자 공부하는 머신러닝+딥러닝 6장. 비지도 학습] 비지도학습비지도학습의 대표적인 종류는 클러스터링(Clustering)이 있다. 이 외에도 Dimentionality Reduction, Hidden Markov Model이 있다.예를 들어 여러 과일의 사진이 있고 이 사진이 어떤 과일의 사진인지 정답이 없는 데이터에 대해 색깔이 무엇인지, 모양이 어떠한지 등에 대한 피러르 토대로 바나나다, 사과다 등으로 군집화 하는 것이다. 지도/비지도 학습 모델(Semi-Supervised Learning)을 섞어서 사용할 수도 있다. 소량의 분류된 데이터를 사용해 분류되지 않은 더 큰 데이터 세트를 보강하는 방법으로 활용할 수도 있다. 최근 각광받고 있는 GAN(generative Adversarial Network) 모델도 비지도 학습에 해당한다. 과일 분류하기 예시1!wget https://bit.ly/fruits_300_data -O fruits_300.npy 코랩의 코드 셀에서 ‘!’ 문자로 시작하면 코랩은 이후 명령을 파이썬 코드가 아니라 리눅스 쉘 명령으로 이해한다. wget 명령은 원격 주소에서 데이터를 다운로드하여 저장한다. 1234import numpy as npimport matplotlib.pyplot as pltfruits = np.load('fruits_300.npy') npy 파일을 load() 메서드를 이용하여 로드한다. 12&gt;&gt;&gt; print(fruits.shape)(300, 100, 100) 첫 번째 차원(300)은 샘플의 개수 두 번째 차원(100)은 이미지 높이, 세 번째 차원(100)은 이미지 너비 1234567&gt;&gt;&gt; print(fruits[0, 0, :])[ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 2 3 2 1 2 1 1 1 1 2 1 3 2 1 3 1 4 1 2 5 5 5 19 148 192 117 28 1 1 2 1 4 1 1 3 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 첫 번째 해에 있는 픽셀 100개에 들어 있는 값을 출력하면 위와 같다.이 넘파이 배열은 흑백 사진을 담고 있으므로 0~255까지의 정숫값을 가진다. 이 첫 번째 이미지를 배열과 비교하기 위해 그림으로 그리면 아래와 같다. 12plt.imshow(fruits[0], cmap='gray')plt.show() cmap : 사용할 컬러의 스케일을 지정해줄 수 있음 우리가 보는 것과 컴퓨터가 처리하는 방식이 다르기 때문에 위와 같이 흑백 이미지를 반전하여 사용한다.cmap 매개변수를 ‘gray_r’로 지정하면 다시 반전하여 우리 눈에 보기 좋게 출력 가능하다. 12plt.imshow(fruits[0], cmap='gray_r')plt.show() 이 그림에서 밝은 부분은 0에 가깝고 짙은부분은 255에 가깝다. 픽셀값 분석하기로드 한 데이터의 처음 100개는 사과, 그다음 100개는 파인애플, 마지막 100개는 바나나이다.각 과일 사진의 평균을 내서 차이를 확인해보겠다.사용하기 쉽게 fruits 데이터를 사과, 파인애플, 바나나로 각각 나눠 보겠다. 123apple = fruits[0:100].reshape(-1, 100*100)pineapple = fruits[100:200].reshape(-1, 100*100)banana = fruits[200:300].reshape(-1, 100*100) reshape() 메서드를 사용해 두 번째 차원(100)과 세 번째 차원(100)을 10,000으로 합친다. 첫 번째 차원을 -1로 지정하면 자동으로 남은 차원을 할당한다. 이제 apple, pineapple, banana 배열의 크기는 100, 10000)이다. 각 배열에 들어 있는 샘플의 픽셀 평균값을 계산하기 위해 mean() 메서드를 사용하겠다.샘플마다 픽셀의 평균값을 계산해야 하므로 mean() 메서드가 평균을 계산할 축을 지정해야 한다.axis=0으로 하면 첫 번째 축인 행을 따라 계산한다.axis=1로 지정하면 두 번째 축인 열을 따라 계산한다. 1234567891011121314&gt;&gt;&gt; print(apple.mean(axis=1))[ 88.3346 97.9249 87.3709 98.3703 92.8705 82.6439 94.4244 95.5999 90.681 81.6226 87.0578 95.0745 93.8416 87.017 97.5078 87.2019 88.9827 100.9158 92.7823 100.9184 104.9854 88.674 99.5643 97.2495 94.1179 92.1935 95.1671 93.3322 102.8967 94.6695 90.5285 89.0744 97.7641 97.2938 100.7564 90.5236 100.2542 85.8452 96.4615 97.1492 90.711 102.3193 87.1629 89.8751 86.7327 86.3991 95.2865 89.1709 96.8163 91.6604 96.1065 99.6829 94.9718 87.4812 89.2596 89.5268 93.799 97.3983 87.151 97.825 103.22 94.4239 83.6657 83.5159 102.8453 87.0379 91.2742 100.4848 93.8388 90.8568 97.4616 97.5022 82.446 87.1789 96.9206 90.3135 90.565 97.6538 98.0919 93.6252 87.3867 84.7073 89.1135 86.7646 88.7301 86.643 96.7323 97.2604 81.9424 87.1687 97.2066 83.4712 95.9781 91.8096 98.4086 100.7823 101.556 100.7027 91.6098 88.8976] 사과 샘플 100개에 대한 픽셀 평균값을 계산한 것이다. 히스토그램을 그려보면 평균값이 어떻게 분포되어 있는지 한눈에 볼 수 있다. 히스토그램이란히스토그램은 값이 발생한 빈도를 그래프로 표시한 것이다. 보통 x축이 값의 구간(계급)이고, y축은 발생 빈도(도수)이다. 12345plt.hist(np.mean(apple, axis=1), alpha=0.8)plt.hist(np.mean(pineapple, axis=1), alpha=0.8)plt.hist(np.mean(banana, axis=1), alpha=0.8)plt.legend(['apple', 'pineapple', 'banana'])plt.show() 사과와 파인애플은 90~100 사이에 많이 모여있다. 바나나는 픽셀 평균값만으로 사과나 파인애플과 확실히 구분된다. 사과와 파인애플은 많이 겹쳐져있어서 픽셀값만으로는 구분하기 쉽지 않다. 해결책으로 샘플의 평균값이 아니라 픽셀별 평균값을 비교하는 방법이 있다.전체 샘플에 대해 각 픽셀의 평균을 계산하는 것이다.픽셀의 평균을 계산하는 것은 axis=0으로 지정하면 된다. 12345fig, axs = plt.subplots(1, 3, figsize=(20, 5))axs[0].bar(range(10000), np.mean(apple, axis=0))axs[1].bar(range(10000), np.mean(pineapple, axis=0))axs[2].bar(range(10000), np.mean(banana, axis=0))plt.show() 순서대로 사과, 파인애플, 바나나 그래프이다. 각 과일마다 값이 높은 구간이 다르다. 픽셀 평균값을 100*100 크기로 바꿔서 이미지처럼 출력하여 위 그래프와 비교하면 더 좋다.픽셀을 평균 낸 이미지를 모든 사진을 합쳐 놓은 대표 이미지로 생각할 수 있다. 123456789apple_mean = np.mean(apple, axis=0).reshape(100, 100)pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)banana_mean = np.mean(banana, axis=0).reshape(100, 100)fig, axs = plt.subplots(1, 3, figsize=(20, 5))axs[0].imshow(apple_mean, cmap='gray_r')axs[1].imshow(pineapple_mean, cmap='gray_r')axs[2].imshow(banana_mean, cmap='gray_r')plt.show() 세 과일은 픽셀 위치에 따라 값의 크기가 차이난다.이 대표 이미지와 가까운 사진을 골라낸다면 사과, 파인애플, 바나나를 구분할 수 있을 것이다. 이처럼 흑백 사진에 있는 픽셀값을 사용해 과일 사진을 모으는 작업을 해 보았다. 이렇게 비슷한 샘플끼리 그룹으로 모으는 작업을 군집(clustering)이라고 한다. 군집은 대표적인 비지도 학습 작업 중 하나이고, 군집 알고리즘에서 만든 그룹을 클러스터(cluster)라고 부른다. k-means앞에서는 사과, 파인애플, 바나나에 있는 각 픽셀의 평균값을 구해서 가장 가까운 사진을 골랐다. 이 경우에는 사과, 파인애플, 바나나 사진임을 미리 알고 있었기 때문에 각 과일의 평균을 구할 수 있었다. 하지만 진짜 비지도 학습에서는 사진에 어떤 과일이 들어 있는지 알지 못한다.이런 경우 어떻게 평균값을 구할 수 있을까? 바로 k-평균(k-means) 군집 알고리즘이 평균값을 자동으로 찾아준다. k-means 알고리즘 작동방식 무작위로 k개의 클러스터 중심을 정한다. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정한다. 클러스터에서 속한 샘플의 평균값으로 클러스터 중심을 변경한다. 클러스터 중심에 변화가 없을 때까지 2번으로 돌아가 반복한다. k-means 모델 만들기1. 데이터 준비하기1234!wget https://bit.ly/fruits_300_data -O fruits_300.npyimport numpy as npfruits = np.load('fruits_300.npy')fruits_2d = fruits.reshape(-1, 100*100) 준비된 넘파이 배열을 100*10000 크기로 재배열한다. 2. k-means 알고리즘으로 모델 학습하기사이킷런의 k-평균 알고리즘은 sklearn.cluster 모듈에 KMeans 클래스가 구현되어 있다.n-cluster 매개변수로 클러스터 갯수를 지정할 수 있다.3개로 지정 후 모델을 훈련시킨다. 123from sklearn.cluster import KMeanskm = KMeans(n_clusters=3, random_state=42)km.fit(fruites_2d) 군집된 결과는 KMeans 객체의 labels_ 속성된 결과에 저장된다.클러스터 갯수가 3이기 때문에 배열의 값은 0,1,2 중 하나이다.단, 레이블값과 순서에 의미는 없다. 12345678910&gt;&gt;&gt; print(km.labels_)[2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 이를 통해 각 클러스터의 샘플의 갯수를 알 수 있다. 12&gt;&gt;&gt; print(np.unique(km.labels_, return_counts=True))(array([0, 1, 2], dtype=int32), array([111, 98, 91])) 3. 각 클러스터의 그림 출력하기각 클러스터가 어떤 이미지를 나타냈는지 그림으로 출력하기 위해 간단한 유틸리티 함수를 만들어본다. 12345678910111213141516import matplotlib.pyplot as pltdef draw_fruits(arr, ratio=1): n = len(arr) # n은 샘플 개수입니다 # 한 줄에 10개씩 이미지를 그립니다. 샘플 개수를 10으로 나누어 전체 행 개수를 계산합니다. rows = int(np.ceil(n/10)) # 행이 1개 이면 열 개수는 샘플 개수입니다. 그렇지 않으면 10개입니다. cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False) for i in range(rows): for j in range(cols): if i*10 + j &lt; n: # n 개까지만 그립니다. axs[i, j].imshow(arr[i*10 + j], cmap='gray_r') axs[i, j].axis('off') plt.show() draw_fruits()는 (샘플갯수, 너비, 높이)의 3차원 배열을 받아 가로로 10개의 이미지를 출력하는 함수이다. figsize는 ratio 매개변수에 비례하여 커진다. draw_fruits()에 fruits 배열을 불리언 인덱싱을 통해 넣어준다. 1draw_fruits(fruits[km.labels_==0]) 레이블 0에는 파인애플과 바나나, 사과가 섞여있는 것을 볼 수 있다. k-means 알고리즘이 이 샘플들을 완벽하게 분류하진 못했지만, 비슷한 샘플을 잘 모은 것을 볼 수 있다. 사과를 완벽하게 분류 했다. 4. 클러스터 중심KMeans 클래스가 최종적으로 찾은 클러스터 중심은 cluser_centers_ 속성에 저장되어 있다.이를 그림으로 표현해보면 아래와 같다. 1draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3) 이전에 각 과일의 평균 픽셀값을 출력했던 것과 비슷함을 확인할 수 있다. 훈련 데이터 샘플에서 클러스터 중심까지 거리로 변환해주는 transfor() 메서드와, 데이터를 예측하는 predict() 메서드가 있다.클러스터 중심이 가장 가까운 것이 예측 클래스로 출력된다. 12345&gt;&gt;&gt; print(km.transform(fruits_2d[100:101]))[[3393.8136117 8837.37750892 5267.70439881]]&gt;&gt;&gt; print(km.predict(fruits_2d[100:101]))[0] k-means 알고리즘은 클러스터 중심을 옮기면서 최적의 클러스터를 찾는 과정을 반복하는데, 알고리즘이 반복한 횟수는 n_iter_에 저장된다. 12&gt;&gt;&gt; print(km.n_iter_)4 최적의 k 찾기 : 엘보우 방법k-means 알고리즘의 단점 중 하나는, 클러스터 갯수를 사전에 지정해야 한다는 것이다.군집 알고리즘에서 적절한 k값을 찾는 완벽한 방법은 없다. 저마다 장단점이 있지만 가장 대표적인 엘보우 방법을 알아보겠다. k-means 알고리즘은 클러스터 중심과 클러스터에 속한 샘플 사이의 거리를 잴 수 있는데, 이것의 제곱합을 이너셔(inertia)라고 한다. 이너셔는 클러스터에 속한 샘플이 얼마나 가깝게 모여있는지를 나타내는 값인데, 클러스터 개수가 늘어나면 이너셔도 줄어든다. 엘보우 방법은 클러스터 갯수를 늘려가면서 이너셔의 변화를 관찰하여 최적의 클러스터를 찾는 방법이다. 클러스터 갯수에 대한 이너셔를 그래프로 그리면 꺽이는 지점이 있는데, 그 지점이 바로 적절한 클러스터 갯수이다.KMeans 클래스는 자동으로 이너셔를 계산해서 inertia_ 속성으로 제공한다. 123456789inertia = []for k in range(2, 7): km = KMeans(n_clusters=k, random_state=42) km.fit(fruits_2d) inertia.append(km.inertia_)plt.plot(range(2, 7), inertia)plt.xlabel('k')plt.ylabel('inertia')plt.show() 주성분 분석차원 축소주성분 분석설명된 분산","link":"/2024/04/01/unsupervisedLearning/"}],"tags":[{"name":"CLI","slug":"CLI","link":"/tags/CLI/"},{"name":"MachineLearning","slug":"MachineLearning","link":"/tags/MachineLearning/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"}],"categories":[{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"CLI","slug":"Linux/CLI","link":"/categories/Linux/CLI/"},{"name":"AI","slug":"AI","link":"/categories/AI/"},{"name":"MachineLearning","slug":"AI/MachineLearning","link":"/categories/AI/MachineLearning/"},{"name":"NLP","slug":"AI/NLP","link":"/categories/AI/NLP/"}],"pages":[]}