{"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Reactive Design Pattern Actors Cluster Cassandra Event Sourcing &amp; CQRS Persistence(Event Sourcing) Lagom core concepts Docker &amp; Kubernetes Cassandra, Zookeeper, Kafka on Docker Akka, Lagom on Kubernetes Programming in Scala Webflux","link":"/2020/03/21/hello-world/"},{"title":"권한 관리 - chmod란","text":"유닉스 및 유닉스 유사 운영 체제에서 chmod는 파일 시스템 개체(파일 및 디렉토리)의 액세스 권한을 변경하는 데 사용되는 명령 및 system call이다.특수 모드 플래그를 변경할 때도 사용된다. 요청은 umask로 필터링된다. 이름은 Change mode의 약칭이다.","link":"/2020/04/08/chmod-types/"},{"title":"머신러닝을 위한 데이터 다루기","text":"머신러닝(machine learning)이란 규칙을 일일이 프로그래밍하지 않아도 자동으로 데이터에서 규칙을 학습하는 알고리즘을 연구하는 분야이다.최근 머신러닝의 발전은 통계나 수학 이론보다 경험을 바탕으로 발전하는 경우도 많다. 컴퓨터 과학 분야가 이런 발전을 주도하고 있다. 컴퓨터 과학 분야의 대표적인 머신러닝 라이브러리는 사이킷런(scikit-learn)이다. 사이킷런 라이브러리는 파이썬 API를 사용하는데 파이썬 언어는 배우기 쉽고 컴파일하지 않아도 되기 때문에 사용하기 편리하다.연구자들은 새로운 알고리즘을 끊임없이 개발하여 발표한다. 많은 사람이 이를 검증하고 사용해 본 다음 장단점을 파악하게 된다. 어느정도 시간이 지나서 이런 알고리즘이 유익하다고 증명되어 널리 사용하게 되면 사이킷런 라이브러리 개발자들이 이 알고리즘을 라이브러리에 추가한다. 그러므로 머신러닝 라이브러리에 포함된 알고리즘들은 안정적이며 성능이 검증되어 있다. 머신러닝에 쓰이는 용어 특성 : 데이터를 표현하는 하나의 성질ex) 생선 데이터 각각의 길이와 무게 훈련 : 머신러닝 알고리즘이 데이터에서 규칙을 찾는 과정, 사이킷런에서는 fit() 메서드가 하는 역할 K-최근접 이웃 알고리즘 : 가장 간단한 머신러닝 알고리즘 중 하나. 사실 어떤 규칙을 찾기보다는 전체 데이터를 메모리에 가지고 있는 것이 전부 모델 : 머신러닝 프로그램에서 알고리즘이 구현된 객체를 모델이라고 부름. 종종 알고리즘 자체를 모델이라고 부르기도 함 정확도 : 정확한 답을 몇 개 맞췄는지를 백분율로 나타낸 값. 사이킷런에서는 0~1 사이의 값으로 출력 됨. matplotlib scatter()는 산점도를 그리는 맷플롯십 함수이다. 처음 2개의 매개변수로 x축과 y축 값을 전달한다. 이 값은 파이썬 list 또는 numpy 배열입니다. c 매개변수로 색깔을 지정합니다. RGB를 16진수로 지저하거나 색깔코드 ‘b’, ‘g’, ‘r’, ‘c’, ‘m’, ‘y’, ‘k’, ‘w’ 중 하나를 지정합니다. 지저하지 않을 경우 10개의 기본 색깔을 사용해 그래프를 그립니다. maker 매개변수로 마커 스타일을 지정합니다. maker의 기본값은 o(circle, 원)입니다. scikit-learn KNeighborsClassifier()는 k-최근접 이웃 분류 모델을 만드는 사이킷런 클래스이다. n_neighbors 매개변수로 이웃의 개수를 지정한다. 기본값은 5이다. p 매개변수로 거리 재는 방법을 지정한다.(기본값 : 2) 1일 경우 : 맨해튼 거리 2일 경우 : 유클리디안 거리 n_jobs 매개변수로 사용할 CPU 코어를 지정한다. -1로 설정하면 모든 CPU 코어를 사용한다. 이웃 간의 거리 계산 속도를 높일 수 있지만 fit() 메서드에는 영향이 없다. 기본값은 1 fit() : 사이킷런 모델을 훈련할 때 사용하는 메서드이다. 처음 두 매개변수로 훈련에 사용할 특성과 정답 데이터를 전달한다. predict() : 사이킷런 모델을 훈련하고 예측할 때 사용하는 메서드이다. 특성 데이터 하나만 매개변수로 받는다. score() : 훈련된 사이킷런 모델의 성능을 측정한다. 처음 두 매개변수로 특성과 정답 데이터를 전달한다. 이 메서드는 먼저 predict() 메서드로 예측을 수행한 다음 분류 모델일 경우 정답과 비교하여 올바르게 예측한 개수의 비율을 반환한다. 지도학습과 비지도학습지도학습 (supervised learning) 지도학습에서는 데이터와 정답을 입력(input)과 타깃(target)이라고 하고, 이 둘을 합쳐 훈련 데이터(training data)라고 부른다. 입력으로 사용된 길이와 무게를 특성(feature)이라고 한다. 훈련세트와 테스트세트 훈련 세트 : 모델을 훈련할 때 사용하는 데이터. 보통 훈련 세트가 크면 클수록 좋다. 따라서 테스트 세트를 제외한 모든 데이터를 사용한다. 테스트 세트 : 전체 데이터에서 20~30%를 테스트 세트로 사용하는 경우가 많다. 전체 데이터가 아주 크다면 1%만 덜어내도 충분할 수 있다. 샘플링 편향(sampling bias) 훈련 세트와 테스트 세트에 샘플이 골고루 섞여 있지 않은 경우 넘파이 (numpy) numpy는 파이썬의 대표적인 배열 라이브러리이다. 파이썬의 리스트로 2차원 리스트를 표현할 수 있지만 고차원 리스트를 표현하려면 번거롭다. 넘파이는 고차원의 배열을 손쉽게 만들고 조작할 수 있는 간편한 도구를 많이 제공한다. seed() : 넘파이에서 난수를 생성하기 위한 정수 초깃값을 지정한다. 초깃값이 같으면 동일한 난수를 뽑을 수 있다. 따라서 랜덤 함수의 결과를 동일하게 재현하고 싶을 때 사용한다. arrange() : 일정한 간격의 정수 또는 실수 배열을 만든다. 기본 간격은 1이다. 매개변수가 하나이면 종료 숫자를 의미한다. 0에서 종료 숫자까지 배열을 만든다. 종료 숫자는 배열에 포함되지 않는다. 12&gt;&gt;&gt; print(np.arange(3))[0 1 2] 매개변수가 2개이면 시작 숫자, 종료 숫자를 의미한다. 12&gt;&gt;&gt; print(np.arange(1, 3))[1 2] 매개변수가 3개면 마지막 매개변수가 간격을 나타낸다. 12&gt;&gt;&gt; print(np.arange(1, 3, 0.2))[1. 1.2 1.4 1.6 1.8 2. 2.2 2.4 2.6 2.8] shuffle() : 주어진 배열을 랜덤하게 섞는다. 다차원 배열일 경우 첫 번째 축(행)에 대하여만 섞는다.123456&gt;&gt;&gt; arr = np.array([[1, 2], [3, 4], [5, 6]])&gt;&gt;&gt; np.random.shuffle(arr)&gt;&gt;&gt; print(arr)[[1 2] [5 6] [3 4]] 파이썬 리스트를 넘파이 배열로 바꾸는것은 쉽다. 넘파이 array() 함수에 파이썬 리스트를 전달하면 끝이다. 1input_arr = np.array(python_list) 넘파이는 슬라이싱 외에 배열 인덱싱(array indexing)이란 기능을 제공한다. 배열 인덱싱은 1개의 인덱스가 아닌 여러개의 인덱스로 한 번에 여러 개의 원소를 선택할 수 있다.예를 들면 아래처럼 input_arr에서 두 번째와 네 번째 샘플을 선택하여 출력 가능하다. 123print(input_arr[[1,3]])&gt;&gt; [[ 26.3 290. ] [ 29. 363. ]] 비지도학습 (unsupervised learning)타깃 데이터가 없다. 따라서 무엇을 예측하는것이 아니라 입력 데이터에서 어떤 특징을 찾는 데 주로 활용한다. 데이터 전처리데이터를 표현하는 기준이 다르면 알고리즘이 올바르게 예측할 수 없다. (두 특성의 스케일이 다른 경우 등)알고리즘이 거리기반일 때 특히 그렇다. 여기에는 k-최근접 이웃도 포함된다. 이런 알고리듬들은 샘플 간의 거리에 영향을 많이 받으므로 제대로 사용하려면 특성값을 일정한 기준으로 맞춰 주어야 한다. 이런 작업을 데이터 전처리(data preprocessing)라고 부른다. 표준점수데이터 전처리 방법 중 하는 표준점수이다(혹은 z 점수라고도 부른다). 표준점수는 각 특성값이 0에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다. 이를 통해 실제 특성값의 크기와 상관없이 동일한 조건으로 비교할 수 있다. numpy의 std() 함수를 이용하여 표준점수를 계산할 수 있다.12mean = np.mean(train_input, axis=0)std = np.std(train_input, axis=0) 브로드캐스팅크기가 다른 넘파이 배열에서 자동으로 사칙 연산을 모든 행이나 열로 확장하여 수행하는 기능이다. scikit-learntrain_test_split()훈련 데이터를 훈련 세트와 테스트 세트로 나누는 함수다. 여러 개의 배열을 전달할 수 있다. 테스트 세트로 나눌 비율은 test_size 매개변수에서 지정할 수 있으며 기본값은 0.25(25%)이다.shuffle 매개변수로 훈련 세트와 테스트 세트로 나누기 전에 무작위로 섞을지 여부를 결정할 수 있다. 기본값은 true이다. stratify 매개변수에 클래스 레이블이 담긴 배열(일반적으로 타깃 데이터)을 전달하면 클래스 비율에 맞게 훈련 세트와 테스트 세트를 나눈다. kneighbors()k-최근접 이웃 객체의 메서드이다. 이 메서드는 입력한 데이터에 가장 가까운 이웃을 찾아 거리와 이웃 샘플의 인덱스를 반환한다. 기본적으로 이웃의 개수는 KNeighborClassifier 클래스의 객체를 생성할 때 지정한 개수를 사용한다. 하지만 n_neighbors 매개변수에서 다르게 지정할 수도 있다. return_distance 매개변수를 False로 지정하면 이웃 샘플의 인덱스만 반환하고 거리는 반환하지 않는다. 이 매개변수의 기본값은 True이다. 튜플이란? 파이썬 튜플은 리스트와 매우 비슷하다. 리스트처럼 원소에 순서가 있지만 한 번 만들어진 튜플은 수정할 수 없다. 튜플을 사용하면 함수로 전달한 값이 바뀌지 않는다는 것을 믿을 수 있기 때문에 매개변수 값으로 많이 사용된다. column_stack : 전달받은 리스트를 일렬로 세운 다음 차례대로 나란히 연결. 1234&gt;&gt;&gt; np.column_stack(([1,2,3],[4,5,6]))array([[1, 4], [2, 5], [3, 6]])","link":"/2024/03/25/machineLearning/"},{"title":"RAG란 무엇인가?","text":"RAG는 “Retrieve, Generate, and Rank”의 약자로, 주로 자연어 처리(NLP)와 관련된 작업에서 사용되는 기술방법론이다. 이 접근 방식은 정보검색(IR: Information Retrieval)과 생성적 모델(Generative Models)을 결합하여, 복잡한 질문에 대해 더 정확하고 관련성 높은 답변을 생성하는데 사용된다. RAG 모델은 특히 대규모 텍스트 데이터셋에서 정확한 정보를 검색하고, 이를 기반으로 새로운 텍스트를 생성하여 사용자의 질문에 답변하는 데 효과적이다. 작동방식RAG 모델의 작동 방식은 크게 세 단계로 나눌 수 있다. 검색(Retrieve) : 사용자의 질문이 주어지면, 모델은 대규모 데이터셋에서 질문과 관련된 정보나 문서를 검색한다. 이 과정에서는 주로 효율적인 검색 알고리즘이나 기술이 사용된다. 생성(Generate) : 검색된 정보를 기반으로, 모델은 관련성 높은 답변을 생성한다. 이 단계에서는 생성적 딥러닝 모델, 특히 변형자(Transformer) 기반의 언어 모델이 사용될 수 있다. 순위 매기기(Rank) : 모델이 생성한 여러 답변 중에서 최종 사용자에게 제시될 가장 적합한 답변을 선택하기 위해, 답변들의 순위를 매긴다. 순위 매기기는 답변의 정확성, 관련성, 유용성 등 여러 기준을 고려하여 수행된다. 응용분야RAG 모델은 다양한 NLP 작업에 활용될 수 있으며, 특히 정보가 풍부한 오픈 도메인 질의응답(Open-Domain Question Answering), 자연어 이해(Natural Language Understanding), 챗봇(Chatbots) 등의 분야에서 유용하다. 장점 정확성과 관련성 향상 : RAG 모델은 관련 문서를 검색하여 정보를 기반으로 답변을 생성하기 때문에, 답변의 정확성과 관련성이 향상될 수 있다. 유연성 : 다양한 종류의 데이터와 질문에 적용될 수 있는 높은 유연성을 가진다. 지식 기반 학습 : RAG 모델은 대규모 데이터셋에서 검색한 정보를 활용하여 학습히기 때문에, 지식 기반 학습에 강점을 보인다. 한계 리소스 요구량 : 대규모 데이터셋에서 효율적인 검색과 순위 매기기를 수행하려면 상당한 계산 리소스가 필요할 수 있다. 검색 품질 : 최종 답변의 품질은 검색 단계에서 검색된 문서의 품질에 크게 의존한다. 검색 알고리즘의 성능이 중요하다. RAG는 AI와 NLP 분야에서 중요한 연구주제이며, 지속적인 기술 발전으로 인해 응용 가능성이 확대되고 있다.","link":"/2024/03/26/WhatIsRAG/"},{"title":"선형회귀 알고리즘","text":"지도 학습 알고리즘은 크게 분류와 회귀(regression)으로 나뉜다. 분류는 말 그대로 샘플을 몇 개의 클래스 중 하나로 분류하는 문제이다. 회귀는 클래스 중 하나로 분류하는 것이 아니라 임의의 어떤 숫자를 예측하는 문제이다. 예를 들면 내년도 경제 성장률을 예측하거나 배달이 도착할 시간을 예측하는 것이 회귀 문제이다. 회귀는 정해진 클래스가 없고 임의의 수치를 출력한다. [출처 : 혼자 공부하는 머신러닝+딥러닝 3장. 회귀알고리즘과 모델규제] K-최근접 이웃 회귀(출처 : kNN 최근접 이웃 알고리즘) 녹색 영화는 액션영화일까? 로맨틱 영화일까?녹색 영화는 액션 영화와 로맨틱 영화 가운데 있다.그래서 상당히 답을하기 곤란한 상황이다.현실 세계에서 이게 액션 영화다 로맨틱 영화다 라고 딱 부러지게 얘기하기는 어렵다.이럴 경우에 머신러닝을 사용해 예측갑을 가지고 이야기 할 수가 있다. 그래서 기존의 데이터, 녹색 별을 제외한 기존의 데이터를 중심으로 이 녹색영화가 액션 영화다, 로맨틱 영화다라고 이야기 하는 방법이 knn 알고리즘이다. y축에 보이는 것처럼 발차기 횟수가 많을 경우에는 액션 영화의 가능성이 크고x축의 키스 횟수가 많을때는 로맨틱 영화다라고 볼 수가 있다. 그렇다면 여기서 knn 알고리즘을 간단하게 살펴보도록 하겠다.일단은 k를 정해줘야 한다.k는 최근접점을 우리가 몇개까지 볼것인지 정하는 것이다.일단 k=3으로 쓰겠다. 이것으로 한번 예측값을 내보도록 하겠다. k는 기본적으로 홀수를 쓴다. 왜냐하면 짝수로 쓰면 2:2와 같은 상황이 되어 답을 할 수 없는 상황이 되기 때문이다. 위 그래프에서 보이는 것처럼 최근접 거리에 있어 써클안에 액션 영화가 2개가 있고로맨틱 영화가 하나가 있다.그래서 원 안에 로맨틱 영화보다 액션 영화가 더 많기 때문에 녹색 영화는 액션 영화에요 라고 예측 값을 리턴할 수 있다.이것이 바로 knn 알고리즘의 핵심이다. 그렇다면 최근접점을 프로그램상에서 어떻게 구하는지 보자.피타고라스의 정리를 이용해서 구한다.두 정점의 거리를 구해서 가장 작은 거리의 점들부터 비교를 해나가는 것이다. 결정계수 (R^2)과대적합선형회귀모델 파라미터다항 회귀특성 공학과 규제다중 회귀특성 공학릿지라쏘하이퍼파라미터","link":"/2024/03/27/linearRegression/"},{"title":"AWS Landing Zone 구축하기","text":"“Landing Zone”은 비행기가 안전하게 착륙할 수 있는 공간을 의미한다.즉, AWS Landing Zone을 설계한다는 것은 AWS 상에서 시스테믈 안정적으로 구축하기 위한 기본 사항을 준비하고 설계, 구축하는 일련의 과정이다. Landing Zone 이란?Multi Account의 확장 가능하고 안전한 환경을 제공하는 AWS 솔루션AWS의 컨트롤 타워는 ‘랜딩존’이라는 다중 계정의 AWS 환경을 쉽게 설정하고 관리하는 환경을 제공한다. 클라우드 세상의 컨트롤 타워클라우드를 잘 사용하기 위해서는 고려해야 할 사항이 많다. 성능, 보안, 안정성 등은 기본이고 가격 경쟁력이나 지속 가능성까지도 고려해야한다.서비스의 확장과 함께 탄탄한 아키텍처 기반에 대한 고민을 해결하기 위해 랜딩존을 사용할 수 있다고 한다.즉, 클라우드 환경 또는 확장 가능하고 유연한 아키텍처 설계를 위한 시작점으로써 랜딩존을 사용가능하다고 한다.랜딩 존은 아래와 같은 기능을 제공한다. 다계정 아키텍처가 있는 AWS 환경 초기 보안 기준 ID 및 액세스 관리 지배구조 데이터 보안 네트워크 설계 로그","link":"/2024/04/03/AWS-LandingZone/"},{"title":"로지스틱 회귀","text":"로지스틱 회귀는 선형 방정식을 사용한 분류 알고리즘이다. 시그모이드 함수나 소프트맥스 알고리즘을 사용하여 클래스 확률을 출력할 수 있다. 이 책에서는, 구성품을 모른채 먼저 구매할 수 있는 럭키백이 있다고 가정하고 럭키백을 열어봐야 구성품을 알 수 있다고 한다.럭키백에 들어간 생선의 크기, 무게 등 특성이 주어졌을 때, 어떤 생선인지에 대한 확률을 출력해야 한다. 이를 확인할 수 있는 로지스틱 회귀를 알아보고, 이진분류에 필요한 시그모이드 함수와 다중 분류에 필요한 소프트맥스 함수를 알아본다 [출처 : 혼자 공부하는 머신러닝+딥러닝 4장. 다양한 분류 알고리즘] 데이터 준비하기csv파일을 pandas로 읽어와 타깃 데이터, 입력 데이터로 나눈다. 12345678import pandas as pdfish = pd.read_csv(&quot;http://bit.ly/fish_csv_data&quot;)print(pd.unique(fish['Species'])) # 열에서 고유한 값 출력#fish의 종류를 타깃 데이터, 나머지 특성을 입력 데이터fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()fish_target = fish['Species'].to_numpy() 훈련 세트와 테스트세트로 나눈 후 표준점수로 전처리한다. 1234567891011from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaler#훈련세트와 테스트세트로 나눠주기train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)#입력 데이터 전처리ss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) k-최근접 이웃 분류기로 확률 예측사이킷런의 KneighborsClassifier 클래스로 모델을 훈련한다. 123from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier(n_neighbors=3)kn.fit(train_scaled, train_target) 12&gt;&gt;&gt; print(kn.classes_)['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'] 사이킷런에서는 문자열로 된 타깃값을 그대로 사용할 수 있지만, 순서가 자동으로 알파벳 순서로 매겨진다. 훈련된 모델로 테스트 세트의 5개 샘플의 종류를 예측한다. 12&gt;&gt;&gt; print(kn.predict(test_scaled[:5]))['Perch' 'Smelt' 'Pike' 'Perch' 'Perch'] pridict_proba() 메서드는 클래스별 확률값을 반환한다.Numpy의 round()는 반올림 함수이며 decimals 매개변수는 유지할 소수점 아래 자리를 지정할 수 있다. 123import numpy as np# 클래스별 확률값 반환proba = kn.predict_proba(test_scaled[:5]) 1234&gt;&gt;&gt; print(np.round(proba, decimals=4)) #소숫점 4자리까지 반올림해 반환[[0. 0. 1. 0. 0. 0. 0. ][0. 0. 0. 0. 0. 1. 0. ][0. 0. 0. 1. 0. 0. 0. ][0. 0. 0.6667 0. 0.3333 0. 0. ][0. 0. 0.6667 0. 0.3333 0. 0. ]] 4번째 샘플의 경우 Perch일 확률이 2/3, Roach일 확률이 1/3이다. 1234distances, indexes = kn.kneighbors(test_scaled[3:4])&gt;&gt;&gt; print(train_target[indexes])[['Roach' 'Perch' 'Perch']] 4번째 샘플의 이웃은 Perch가 2개, Roach가 1개로, 구한 확률이 맞음을 보여준다. 단, 이 방법은 3개의 최근접 이웃만을 사용하기에 확률은 0, 1/3, 2/3, 1 뿐이라는 한계가 있다. 로지스틱 회귀로지스틱 회귀는 이름은 회귀이지만 분류 모델이다.선형 회귀와 동일하게 선형 방정식을 학습한다.z = a x Weight + b x length + ··· + fa, b, c, d, e는 계수이며 z는 어떤 값도 될 수 있다. 하지만 확률로 표현하려면 0~1 사이의 값이 되어야 하기 때문에z가 아주 큰 음수일때 0이 되고, 아주 큰 양수일 때 1이 되도록 바꾼다.이는 시그모이드 함수를 사용하면 가능하다. 시그모이드 함수 넘파이를 이용해서 간단하게 그려본다. 1234567import matplotlib.pyplot as pltz = np.arange(-5, 5, 0.1)phi = 1 / (1 + np.exp(-z))plt.plot(z, phi)plt.xlabel('z')plt.ylabel('phi')plt.show() 이진 분류를 먼저 수행해 볼 것이다.이진 분류에서 시그모이드 출력이 0.5보다 크면 양성클래스, 작으면 음성클래스로 판단한다. 로지스틱 회귀로 이진분류 수행하기불리언 인덱싱으로 도미와 빙어 데이터를 골라낸다. 123bream_smelt_indexes = (train_target == &quot;Bream&quot;) | (train_target == &quot;Smelt&quot;)train_bream_smelt = train_scaled[bream_smelt_indexes]target_bream_smelt = train_target[bream_smelt_indexes] 이 데이터로 로지스틱 회귀 모델을 훈련한다. 123from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()lr.fit(train_bream_smelt, target_bream_smelt) 훈련한 모델로, 5개의 테스트 샘플을 예측해본다. 12&gt;&gt;&gt; print(lr.predict(train_bream_smelt[:5]))['Bream' 'Smelt' 'Bream' 'Bream' 'Bream'] 각각의 샘플 확률을 예측해본다. 1234&gt;&gt;&gt; print(lr.predict_proba(train_bream_smelt[:5]))[[0.99759855 0.00240145][0.02735183 0.97264817][0.99486072 0.00513928][0.98584202 0.01415798][0.99767269 0.00232731]] 로지스틱 회귀가 학습한 계수도 볼 수 있다. 12&gt;&gt;&gt; print(lr.coef_, lr.intercept_)[[-0.4037798 -0.57620209 -0.66280298 -1.01290277 -0.73168947]][-2.16155132] 계수들과 절편을 볼 수 있다. 따라서 이 로지스틱 회귀 모델이 학습한 방정식은 다음과 같다.z = -0.404 x 무게 + -0.576 x 길이 + ··· + -2.161 z값과 시그노이드 함수의 값 또한 볼 수 있다. 12345decisions = lr.decision_function(train_bream_smelt[:5])print(decisions)from scipy.special import expitprint(expit(decisions)) 1[-6.02927744 3.57123907 -5.26568906 -4.24321775 -6.0607117 ] [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731] 로지스틱 회귀로 다중 분류 수행하기LogisticRegression 클래스는 반복적인 알고리즘을 사용하며, max_iter 매개변수에서 반복값을 지정하며 기본값은 100 이다. 또한 릿지 회귀와 같이 계수의 제곱을 규제하며, L2 규제라고도 불린다. 릿지회귀에서 alpha로 규제의 양을 조절한 것과 달리, C 매개변수로 조절한다. C의 기본값은 1이며 작을수록 규제가 커진다. 123456lr = LogisticRegression(C=20, max_iter=1000)lr.fit(train_scaled, train_target)print(lr.score(train_scaled, train_target))print(lr.score(test_scaled, test_target))print(lr.predict(test_scaled[:5])) # 샘플 5개의 종류 예측 1230.93277310924369750.925['Perch' 'Smelt' 'Pike' 'Roach' 'Perch'] 과대적합이나 과소적합이 되지 않았다. 5개 샘플에 대한 예측 샘플도 볼 수 있다. 123print(lr.classes_)proba = lr.predict_proba(test_scaled[:5])print(np.round(proba, decimals=3)) 123456['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'][[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] 클래스 정보와 클래스 예측 확률을 볼 수 있다. 다중 분류에서의 예측 확률은 소프트맥스 함수를 사용하여 7개의 z값을 확률로 변환한다. 소프트맥스 함수소프트 맥스 함수는 다중 분류에서 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 만든다. 수식 넣을 방법 필요…","link":"/2024/03/28/Logistic-Regression/"},{"title":"개체명 인식(Named Entity Recognition)","text":"NER(Named Entity Recognition)은 자연어 처리(Natural Language Processing, NLP)의 한 영역으로, 텍스트에서 사람, 조직, 위치, 날짜, 시간, 통화, 비율과 같은 명명된 엔티티(명사)를 식별하고 분류하는 기술이다. NER 시스템은 주어진 문서에서 중요한 정보를 추출하는 데 사용되며, 정보 검색, 질의 응답 시스템, 콘텐츠 요약, 고객 지원 시스템, 그리고 감정 분석 등 다양한 NLP 응용 프로그램에 활용된다. NER의 주요 기능: 엔티티 식별: 텍스트 데이터에서 사람의 이름, 기업 이름, 지리적 위치 등과 같은 엔티티를 식별합니다. 엔티티 분류: 식별된 엔티티를 사전 정의된 카테고리(예: 인물, 조직, 위치 등)로 분류합니다. 관계 추출: 텍스트 내에서 엔티티 간의 관계를 파악하고 추출합니다. 이는 텍스트 내에서 엔티티 간의 상호 작용을 이해하는 데 도움이 됩니다. NER의 활용 사례: 정보 추출: 뉴스 기사, 소셜 미디어 포스트, 문서에서 중요한 정보를 추출하여 구조화된 데이터로 변환합니다. 문서 분류: 엔티티 정보를 기반으로 문서를 자동으로 분류하고 정리합니다. 지능형 검색 엔진: 특정 엔티티에 관한 정보를 효율적으로 검색하기 위해 사용됩니다. 챗봇과 가상 비서: 사용자 질문에서 중요한 엔티티를 식별하여 보다 정확한 답변을 제공합니다. 감성 분석: 제품, 서비스, 브랜드에 대한 특정 엔티티의 언급을 분석하여 고객의 의견과 태도를 이해합니다.NER 기술의 발전은 딥러닝과 인공 신경망 모델의 진보 덕분에 크게 향상되었습니다. 이러한 모델들은 맥락과 의미를 더 잘 이해할 수 있게 해주며, NER의 정확도와 효율성을 높여줍니다.","link":"/2024/04/03/NER/"},{"title":"비지도학습","text":"지도 학습과는 달리 정답 라벨이 없는 데이터를 비슷한 특징끼리 군집화하여 새로운 데이터에 대한 결과를 예측하는 방법을 비지도학습이라고 한다.라벨링 되어있지 않은 데이터로부터 패턴이나 형태를 찾아야 하기 때문에 지도학습보다는 조금 더 난이도가 있다고 할 수 있다.실제로 지도 학습에서 적절한 피처를 찾아내기 위한 전처리 방법으로 비지도 학습을 이용하기도 한다. [출처 : 혼자 공부하는 머신러닝+딥러닝 6장. 비지도 학습] 비지도학습비지도학습의 대표적인 종류는 클러스터링(Clustering)이 있다. 이 외에도 Dimentionality Reduction, Hidden Markov Model이 있다.예를 들어 여러 과일의 사진이 있고 이 사진이 어떤 과일의 사진인지 정답이 없는 데이터에 대해 색깔이 무엇인지, 모양이 어떠한지 등에 대한 피러르 토대로 바나나다, 사과다 등으로 군집화 하는 것이다. 지도/비지도 학습 모델(Semi-Supervised Learning)을 섞어서 사용할 수도 있다. 소량의 분류된 데이터를 사용해 분류되지 않은 더 큰 데이터 세트를 보강하는 방법으로 활용할 수도 있다. 최근 각광받고 있는 GAN(generative Adversarial Network) 모델도 비지도 학습에 해당한다. 과일 분류하기 예시1!wget https://bit.ly/fruits_300_data -O fruits_300.npy 코랩의 코드 셀에서 ‘!’ 문자로 시작하면 코랩은 이후 명령을 파이썬 코드가 아니라 리눅스 쉘 명령으로 이해한다. wget 명령은 원격 주소에서 데이터를 다운로드하여 저장한다. 1234import numpy as npimport matplotlib.pyplot as pltfruits = np.load('fruits_300.npy') npy 파일을 load() 메서드를 이용하여 로드한다. 12&gt;&gt;&gt; print(fruits.shape)(300, 100, 100) 첫 번째 차원(300)은 샘플의 개수 두 번째 차원(100)은 이미지 높이, 세 번째 차원(100)은 이미지 너비 1234567&gt;&gt;&gt; print(fruits[0, 0, :])[ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 2 3 2 1 2 1 1 1 1 2 1 3 2 1 3 1 4 1 2 5 5 5 19 148 192 117 28 1 1 2 1 4 1 1 3 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 첫 번째 해에 있는 픽셀 100개에 들어 있는 값을 출력하면 위와 같다.이 넘파이 배열은 흑백 사진을 담고 있으므로 0~255까지의 정숫값을 가진다. 이 첫 번째 이미지를 배열과 비교하기 위해 그림으로 그리면 아래와 같다. 12plt.imshow(fruits[0], cmap='gray')plt.show() cmap : 사용할 컬러의 스케일을 지정해줄 수 있음 우리가 보는 것과 컴퓨터가 처리하는 방식이 다르기 때문에 위와 같이 흑백 이미지를 반전하여 사용한다.cmap 매개변수를 ‘gray_r’로 지정하면 다시 반전하여 우리 눈에 보기 좋게 출력 가능하다. 12plt.imshow(fruits[0], cmap='gray_r')plt.show() 이 그림에서 밝은 부분은 0에 가깝고 짙은부분은 255에 가깝다. 픽셀값 분석하기로드 한 데이터의 처음 100개는 사과, 그다음 100개는 파인애플, 마지막 100개는 바나나이다.각 과일 사진의 평균을 내서 차이를 확인해보겠다.사용하기 쉽게 fruits 데이터를 사과, 파인애플, 바나나로 각각 나눠 보겠다. 123apple = fruits[0:100].reshape(-1, 100*100)pineapple = fruits[100:200].reshape(-1, 100*100)banana = fruits[200:300].reshape(-1, 100*100) reshape() 메서드를 사용해 두 번째 차원(100)과 세 번째 차원(100)을 10,000으로 합친다. 첫 번째 차원을 -1로 지정하면 자동으로 남은 차원을 할당한다. 이제 apple, pineapple, banana 배열의 크기는 100, 10000)이다. 각 배열에 들어 있는 샘플의 픽셀 평균값을 계산하기 위해 mean() 메서드를 사용하겠다.샘플마다 픽셀의 평균값을 계산해야 하므로 mean() 메서드가 평균을 계산할 축을 지정해야 한다.axis=0으로 하면 첫 번째 축인 행을 따라 계산한다.axis=1로 지정하면 두 번째 축인 열을 따라 계산한다. 1234567891011121314&gt;&gt;&gt; print(apple.mean(axis=1))[ 88.3346 97.9249 87.3709 98.3703 92.8705 82.6439 94.4244 95.5999 90.681 81.6226 87.0578 95.0745 93.8416 87.017 97.5078 87.2019 88.9827 100.9158 92.7823 100.9184 104.9854 88.674 99.5643 97.2495 94.1179 92.1935 95.1671 93.3322 102.8967 94.6695 90.5285 89.0744 97.7641 97.2938 100.7564 90.5236 100.2542 85.8452 96.4615 97.1492 90.711 102.3193 87.1629 89.8751 86.7327 86.3991 95.2865 89.1709 96.8163 91.6604 96.1065 99.6829 94.9718 87.4812 89.2596 89.5268 93.799 97.3983 87.151 97.825 103.22 94.4239 83.6657 83.5159 102.8453 87.0379 91.2742 100.4848 93.8388 90.8568 97.4616 97.5022 82.446 87.1789 96.9206 90.3135 90.565 97.6538 98.0919 93.6252 87.3867 84.7073 89.1135 86.7646 88.7301 86.643 96.7323 97.2604 81.9424 87.1687 97.2066 83.4712 95.9781 91.8096 98.4086 100.7823 101.556 100.7027 91.6098 88.8976] 사과 샘플 100개에 대한 픽셀 평균값을 계산한 것이다. 히스토그램을 그려보면 평균값이 어떻게 분포되어 있는지 한눈에 볼 수 있다. 히스토그램이란히스토그램은 값이 발생한 빈도를 그래프로 표시한 것이다. 보통 x축이 값의 구간(계급)이고, y축은 발생 빈도(도수)이다. 12345plt.hist(np.mean(apple, axis=1), alpha=0.8)plt.hist(np.mean(pineapple, axis=1), alpha=0.8)plt.hist(np.mean(banana, axis=1), alpha=0.8)plt.legend(['apple', 'pineapple', 'banana'])plt.show() 사과와 파인애플은 90~100 사이에 많이 모여있다. 바나나는 픽셀 평균값만으로 사과나 파인애플과 확실히 구분된다. 사과와 파인애플은 많이 겹쳐져있어서 픽셀값만으로는 구분하기 쉽지 않다. 해결책으로 샘플의 평균값이 아니라 픽셀별 평균값을 비교하는 방법이 있다.전체 샘플에 대해 각 픽셀의 평균을 계산하는 것이다.픽셀의 평균을 계산하는 것은 axis=0으로 지정하면 된다. 12345fig, axs = plt.subplots(1, 3, figsize=(20, 5))axs[0].bar(range(10000), np.mean(apple, axis=0))axs[1].bar(range(10000), np.mean(pineapple, axis=0))axs[2].bar(range(10000), np.mean(banana, axis=0))plt.show() 순서대로 사과, 파인애플, 바나나 그래프이다. 각 과일마다 값이 높은 구간이 다르다. 픽셀 평균값을 100*100 크기로 바꿔서 이미지처럼 출력하여 위 그래프와 비교하면 더 좋다.픽셀을 평균 낸 이미지를 모든 사진을 합쳐 놓은 대표 이미지로 생각할 수 있다. 123456789apple_mean = np.mean(apple, axis=0).reshape(100, 100)pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)banana_mean = np.mean(banana, axis=0).reshape(100, 100)fig, axs = plt.subplots(1, 3, figsize=(20, 5))axs[0].imshow(apple_mean, cmap='gray_r')axs[1].imshow(pineapple_mean, cmap='gray_r')axs[2].imshow(banana_mean, cmap='gray_r')plt.show() 세 과일은 픽셀 위치에 따라 값의 크기가 차이난다.이 대표 이미지와 가까운 사진을 골라낸다면 사과, 파인애플, 바나나를 구분할 수 있을 것이다. 이처럼 흑백 사진에 있는 픽셀값을 사용해 과일 사진을 모으는 작업을 해 보았다. 이렇게 비슷한 샘플끼리 그룹으로 모으는 작업을 군집(clustering)이라고 한다. 군집은 대표적인 비지도 학습 작업 중 하나이고, 군집 알고리즘에서 만든 그룹을 클러스터(cluster)라고 부른다. k-means앞에서는 사과, 파인애플, 바나나에 있는 각 픽셀의 평균값을 구해서 가장 가까운 사진을 골랐다. 이 경우에는 사과, 파인애플, 바나나 사진임을 미리 알고 있었기 때문에 각 과일의 평균을 구할 수 있었다. 하지만 진짜 비지도 학습에서는 사진에 어떤 과일이 들어 있는지 알지 못한다.이런 경우 어떻게 평균값을 구할 수 있을까? 바로 k-평균(k-means) 군집 알고리즘이 평균값을 자동으로 찾아준다. k-means 알고리즘 작동방식 무작위로 k개의 클러스터 중심을 정한다. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정한다. 클러스터에서 속한 샘플의 평균값으로 클러스터 중심을 변경한다. 클러스터 중심에 변화가 없을 때까지 2번으로 돌아가 반복한다. k-means 모델 만들기1. 데이터 준비하기1234!wget https://bit.ly/fruits_300_data -O fruits_300.npyimport numpy as npfruits = np.load('fruits_300.npy')fruits_2d = fruits.reshape(-1, 100*100) 준비된 넘파이 배열을 100*10000 크기로 재배열한다. 2. k-means 알고리즘으로 모델 학습하기사이킷런의 k-평균 알고리즘은 sklearn.cluster 모듈에 KMeans 클래스가 구현되어 있다.n-cluster 매개변수로 클러스터 갯수를 지정할 수 있다.3개로 지정 후 모델을 훈련시킨다. 123from sklearn.cluster import KMeanskm = KMeans(n_clusters=3, random_state=42)km.fit(fruites_2d) 군집된 결과는 KMeans 객체의 labels_ 속성된 결과에 저장된다.클러스터 갯수가 3이기 때문에 배열의 값은 0,1,2 중 하나이다.단, 레이블값과 순서에 의미는 없다. 12345678910&gt;&gt;&gt; print(km.labels_)[2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 이를 통해 각 클러스터의 샘플의 갯수를 알 수 있다. 12&gt;&gt;&gt; print(np.unique(km.labels_, return_counts=True))(array([0, 1, 2], dtype=int32), array([111, 98, 91])) 3. 각 클러스터의 그림 출력하기각 클러스터가 어떤 이미지를 나타냈는지 그림으로 출력하기 위해 간단한 유틸리티 함수를 만들어본다. 12345678910111213141516import matplotlib.pyplot as pltdef draw_fruits(arr, ratio=1): n = len(arr) # n은 샘플 개수입니다 # 한 줄에 10개씩 이미지를 그립니다. 샘플 개수를 10으로 나누어 전체 행 개수를 계산합니다. rows = int(np.ceil(n/10)) # 행이 1개 이면 열 개수는 샘플 개수입니다. 그렇지 않으면 10개입니다. cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False) for i in range(rows): for j in range(cols): if i*10 + j &lt; n: # n 개까지만 그립니다. axs[i, j].imshow(arr[i*10 + j], cmap='gray_r') axs[i, j].axis('off') plt.show() draw_fruits()는 (샘플갯수, 너비, 높이)의 3차원 배열을 받아 가로로 10개의 이미지를 출력하는 함수이다. figsize는 ratio 매개변수에 비례하여 커진다. draw_fruits()에 fruits 배열을 불리언 인덱싱을 통해 넣어준다. 1draw_fruits(fruits[km.labels_==0]) 레이블 0에는 파인애플과 바나나, 사과가 섞여있는 것을 볼 수 있다. k-means 알고리즘이 이 샘플들을 완벽하게 분류하진 못했지만, 비슷한 샘플을 잘 모은 것을 볼 수 있다. 사과를 완벽하게 분류 했다. 4. 클러스터 중심KMeans 클래스가 최종적으로 찾은 클러스터 중심은 cluser_centers_ 속성에 저장되어 있다.이를 그림으로 표현해보면 아래와 같다. 1draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3) 이전에 각 과일의 평균 픽셀값을 출력했던 것과 비슷함을 확인할 수 있다. 훈련 데이터 샘플에서 클러스터 중심까지 거리로 변환해주는 transfor() 메서드와, 데이터를 예측하는 predict() 메서드가 있다.클러스터 중심이 가장 가까운 것이 예측 클래스로 출력된다. 12345&gt;&gt;&gt; print(km.transform(fruits_2d[100:101]))[[3393.8136117 8837.37750892 5267.70439881]]&gt;&gt;&gt; print(km.predict(fruits_2d[100:101]))[0] k-means 알고리즘은 클러스터 중심을 옮기면서 최적의 클러스터를 찾는 과정을 반복하는데, 알고리즘이 반복한 횟수는 n_iter_에 저장된다. 12&gt;&gt;&gt; print(km.n_iter_)4 최적의 k 찾기 : 엘보우 방법k-means 알고리즘의 단점 중 하나는, 클러스터 갯수를 사전에 지정해야 한다는 것이다.군집 알고리즘에서 적절한 k값을 찾는 완벽한 방법은 없다. 저마다 장단점이 있지만 가장 대표적인 엘보우 방법을 알아보겠다. k-means 알고리즘은 클러스터 중심과 클러스터에 속한 샘플 사이의 거리를 잴 수 있는데, 이것의 제곱합을 이너셔(inertia)라고 한다. 이너셔는 클러스터에 속한 샘플이 얼마나 가깝게 모여있는지를 나타내는 값인데, 클러스터 개수가 늘어나면 이너셔도 줄어든다. 엘보우 방법은 클러스터 갯수를 늘려가면서 이너셔의 변화를 관찰하여 최적의 클러스터를 찾는 방법이다. 클러스터 갯수에 대한 이너셔를 그래프로 그리면 꺽이는 지점이 있는데, 그 지점이 바로 적절한 클러스터 갯수이다.KMeans 클래스는 자동으로 이너셔를 계산해서 inertia_ 속성으로 제공한다. 123456789inertia = []for k in range(2, 7): km = KMeans(n_clusters=k, random_state=42) km.fit(fruits_2d) inertia.append(km.inertia_)plt.plot(range(2, 7), inertia)plt.xlabel('k')plt.ylabel('inertia')plt.show() 주성분 분석차원 축소주성분 분석설명된 분산","link":"/2024/04/01/Unsupervised-Learning/"},{"title":"인공 신경망 (Artificial Neural Network)","text":"인공 신경망(Artificial Neural Network, ANN)은 인간의 뇌 구조와 기능을 모방하여 만들어진 컴퓨팅 시스템이다. 인공 신경망은 데이터를 처리하고 학습하는데 사용되며, 주로 패턴 인식, 분류, 예측 등과 같은 다양한 머신러닝과 딥러닝 문제를 해결하는 데 활용한다. 생물학적 뉴런에서 영감 받아 만든 머신러닝 알고리즘이지만, 실제 우리 뇌를 모델링한 것은 아니다. 신경망은 기존 머신러닝 알고리즘으로 다루기 어려웠던 이미지, 음성, 텍스트 분야에서 뛰어난 성능을 발휘하면서 크게 주목받고 있다. 인공 신경망 알고리즘을 종종 딥러닝이라고 부른다. [출처 : 혼자 공부하는 머신러닝+딥러닝 7장. 인공 신경망] 1. 패션 MNIST딥러닝을 배울 때 많이 쓰는 데이터셋이 MNIST 데이터셋이다.10종류의 패션 아이템으로 구성되어있다. keras.datasets.fashion_mnist모듈 아래 load_data() 함수로 훈련 데이터와 테스트 데이터를 얻을 수 있다. 12345from tensorflow import keras(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()print(train_input.shape, traion_taret.shape)print(test_input.shape, test_target.shape) 12(60000, 28, 28) (60000,)(10000, 28, 28) (10000,) 28*28 사이즈의 이미지가 훈련세트에는 6만개, 테스트세트에는 1만개 들어있다. 훈련 데이터에서 몇 개의 샘플을 그림으로 표현해본다. 123456import matplotlib.pyplot as pltfig, axs = plt.subplots(1, 10, figsize=(10,10))for i in range(10): axs[i].imshow(train_input[i], cmap='gray_r') axs[i].axis('off')plt.show() 12import numpy as npprint(np.unique(train_target, return_counts=True)) 1(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])) 12각 레이블 :0 티셔츠, 1 바지, 2 스웨터, 3 드레스, 4 코트, 5 샌달, 6 셔츠, 7 스니커즈, 8 가방, 9 앵클부츠 각 레이블마다 6000개의 샘플이 들어있다. 2. 로지스틱 회귀로 패션 아이템 분류하기이 훈련 샘플은 60,000개나 되기 때문에, 전체 데이터를 한꺼번에 사용하여 모델을 훈련시키기보다, 샘플을 하나씩 꺼내서 모델을 훈련하는 방법이 효율적이다. 이 상황에서 가장 잘 맞는 방법은 확률적 경사 하강법이다.SGDClassifier 클래스의 매개변수 loss='log'로 지정하여 확률적 경사 하강법 모델을 만든다. 2-1. 데이터를 표준화 전처리하기확률적 경사 하강법은 여러 특성 중 기울기가 가장 가파른 방향을 따라 이동한다. 특성마다 값의 범위가 다르다면 올바르게 손실 함수의 경사를 내려올 수 없다. 이 데이터셋의 각 픽셀은 0255 사이의 정수값을 가지기 때문에, 255로 나누어 01 사이의 값으로 정규화한다. 12train_scaled = train_input / 255.0train_scaled = train_scaled.reshape(-1, 28*28) SGDClassifier는 2차원 입력을 다루지 못하기 때문에 각 샘플을 1차원 배열로 만들어야 한다. reshape() 메서드의 두 번째 매개변수를 28*28 이미지 크기에 맞게 지정하면 척 번째 차원은 변하지 않고, 원본 데이터의 두 번째, 세 번째 차원이 1차원으로 합쳐진다. 1print(train_scaled.shape) 1(60000, 784) 2-2. 모델을 만들고 교차검증하기SGDClassifier 클래스와 cross_validate()로 교차검증을 진행한다. 123456from sklearn.model_selection import cross_validatefrom sklearn.linear_model import SGDClassifiersc = SGDClassifier(loss='log_loss', max_iter=5, random_state=42)scores = cross_validate(sc, train_scaled, train_traget, n_jobs=-1)print(np.mean(scores['test_score'])) 10.8196000000000001 2-3. 로지스틱 회귀 공식 복습앞서 배운 로지스틱 회귀 공식은 이렇다.z = a * weight + b * length + ... + f 이를 MNIST 데이터셋에 적용하면 이렇다.z_티셔츠 = w1 * 픽셀1 + w2 * 픽셀2 + ... + w784 * 픽셀784 + bz_바지 = w1' * 픽셀1 + w2' * 픽셀2 + ... + w784' * 픽셀784 + b' 이와 같이 10개의 클래스에 대한 선형 방정식을 구한 뒤, 소프트맥스 함수를 통과하여 각 클래스에 대한 확률을 얻을 수 있다. 2-4. SGDClassifier 복습SGDClassifier는 사이킷런(Scikit-learn) 라이브러리에서 제공하는 확률적 경사 하강법(Stochastic Gradient Descent, SGD)을 사용한 선형 분류기이다. “SGD”는 반복적으로 주어진 데이터셋을 작은 배치로 나누어 모델의 가중치를 업데이트하는 최적화 알고리즘을 말하며, 이를 통해 분류 또는 회귀 문제를 해결할 수 있다. SGDClassifier는 특히 대규모 데이터셋에 대한 선형 분류 문제에 효과적인 방법으로 널리 사용된다. 이 클래스는 다양한 선형 모델, 예를 들어 로지스틱 회귀(Logistic Regression), 선형 서포트 벡터 머신(Linear Support Vector Machine) 등을 구현할 수 있도록 지원한다. 주요 기능 및 특징: 효율성: 대규모 데이터셋에 대해 효율적인 학습이 가능하다. 유연성: 다양한 손실 함수(Loss Function)를 지원하며, 이를 통해 다양한 선형 분류 문제를 해결할 수 있다. 예를 들어, loss=&quot;hinge&quot;는 선형 SVM을, loss=&quot;log&quot;는 로지스틱 회귀를 의미합니다. 정규화: l2, l1, elasticnet과 같은 정규화 옵션을 제공하여 모델의 복잡도를 조절하고, 과적합을 방지할 수 있다. 온라인 학습 지원: partial_fit 메서드를 사용하여 데이터가 순차적으로 도착할 때 모델을 점진적으로 업데이트할 수 있다. 사용 예시:123456789101112from sklearn.linear_model import SGDClassifierfrom sklearn.datasets import make_classification# 예제 데이터 생성X, y = make_classification(n_samples=1000, n_features=20, random_state=42)# SGDClassifier 인스턴스 생성 및 학습clf = SGDClassifier(loss=&quot;hinge&quot;, penalty=&quot;l2&quot;, max_iter=1000)clf.fit(X, y)# 예측predictions = clf.predict(X[:5]) 이 코드는 먼저 가상의 분류 데이터셋을 생성하고, SGDClassifier를 사용하여 선형 SVM 모델 (loss-&quot;hinge&quot;)을 학습한 뒤, 몇 개의 샘플에 대해 예측을 수행한다.SGDClassifier는 특히 대규모 데이터셋을 다룰 때 그 장점이 두드러지며, 다양한 선형 분류 문제에 적용될 수 있다. 3. 인공신경망으로 모델 만들기3-1. 인공 신경망 가장 간단한 인공 신경망은 출력층 하나가 있는 인공 신경망이다.확률적 경사 하강법을 사용하는 로지스틱 회귀와 같다.보통 인공 신경망을 이야기 할 때나 딥러닝을 이야기 할 때는 출력층 하나가 아니라 더 많은 층이 있는 경우를 이야기한다. 입력층 : 픽셀값 자체이고, 특별한 계산을 수행하지 않는다. 출력층 : z1 ~ z10을 계산하고 이를 바탕으로 클래스를 예측 뉴런 : z값을 계산하는 단위, 뉴런에서 일어나는 일은 선형 계산이 전부이다. 이제는 뉴런이란 표현 대신 유닛(unit)이라고 부르는 사람이 더 많다. 3-2. 텐서플로우와 케라스 텐서플로우 : 구글이 2015년 11월 오픈소스로 공개한 딥러닝 라이브러리케라스 : 텐서플로의 고수준 API 딥러닝 라이브러리가 다른 머신러닝 라이브러리와 다른 점 중 하나는 그래픽 처리 장치인 GPU를 사용하여 인공 신경망을 훈련한다는 것이다. GPU는 벡터와 행렬 연산에 매우 최적화되어 있기 때문에 곱셈과 덧셈이 많이 수행되는 인공 신경망에 큰 도움이 된다. 케라스 라이브러리는 직접 GPU 연산을 수행하지 않는다. 대신 GPU 연산을 수행하는 다른 라이브러리를 백엔드로 사용한다. 예를 들면 텐서플로가 케라스의 백엔드 중 하나이다. 이런 케라스를 멀티-백엔드 케라스라고 부른다. 케라스 API만 익히면 다양한 딥러닝 라이브러리를 입맛대로 골라서 쓸 수 있다. 3-3. 케라스 모델 만들기인공신경망에서는 교차 검증을 잘 사용하지 않고, 검증 세트를 별도로 덜어 내어 사용한다. 데이터셋이 충분히 크고, 교차검증에는 오랜시간이 걸리기 때문이다. 12345from sklearn.model_selection import train_test_splittrain_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)print(train_scaled.shape, train_target.shape) #훈련세트print(val_scaled.shape, val_target.shape) #검증세트 12(48000, 784) (48000,)(12000, 784) (12000,) 가장 기본이 되는 층인 밀집층을 만들어본다.입력층은 784개의 뉴런으로 구성되며, 출력층은 10개의 뉴런으로 구성된다.밀집층은 각 뉴런이 모두 연결되어야 하기 때문에, 784*10 = 7840개의 선이 포함된다.이를 완전 연결층이라고도 부른다. 12dense = keras.layers.Dense(10, activation='softmax', input_shape=(784, ))model = keras.Sequential(dense) # 밀집층을 가진 신경망 모델 뉴런의 갯수를 10으로 지정하고, 10개의 뉴런에서 출력되는 값을 확률로 바꾸기 위해 소프트맥스 함수를 이용한다. 만약 이진분류라면 activation='sigmoid'로 입력한다.이후 밀집층을 가진 신경망 모델을 만들기 위해 Sequential클래스를 사용한다. 소프트맥스와 같이 뉴런의 선형방정식 계산 결과에 적용되는 함수를 활성화 함수라고 부른다. 케라스 모델은 훈련하기 전 설정 단계가 있다. 1model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy') loss매개변수에 이진 분류라면 binary_crossentropy, 다중 분류라면 categorical_crossentropy를 사용한다. 정수로 된 타깃값을 원-핫 인코딩으로 바꾸지 않고 사용하려면 loss='sparse_categorical_crossentropy',타깃값을 원-핫 인코딩으로 준비했다면 loss='categorical_crossentropy'으로 지정한다. metrics 매개변수는 accuracy를 지정하면 에포크마다 정확도를 함께 출력해준다. 1model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 3s 1ms/step - loss: 0.6125 - accuracy: 0.7901Epoch 2/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4786 - accuracy: 0.8392Epoch 3/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4556 - accuracy: 0.8475Epoch 4/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4452 - accuracy: 0.8512Epoch 5/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4376 - accuracy: 0.8547 epochs 매개변수로 에포크 횟수를 지정할 수 있다.evaluate() 메서드로 모델의 성능을 평가해본다. 1model.evaluate(val_scaled, val_target) 12375/375 [==============================] - 1s 3ms/step - loss: 0.4630 - accuracy: 0.8458[0.46303632855415344, 0.8458333611488342] fit() 메서드와 비슷한 출력을 보여준다.","link":"/2024/04/02/Artificial-Neural-Network/"},{"title":"심층 신경망(Deep Neural Network)","text":"심층 신경망(Deep Neural Network, DNN)은 여러 개의 은닉층을 포함하는 인공 신경망의 한 종류이다. 인공 신경망은 입력층(input layer), 하나 이상의 은닉층(hidden layers), 그리고 출력층(output layer)으로 구성되며, 이 중에서 은닉층이 여러 개인 경우를 심층 신경망이라고 한다. 심층 신경망은 복잡한 데이터에서 높은 수준의 추상화와 패턴 인식을 수행할 수 있으며, 이미지 인식, 자연어 처리, 음성 인식 등 다양한 분야에서 광범위하게 활용된다. [출처 : 혼자 공부하는 머신러닝+딥러닝 7장. 인공 신경망] 심층 신경망의 특징: 다층 구조: 심층 신경망은 두 개 이상의 은닉층을 가진다. 은닉층의 수가 많을수록 네트워크는 더 복잡한 패턴과 관계를 학습할 수 있다. 비선형성: 심층 신경망은 비선형 활성화 함수를 사용하여 입력 데이터의 비선형 특성을 모델링한다. 이를 통해 선형 모델로는 표현할 수 없는 복잡한 패턴을 학습할 수 있다. 자동 특성 추출: 심층 신경망은 데이터로부터 중요한 특성을 자동으로 학습하고 추출할 수 있다. 이는 수동으로 특성을 설계하는 작업을 줄여준다. 범용 근사자: 이론적으로 심층 신경망은 어떤 함수도 근사할 수 있는 범용 함수 근사자(universal function approximator)로 간주된다. 활용 분야: 컴퓨터 비전: 이미지 분류, 객체 탐지, 이미지 생성 등에 활용된다. 자연어 처리: 기계 번역, 감성 분석, 텍스트 요약 등의 작업에 사용된다. 음성 인식: 음성을 텍스트로 변환하거나, 음성 명령을 인식하는 데 사용된다. 게임 및 로봇 공학: 자율 주행, 게임 AI, 로봇의 의사 결정 등에 활용된다. 도전 과제: 과적합(Overfitting): 모델이 훈련 데이터에 지나치게 최적화되어 새로운 데이터에 대한 일반화 능력이 떨어질 수 있다. 해석성(Interpretability): 심층 신경망의 결정 과정이 “블랙 박스”처럼 보일 수 있어, 모델의 예측을 해석하기 어려울 수 있다. 계산 비용: 심층 신경망의 학습은 대량의 데이터와 고성능의 컴퓨팅 자원을 요구한다. 이제 여러 개의 층을 추가하여 다층 인공 신경망, 즉 심층 신경망을 만들고, 은닉층에 사용하는 활성화 함수인 렐루 함수, 가중치와 절편을 학습하기 위한 옵티마이저를 알아본다. 1. 데이터 준비데이터를 표준화 전처리하고 훈련세트와 검증세트로 나눈다. 123456789from tensorflow import kerasfrom tensorflow import kerasfrom sklearn.model_selection import train_test_split(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()train_scaled = train_input / 255.0train_scaled = train_scaled.reshape(-1, 28*28)train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42) 2. 시그모이드 함수로 밀집층 추가하기인공 신경망과 달리, 입력층과 출력층 사이에 밀집층을 추가한다. 이를 은닉층이라고 한다. 2-1. 은닉층의 활성화 함수 : 시그모이드인공 신경망에서 출력층에 적용했던 소프트맥스 함수도 활성화 함수이다.단 출력층에서는 보통 이진 분류에서는 시그모이드 함수, 다중 분류에서는 소프트맥스를 사용한다.은닉층에도 활성화 함수가 적용되는데, 대표적으로 시그모이드 함수와 볼 렐루 함수가 있다. 은닉층 활성화 함수를 적용하는 이유는 선형 계산을 비선형으로 비틀어 주어 다음 층의 계산과 합쳐지지 않고 역할을 수행할 수 있기 때문이다. 아래 그림은 시그모이드 그래프이다. 이 함수는 뉴런의 출력 z값을 0과 1사이로 압축한다. 이를 사용해 은닉층을 만든다. 2-2. 시그모이드 활성화 함수로 심층 신경망 생성하기1234dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))#출력층에서 10개의 클래스를 분류하므로 10개의 뉴런, 소프트맥스 활성화함수dense2 = keras.layers.Dense(10, activation='softmax') activation='sigmoid'로 활성화 함수를 시그모이드로 지정할 수 있다.은닉층에서 100개의 뉴런을 지정했는데, 이는 특별한 기준이 없지만, 출력층의 뉴런보다는 많이 만들어야한다. 이제 위의 두 개층을 Sequential 클래스에 추가하여 심층 신경망을 만든다.두 개의 층을 리스트로 Sequential 클래스에 전달한다. 12model = keras.Sequential([dense1, dense2])model.summary() 1234567891011Model: &quot;sequential_2&quot;_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 100) 78500 dense_1 (Dense) (None, 10) 1010 =================================================================Total params: 79,510Trainable params: 79,510Non-trainable params: 0_________________________________________________________________ summary() 메서드로 층에 대한 정보를 얻는다. dense의 출력 크기를 보면 (None, 100)으로, 첫번째 차원은 샘플 크기를 나타낸다. 샘플 갯수가 아직 정의되지 않아 None 이며, 후에 fit() 매서드에 훈련 데이터를 주입하면 미니배치 경사 하강법을 사용한다.케라스의 기본 미니베치 크기는 32개이며, fit() 메서드에서 batch_size 매개변수로 바꿀 수 있다. 두번째 100개 출력은, 784개의 특성이 은닉층을 통과하며 100개의 특성으로 압축됨을 뜻한다. 모델 파라미터 갯수는 입력픽셀 784개와 100개의 모든 조합에 대한 가중치, 100개의 절편이 있어 784*100 + 199 = 78500개 이다. 두번째 층의 파라미터 또한 100*10 + 10 = 1010개 이다. 2-3. 층을 추가하는 다른 방법Sequential 클래스의 생성자 안에서 바로 Dense 클래스의 객체를 만드는 방법이 있다. 1234model = keras.Sequential([ keras.layers.Dense(100, activation='sigmoid', input_shape=(784,), name='hidden'), keras.layers.Dense(10, activation='softmax', name=&quot;output&quot;)], name='패션 MNIST모델') 너무 많은 층을 추가하려면 생성자가 매우 길어지기 때문에, add() 메서드도 사용한다. 123model = keras.Sequential()model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))model.add(keras.layers.Dense(10, activation='softmax')) 2-4. 심층 신경망 모델 훈련12model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 5s 3ms/step - loss: 0.5596 - accuracy: 0.8103Epoch 2/51500/1500 [==============================] - 4s 3ms/step - loss: 0.4054 - accuracy: 0.8556Epoch 3/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3716 - accuracy: 0.8658Epoch 4/51500/1500 [==============================] - 4s 3ms/step - loss: 0.3489 - accuracy: 0.8735Epoch 5/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3320 - accuracy: 0.8801 추가된 층이 성능을 항상시켰다는 것을 알 수 있다. 3. 렐루 활성화 함수초창기 인공 신경망의 은닉층에 많이 사용된 활성화 함수는 시그모이드 함수였다.다만 이 함수는 오른쪽과 왼쪽 끝으로 갈수록 그래프가 누워있기 때문에 올바른 출력을 만드는데 신속하게 대응하지 못한다는 단점이 있다. 이는 층이 많은 신경망일수록 효과가 누적되어 학습을 어렵게 한다. 이를 개선하기 위해 렐루함수가 사용된다. 렐루 함수는 max(0,z)로 쓸 수 있다. 이는 특히 이미지 처리에 좋은 성능을 낸다. 3-1. 입력 차원을 일렬로 펼치는 Flatten 층렐루 함수를 적용하기 전, 입력차원을 일렬로 펼치는 Flatten 층을 알아본다.앞에서 reshape() 메서드를 사용하여 사진 데이터를 일렬로 펼쳤지만, 이를 입력층과 은닉층 사이에 추가할 수 있다. 1model.add(keras.layers.Flatten(input_shape=(28, 28))) 3-2. 렐루 함수를 이용한 밀집층 추가123456model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28, 28)))model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dense(10, activation='softmax'))model.summary() 123456789101112Model: &quot;sequential_4&quot;_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 dense_4 (Dense) (None, 100) 78500 dense_5 (Dense) (None, 10) 1010 =================================================================Total params: 79,510Trainable params: 79,510Non-trainable params: 0_________________________________________________________________ Flatten 층을 신경망 모델에 추가하면 입력값의 차원을 짐작할 수 있다. 3-3. 훈련 데이터로 모델 훈련123456(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()train_scaled = train_input / 255.0train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 4s 2ms/step - loss: 0.5249 - accuracy: 0.8142Epoch 2/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3924 - accuracy: 0.8590Epoch 3/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3553 - accuracy: 0.8712Epoch 4/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3310 - accuracy: 0.8810Epoch 5/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3179 - accuracy: 0.8868 검증 세트로 모델을 평가해본다. 1model.evaluate(val_scaled, val_target) 12375/375 [==============================] - 1s 2ms/step - loss: 0.3948 - accuracy: 0.8674[0.39478805661201477, 0.8674166798591614] 은닉층을 추가하지 않은 모델보다 성능이 몇 퍼센트 더 상승했다. 4. 옵티마이저 : 다양한 경사 하강 알고리즘신경망에는 모델이 학습되지 않아 사람이 지정해주어야 하는 하이퍼파라미터가 많다. 다양한 종류의 경사 하강법 알고리즘도 지정할 수 있는데, 이를 옵티마저라고 한다. 4-1. SGD : 확률적 경사 하강법compile() 메서드에서 케라스의 기본 경사 하강법 알고리즘은 RMSprop을 사용했다.확률적 경사 하강법인 SGD를 사용할 수 있는데, 이 역시 미니배치를 사용한다. 1model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy') SGD 객체를 생성하여 옵티마저로 적용할 수 있다. 12sgd = keras.optimizers.SGD()model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy') SGD 클래스의 학습률 기본값은 0.01이며, learning_rate 매개변수에 학습률을 지정할 수 있다. 1sgd = keras.optimizers.SGD(learning_rate=0.1) momentum 매개변수의 기본값은 0이고 0보다 큰 값으로 지정하면 그레디언트를 가속도처럼 사용하는 모멘텀 최적화를 사용할 수 있다. 보통 0.9 이상을 지정한다.nesterov 매개변수를 True로 바꾸면 네스테로프 모멘텀 최적화를 사용한다. 1sgd = keras.optimizers(momentum=0.9, nesterov=True) 네스테로프 모멘텀은 모멘텀 최적화를 두번 반복하여 구현한다. 대부분 기본 확률적 경사 하강법보다 나은 성능을 제공한다. 4-2. Adagrad, RMSprop : 적응적 학습률 사용모델이 최적점에 가까이 갈수록 학습률을 낮출 수 있으며, 이를 통해 안정적으로 최적점에 수렴할 가능성이 높다. 12345adagrad = keras.optimizers.Adagrad()model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy')rmsprop = keras.optimizers.RMSprop()model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics='accurac') 모멘텀 최적화와 RMSprop 장점을 접목한 것이 Adam이다. 4-3. Adam : 모멘텀 최적화와 RMSprop의 장점 접목Adam 클래스의 매개변수 기본값을 사용해 모델을 훈련한다. 12345678model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dense(10, activation='softmax'))# 옵티마이저를 Adam으로 훈련model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 4s 2ms/step - loss: 0.5218 - accuracy: 0.8183Epoch 2/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3916 - accuracy: 0.8586Epoch 3/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3544 - accuracy: 0.8711Epoch 4/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3248 - accuracy: 0.8809Epoch 5/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3058 - accuracy: 0.8880 검증 세트에서의 성능도 확인해본다. 1model.evaluate(val_scaled, val_target) 12375/375 [==============================] - 1s 2ms/step - loss: 0.3426 - accuracy: 0.8767[0.3426271080970764, 0.8767499923706055]","link":"/2024/04/02/Deep-Neural-Network/"},{"title":"합성곱 신경망의 구성요소와 이미지 분류","text":"합성곱 신경망(Convolutional Neural Network, CNN)은 주로 이미지 인식, 영상 처리, 컴퓨터 비전 분야에서 사용되는 심층 신경망의 한 종류이다. CNN은 이미지로부터 패턴을 인식하고 이해하는 데 특화되어 있으며, 이를 위해 합성곱 계층(convolutional layer)과 풀링 계층(pooling layer)을 포함한 특별한 구조를 가진다. [출처 : 혼자 공부하는 머신러닝+딥러닝 8장. 이미지를 위한 인공신경망] CNN의 주요 구성 요소: 합성곱 계층(Convolutional Layer): 이 계층은 이미지로부터 특성을 추출하는 데 사용된다. 여러 개의 필터(또는 커널)를 사용하여 이미지를 스캔하고, 이 과정에서 생성된 특성 맵(feature map)을 통해 이미지의 중요한 정보를 추출한다. 활성화 함수(Activation Function): 대부분의 CNN에서는 ReLU(Rectified Linear Unit) 함수가 활성화 함수로 사용된다. 이 함수는 비선형 변환을 제공하여 네트워크가 복잡한 패턴을 학습할 수 있게 한다. 풀링 계층(Pooling Layer): 특성 맵의 크기를 줄이거나 요약하여 계산량을 감소시키고, 과적합을 방지하는 역할을 한다. 최대 풀링(Max Pooling)과 평균 풀링(Average Pooling)이 일반적으로 사용된다. 완전 연결 계층(Fully Connected Layer): CNN의 마지막 부분에 위치하며, 앞서 추출된 특성을 바탕으로 최종적인 분류나 예측을 수행한다. CNN의 특징 및 장점: 공간적 계층 구조: CNN은 이미지의 공간적 계층 구조를 이해할 수 있으며, 이를 통해 이미지의 로컬 패턴(예: 가장자리, 질감 등)부터 복잡한 객체까지 인식할 수 있다. 매개변수의 공유: 합성곱 필터는 이미지 전체에 걸쳐 공유되므로, 전통적인 심층 신경망에 비해 훨씬 적은 수의 매개변수를 사용한다. 이동 불변성(Translation Invariance): CNN은 이미지 내 객체의 위치가 변해도 동일한 객체를 인식할 수 있다. 활용 분야: 이미지 분류: 사진 속 객체를 분류한다(예: 강아지, 고양이 분류). 객체 탐지: 이미지 내에서 객체의 위치와 종류를 탐지한다. 시맨틱 분할: 이미지를 픽셀 수준에서 분류하여, 각 픽셀이 어떤 객체에 속하는지 결정한다. 얼굴 인식, 자율 주행 자동차, 의료 영상 분석 등 다양한 분야에서 광범위하게 활용된다.","link":"/2024/04/03/Convolutional-Neural-Network/"},{"title":"신경망 모델 훈련","text":"인공 신경망과 심층 신경망을 구성하고 다양한 옵티마이저를 통해 성능을 향상시킬 수 있는 방법에 대해 알아보았다.이번에는 과대적합을 막기 위해 신경망에서 사용하는 규제방법인 드롭아웃, 최상의 훈련 모델을 자동으로 저장하고 유지하는 콜백과 조기종료를 알아보겠다. [출처 : 혼자 공부하는 머신러닝+딥러닝 7장. 인공 신경망]","link":"/2024/04/02/Train-Neural-Network-Model/"}],"tags":[{"name":"CLI","slug":"CLI","link":"/tags/CLI/"},{"name":"MachineLearning","slug":"MachineLearning","link":"/tags/MachineLearning/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"DeepLearning","slug":"DeepLearning","link":"/tags/DeepLearning/"}],"categories":[{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"CLI","slug":"Linux/CLI","link":"/categories/Linux/CLI/"},{"name":"AI","slug":"AI","link":"/categories/AI/"},{"name":"MachineLearning","slug":"AI/MachineLearning","link":"/categories/AI/MachineLearning/"},{"name":"NLP","slug":"AI/NLP","link":"/categories/AI/NLP/"},{"name":"DeepLearning","slug":"AI/DeepLearning","link":"/categories/AI/DeepLearning/"}],"pages":[]}