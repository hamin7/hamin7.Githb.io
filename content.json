{"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Reactive Design Pattern Actors Cluster Cassandra Event Sourcing &amp; CQRS Persistence(Event Sourcing) Lagom core concepts Docker &amp; Kubernetes Cassandra, Zookeeper, Kafka on Docker Akka, Lagom on Kubernetes Programming in Scala Webflux","link":"/2020/03/21/hello-world/"},{"title":"권한 관리 - chmod란","text":"유닉스 및 유닉스 유사 운영 체제에서 chmod는 파일 시스템 개체(파일 및 디렉토리)의 액세스 권한을 변경하는 데 사용되는 명령 및 system call이다.특수 모드 플래그를 변경할 때도 사용된다. 요청은 umask로 필터링된다. 이름은 Change mode의 약칭이다.","link":"/2020/04/08/chmod-types/"},{"title":"머신러닝을 위한 데이터 다루기","text":"머신러닝(machine learning)이란 규칙을 일일이 프로그래밍하지 않아도 자동으로 데이터에서 규칙을 학습하는 알고리즘을 연구하는 분야이다.최근 머신러닝의 발전은 통계나 수학 이론보다 경험을 바탕으로 발전하는 경우도 많다. 컴퓨터 과학 분야가 이런 발전을 주도하고 있다. 컴퓨터 과학 분야의 대표적인 머신러닝 라이브러리는 사이킷런(scikit-learn)이다. 사이킷런 라이브러리는 파이썬 API를 사용하는데 파이썬 언어는 배우기 쉽고 컴파일하지 않아도 되기 때문에 사용하기 편리하다.연구자들은 새로운 알고리즘을 끊임없이 개발하여 발표한다. 많은 사람이 이를 검증하고 사용해 본 다음 장단점을 파악하게 된다. 어느정도 시간이 지나서 이런 알고리즘이 유익하다고 증명되어 널리 사용하게 되면 사이킷런 라이브러리 개발자들이 이 알고리즘을 라이브러리에 추가한다. 그러므로 머신러닝 라이브러리에 포함된 알고리즘들은 안정적이며 성능이 검증되어 있다. 머신러닝에 쓰이는 용어 특성 : 데이터를 표현하는 하나의 성질ex) 생선 데이터 각각의 길이와 무게 훈련 : 머신러닝 알고리즘이 데이터에서 규칙을 찾는 과정, 사이킷런에서는 fit() 메서드가 하는 역할 K-최근접 이웃 알고리즘 : 가장 간단한 머신러닝 알고리즘 중 하나. 사실 어떤 규칙을 찾기보다는 전체 데이터를 메모리에 가지고 있는 것이 전부 모델 : 머신러닝 프로그램에서 알고리즘이 구현된 객체를 모델이라고 부름. 종종 알고리즘 자체를 모델이라고 부르기도 함 정확도 : 정확한 답을 몇 개 맞췄는지를 백분율로 나타낸 값. 사이킷런에서는 0~1 사이의 값으로 출력 됨. matplotlib scatter()는 산점도를 그리는 맷플롯십 함수이다. 처음 2개의 매개변수로 x축과 y축 값을 전달한다. 이 값은 파이썬 list 또는 numpy 배열입니다. c 매개변수로 색깔을 지정합니다. RGB를 16진수로 지저하거나 색깔코드 ‘b’, ‘g’, ‘r’, ‘c’, ‘m’, ‘y’, ‘k’, ‘w’ 중 하나를 지정합니다. 지저하지 않을 경우 10개의 기본 색깔을 사용해 그래프를 그립니다. maker 매개변수로 마커 스타일을 지정합니다. maker의 기본값은 o(circle, 원)입니다. scikit-learn KNeighborsClassifier()는 k-최근접 이웃 분류 모델을 만드는 사이킷런 클래스이다. n_neighbors 매개변수로 이웃의 개수를 지정한다. 기본값은 5이다. p 매개변수로 거리 재는 방법을 지정한다.(기본값 : 2) 1일 경우 : 맨해튼 거리 2일 경우 : 유클리디안 거리 n_jobs 매개변수로 사용할 CPU 코어를 지정한다. -1로 설정하면 모든 CPU 코어를 사용한다. 이웃 간의 거리 계산 속도를 높일 수 있지만 fit() 메서드에는 영향이 없다. 기본값은 1 fit() : 사이킷런 모델을 훈련할 때 사용하는 메서드이다. 처음 두 매개변수로 훈련에 사용할 특성과 정답 데이터를 전달한다. predict() : 사이킷런 모델을 훈련하고 예측할 때 사용하는 메서드이다. 특성 데이터 하나만 매개변수로 받는다. score() : 훈련된 사이킷런 모델의 성능을 측정한다. 처음 두 매개변수로 특성과 정답 데이터를 전달한다. 이 메서드는 먼저 predict() 메서드로 예측을 수행한 다음 분류 모델일 경우 정답과 비교하여 올바르게 예측한 개수의 비율을 반환한다. 지도학습과 비지도학습지도학습 (supervised learning) 지도학습에서는 데이터와 정답을 입력(input)과 타깃(target)이라고 하고, 이 둘을 합쳐 훈련 데이터(training data)라고 부른다. 입력으로 사용된 길이와 무게를 특성(feature)이라고 한다. 훈련세트와 테스트세트 훈련 세트 : 모델을 훈련할 때 사용하는 데이터. 보통 훈련 세트가 크면 클수록 좋다. 따라서 테스트 세트를 제외한 모든 데이터를 사용한다. 테스트 세트 : 전체 데이터에서 20~30%를 테스트 세트로 사용하는 경우가 많다. 전체 데이터가 아주 크다면 1%만 덜어내도 충분할 수 있다. 샘플링 편향(sampling bias) 훈련 세트와 테스트 세트에 샘플이 골고루 섞여 있지 않은 경우 넘파이 (numpy) numpy는 파이썬의 대표적인 배열 라이브러리이다. 파이썬의 리스트로 2차원 리스트를 표현할 수 있지만 고차원 리스트를 표현하려면 번거롭다. 넘파이는 고차원의 배열을 손쉽게 만들고 조작할 수 있는 간편한 도구를 많이 제공한다. seed() : 넘파이에서 난수를 생성하기 위한 정수 초깃값을 지정한다. 초깃값이 같으면 동일한 난수를 뽑을 수 있다. 따라서 랜덤 함수의 결과를 동일하게 재현하고 싶을 때 사용한다. arrange() : 일정한 간격의 정수 또는 실수 배열을 만든다. 기본 간격은 1이다. 매개변수가 하나이면 종료 숫자를 의미한다. 0에서 종료 숫자까지 배열을 만든다. 종료 숫자는 배열에 포함되지 않는다. 12&gt;&gt;&gt; print(np.arange(3))[0 1 2] 매개변수가 2개이면 시작 숫자, 종료 숫자를 의미한다. 12&gt;&gt;&gt; print(np.arange(1, 3))[1 2] 매개변수가 3개면 마지막 매개변수가 간격을 나타낸다. 12&gt;&gt;&gt; print(np.arange(1, 3, 0.2))[1. 1.2 1.4 1.6 1.8 2. 2.2 2.4 2.6 2.8] shuffle() : 주어진 배열을 랜덤하게 섞는다. 다차원 배열일 경우 첫 번째 축(행)에 대하여만 섞는다.123456&gt;&gt;&gt; arr = np.array([[1, 2], [3, 4], [5, 6]])&gt;&gt;&gt; np.random.shuffle(arr)&gt;&gt;&gt; print(arr)[[1 2] [5 6] [3 4]] 파이썬 리스트를 넘파이 배열로 바꾸는것은 쉽다. 넘파이 array() 함수에 파이썬 리스트를 전달하면 끝이다. 1input_arr = np.array(python_list) 넘파이는 슬라이싱 외에 배열 인덱싱(array indexing)이란 기능을 제공한다. 배열 인덱싱은 1개의 인덱스가 아닌 여러개의 인덱스로 한 번에 여러 개의 원소를 선택할 수 있다.예를 들면 아래처럼 input_arr에서 두 번째와 네 번째 샘플을 선택하여 출력 가능하다. 123print(input_arr[[1,3]])&gt;&gt; [[ 26.3 290. ] [ 29. 363. ]] 비지도학습 (unsupervised learning)타깃 데이터가 없다. 따라서 무엇을 예측하는것이 아니라 입력 데이터에서 어떤 특징을 찾는 데 주로 활용한다. 데이터 전처리데이터를 표현하는 기준이 다르면 알고리즘이 올바르게 예측할 수 없다. (두 특성의 스케일이 다른 경우 등)알고리즘이 거리기반일 때 특히 그렇다. 여기에는 k-최근접 이웃도 포함된다. 이런 알고리듬들은 샘플 간의 거리에 영향을 많이 받으므로 제대로 사용하려면 특성값을 일정한 기준으로 맞춰 주어야 한다. 이런 작업을 데이터 전처리(data preprocessing)라고 부른다. 표준점수데이터 전처리 방법 중 하는 표준점수이다(혹은 z 점수라고도 부른다). 표준점수는 각 특성값이 0에서 표준편차의 몇 배만큼 떨어져 있는지를 나타낸다. 이를 통해 실제 특성값의 크기와 상관없이 동일한 조건으로 비교할 수 있다. numpy의 std() 함수를 이용하여 표준점수를 계산할 수 있다.12mean = np.mean(train_input, axis=0)std = np.std(train_input, axis=0) 브로드캐스팅크기가 다른 넘파이 배열에서 자동으로 사칙 연산을 모든 행이나 열로 확장하여 수행하는 기능이다. scikit-learntrain_test_split()훈련 데이터를 훈련 세트와 테스트 세트로 나누는 함수다. 여러 개의 배열을 전달할 수 있다. 테스트 세트로 나눌 비율은 test_size 매개변수에서 지정할 수 있으며 기본값은 0.25(25%)이다.shuffle 매개변수로 훈련 세트와 테스트 세트로 나누기 전에 무작위로 섞을지 여부를 결정할 수 있다. 기본값은 true이다. stratify 매개변수에 클래스 레이블이 담긴 배열(일반적으로 타깃 데이터)을 전달하면 클래스 비율에 맞게 훈련 세트와 테스트 세트를 나눈다. kneighbors()k-최근접 이웃 객체의 메서드이다. 이 메서드는 입력한 데이터에 가장 가까운 이웃을 찾아 거리와 이웃 샘플의 인덱스를 반환한다. 기본적으로 이웃의 개수는 KNeighborClassifier 클래스의 객체를 생성할 때 지정한 개수를 사용한다. 하지만 n_neighbors 매개변수에서 다르게 지정할 수도 있다. return_distance 매개변수를 False로 지정하면 이웃 샘플의 인덱스만 반환하고 거리는 반환하지 않는다. 이 매개변수의 기본값은 True이다. 튜플이란? 파이썬 튜플은 리스트와 매우 비슷하다. 리스트처럼 원소에 순서가 있지만 한 번 만들어진 튜플은 수정할 수 없다. 튜플을 사용하면 함수로 전달한 값이 바뀌지 않는다는 것을 믿을 수 있기 때문에 매개변수 값으로 많이 사용된다. column_stack : 전달받은 리스트를 일렬로 세운 다음 차례대로 나란히 연결. 1234&gt;&gt;&gt; np.column_stack(([1,2,3],[4,5,6]))array([[1, 4], [2, 5], [3, 6]])","link":"/2024/03/25/machineLearning/"},{"title":"RAG란 무엇인가?","text":"RAG는 “Retrieve, Generate, and Rank”의 약자로, 주로 자연어 처리(NLP)와 관련된 작업에서 사용되는 기술방법론이다. 이 접근 방식은 정보검색(IR: Information Retrieval)과 생성적 모델(Generative Models)을 결합하여, 복잡한 질문에 대해 더 정확하고 관련성 높은 답변을 생성하는데 사용된다. RAG 모델은 특히 대규모 텍스트 데이터셋에서 정확한 정보를 검색하고, 이를 기반으로 새로운 텍스트를 생성하여 사용자의 질문에 답변하는 데 효과적이다. 작동방식RAG 모델의 작동 방식은 크게 세 단계로 나눌 수 있다. 검색(Retrieve) : 사용자의 질문이 주어지면, 모델은 대규모 데이터셋에서 질문과 관련된 정보나 문서를 검색한다. 이 과정에서는 주로 효율적인 검색 알고리즘이나 기술이 사용된다. 생성(Generate) : 검색된 정보를 기반으로, 모델은 관련성 높은 답변을 생성한다. 이 단계에서는 생성적 딥러닝 모델, 특히 변형자(Transformer) 기반의 언어 모델이 사용될 수 있다. 순위 매기기(Rank) : 모델이 생성한 여러 답변 중에서 최종 사용자에게 제시될 가장 적합한 답변을 선택하기 위해, 답변들의 순위를 매긴다. 순위 매기기는 답변의 정확성, 관련성, 유용성 등 여러 기준을 고려하여 수행된다. 응용분야RAG 모델은 다양한 NLP 작업에 활용될 수 있으며, 특히 정보가 풍부한 오픈 도메인 질의응답(Open-Domain Question Answering), 자연어 이해(Natural Language Understanding), 챗봇(Chatbots) 등의 분야에서 유용하다. 장점 정확성과 관련성 향상 : RAG 모델은 관련 문서를 검색하여 정보를 기반으로 답변을 생성하기 때문에, 답변의 정확성과 관련성이 향상될 수 있다. 유연성 : 다양한 종류의 데이터와 질문에 적용될 수 있는 높은 유연성을 가진다. 지식 기반 학습 : RAG 모델은 대규모 데이터셋에서 검색한 정보를 활용하여 학습히기 때문에, 지식 기반 학습에 강점을 보인다. 한계 리소스 요구량 : 대규모 데이터셋에서 효율적인 검색과 순위 매기기를 수행하려면 상당한 계산 리소스가 필요할 수 있다. 검색 품질 : 최종 답변의 품질은 검색 단계에서 검색된 문서의 품질에 크게 의존한다. 검색 알고리즘의 성능이 중요하다. RAG는 AI와 NLP 분야에서 중요한 연구주제이며, 지속적인 기술 발전으로 인해 응용 가능성이 확대되고 있다.","link":"/2024/03/26/WhatIsRAG/"},{"title":"선형회귀 알고리즘","text":"지도 학습 알고리즘은 크게 분류와 회귀(regression)으로 나뉜다. 분류는 말 그대로 샘플을 몇 개의 클래스 중 하나로 분류하는 문제이다. 회귀는 클래스 중 하나로 분류하는 것이 아니라 임의의 어떤 숫자를 예측하는 문제이다. 예를 들면 내년도 경제 성장률을 예측하거나 배달이 도착할 시간을 예측하는 것이 회귀 문제이다. 회귀는 정해진 클래스가 없고 임의의 수치를 출력한다. [출처 : 혼자 공부하는 머신러닝+딥러닝 3장. 회귀알고리즘과 모델규제] K-최근접 이웃 회귀(출처 : kNN 최근접 이웃 알고리즘) 녹색 영화는 액션영화일까? 로맨틱 영화일까?녹색 영화는 액션 영화와 로맨틱 영화 가운데 있다.그래서 상당히 답을하기 곤란한 상황이다.현실 세계에서 이게 액션 영화다 로맨틱 영화다 라고 딱 부러지게 얘기하기는 어렵다.이럴 경우에 머신러닝을 사용해 예측갑을 가지고 이야기 할 수가 있다. 그래서 기존의 데이터, 녹색 별을 제외한 기존의 데이터를 중심으로 이 녹색영화가 액션 영화다, 로맨틱 영화다라고 이야기 하는 방법이 knn 알고리즘이다. y축에 보이는 것처럼 발차기 횟수가 많을 경우에는 액션 영화의 가능성이 크고x축의 키스 횟수가 많을때는 로맨틱 영화다라고 볼 수가 있다. 그렇다면 여기서 knn 알고리즘을 간단하게 살펴보도록 하겠다.일단은 k를 정해줘야 한다.k는 최근접점을 우리가 몇개까지 볼것인지 정하는 것이다.일단 k=3으로 쓰겠다. 이것으로 한번 예측값을 내보도록 하겠다. k는 기본적으로 홀수를 쓴다. 왜냐하면 짝수로 쓰면 2:2와 같은 상황이 되어 답을 할 수 없는 상황이 되기 때문이다. 위 그래프에서 보이는 것처럼 최근접 거리에 있어 써클안에 액션 영화가 2개가 있고로맨틱 영화가 하나가 있다.그래서 원 안에 로맨틱 영화보다 액션 영화가 더 많기 때문에 녹색 영화는 액션 영화에요 라고 예측 값을 리턴할 수 있다.이것이 바로 knn 알고리즘의 핵심이다. 그렇다면 최근접점을 프로그램상에서 어떻게 구하는지 보자.피타고라스의 정리를 이용해서 구한다.두 정점의 거리를 구해서 가장 작은 거리의 점들부터 비교를 해나가는 것이다. 결정계수 (R^2)과대적합선형회귀모델 파라미터다항 회귀특성 공학과 규제다중 회귀특성 공학릿지라쏘하이퍼파라미터","link":"/2024/03/27/linearRegression/"},{"title":"AWS Landing Zone 구축하기","text":"“Landing Zone”은 비행기가 안전하게 착륙할 수 있는 공간을 의미한다.즉, AWS Landing Zone을 설계한다는 것은 AWS 상에서 시스테믈 안정적으로 구축하기 위한 기본 사항을 준비하고 설계, 구축하는 일련의 과정이다. Landing Zone 이란?Multi Account의 확장 가능하고 안전한 환경을 제공하는 AWS 솔루션AWS의 컨트롤 타워는 ‘랜딩존’이라는 다중 계정의 AWS 환경을 쉽게 설정하고 관리하는 환경을 제공한다. 클라우드 세상의 컨트롤 타워클라우드를 잘 사용하기 위해서는 고려해야 할 사항이 많다. 성능, 보안, 안정성 등은 기본이고 가격 경쟁력이나 지속 가능성까지도 고려해야한다.서비스의 확장과 함께 탄탄한 아키텍처 기반에 대한 고민을 해결하기 위해 랜딩존을 사용할 수 있다고 한다.즉, 클라우드 환경 또는 확장 가능하고 유연한 아키텍처 설계를 위한 시작점으로써 랜딩존을 사용가능하다고 한다.랜딩 존은 아래와 같은 기능을 제공한다. 다계정 아키텍처가 있는 AWS 환경 초기 보안 기준 ID 및 액세스 관리 지배구조 데이터 보안 네트워크 설계 로그","link":"/2024/04/03/AWS-LandingZone/"},{"title":"로지스틱 회귀","text":"로지스틱 회귀는 선형 방정식을 사용한 분류 알고리즘이다. 시그모이드 함수나 소프트맥스 알고리즘을 사용하여 클래스 확률을 출력할 수 있다. 이 책에서는, 구성품을 모른채 먼저 구매할 수 있는 럭키백이 있다고 가정하고 럭키백을 열어봐야 구성품을 알 수 있다고 한다.럭키백에 들어간 생선의 크기, 무게 등 특성이 주어졌을 때, 어떤 생선인지에 대한 확률을 출력해야 한다. 이를 확인할 수 있는 로지스틱 회귀를 알아보고, 이진분류에 필요한 시그모이드 함수와 다중 분류에 필요한 소프트맥스 함수를 알아본다 [출처 : 혼자 공부하는 머신러닝+딥러닝 4장. 다양한 분류 알고리즘] 데이터 준비하기csv파일을 pandas로 읽어와 타깃 데이터, 입력 데이터로 나눈다. 12345678import pandas as pdfish = pd.read_csv(&quot;http://bit.ly/fish_csv_data&quot;)print(pd.unique(fish['Species'])) # 열에서 고유한 값 출력#fish의 종류를 타깃 데이터, 나머지 특성을 입력 데이터fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()fish_target = fish['Species'].to_numpy() 훈련 세트와 테스트세트로 나눈 후 표준점수로 전처리한다. 1234567891011from sklearn.model_selection import train_test_splitfrom sklearn.preprocessing import StandardScaler#훈련세트와 테스트세트로 나눠주기train_input, test_input, train_target, test_target = train_test_split(fish_input, fish_target, random_state=42)#입력 데이터 전처리ss = StandardScaler()ss.fit(train_input)train_scaled = ss.transform(train_input)test_scaled = ss.transform(test_input) k-최근접 이웃 분류기로 확률 예측사이킷런의 KneighborsClassifier 클래스로 모델을 훈련한다. 123from sklearn.neighbors import KNeighborsClassifierkn = KNeighborsClassifier(n_neighbors=3)kn.fit(train_scaled, train_target) 12&gt;&gt;&gt; print(kn.classes_)['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'] 사이킷런에서는 문자열로 된 타깃값을 그대로 사용할 수 있지만, 순서가 자동으로 알파벳 순서로 매겨진다. 훈련된 모델로 테스트 세트의 5개 샘플의 종류를 예측한다. 12&gt;&gt;&gt; print(kn.predict(test_scaled[:5]))['Perch' 'Smelt' 'Pike' 'Perch' 'Perch'] pridict_proba() 메서드는 클래스별 확률값을 반환한다.Numpy의 round()는 반올림 함수이며 decimals 매개변수는 유지할 소수점 아래 자리를 지정할 수 있다. 123import numpy as np# 클래스별 확률값 반환proba = kn.predict_proba(test_scaled[:5]) 1234&gt;&gt;&gt; print(np.round(proba, decimals=4)) #소숫점 4자리까지 반올림해 반환[[0. 0. 1. 0. 0. 0. 0. ][0. 0. 0. 0. 0. 1. 0. ][0. 0. 0. 1. 0. 0. 0. ][0. 0. 0.6667 0. 0.3333 0. 0. ][0. 0. 0.6667 0. 0.3333 0. 0. ]] 4번째 샘플의 경우 Perch일 확률이 2/3, Roach일 확률이 1/3이다. 1234distances, indexes = kn.kneighbors(test_scaled[3:4])&gt;&gt;&gt; print(train_target[indexes])[['Roach' 'Perch' 'Perch']] 4번째 샘플의 이웃은 Perch가 2개, Roach가 1개로, 구한 확률이 맞음을 보여준다. 단, 이 방법은 3개의 최근접 이웃만을 사용하기에 확률은 0, 1/3, 2/3, 1 뿐이라는 한계가 있다. 로지스틱 회귀로지스틱 회귀는 이름은 회귀이지만 분류 모델이다.선형 회귀와 동일하게 선형 방정식을 학습한다.z = a x Weight + b x length + ··· + fa, b, c, d, e는 계수이며 z는 어떤 값도 될 수 있다. 하지만 확률로 표현하려면 0~1 사이의 값이 되어야 하기 때문에z가 아주 큰 음수일때 0이 되고, 아주 큰 양수일 때 1이 되도록 바꾼다.이는 시그모이드 함수를 사용하면 가능하다. 시그모이드 함수 넘파이를 이용해서 간단하게 그려본다. 1234567import matplotlib.pyplot as pltz = np.arange(-5, 5, 0.1)phi = 1 / (1 + np.exp(-z))plt.plot(z, phi)plt.xlabel('z')plt.ylabel('phi')plt.show() 이진 분류를 먼저 수행해 볼 것이다.이진 분류에서 시그모이드 출력이 0.5보다 크면 양성클래스, 작으면 음성클래스로 판단한다. 로지스틱 회귀로 이진분류 수행하기불리언 인덱싱으로 도미와 빙어 데이터를 골라낸다. 123bream_smelt_indexes = (train_target == &quot;Bream&quot;) | (train_target == &quot;Smelt&quot;)train_bream_smelt = train_scaled[bream_smelt_indexes]target_bream_smelt = train_target[bream_smelt_indexes] 이 데이터로 로지스틱 회귀 모델을 훈련한다. 123from sklearn.linear_model import LogisticRegressionlr = LogisticRegression()lr.fit(train_bream_smelt, target_bream_smelt) 훈련한 모델로, 5개의 테스트 샘플을 예측해본다. 12&gt;&gt;&gt; print(lr.predict(train_bream_smelt[:5]))['Bream' 'Smelt' 'Bream' 'Bream' 'Bream'] 각각의 샘플 확률을 예측해본다. 1234&gt;&gt;&gt; print(lr.predict_proba(train_bream_smelt[:5]))[[0.99759855 0.00240145][0.02735183 0.97264817][0.99486072 0.00513928][0.98584202 0.01415798][0.99767269 0.00232731]] 로지스틱 회귀가 학습한 계수도 볼 수 있다. 12&gt;&gt;&gt; print(lr.coef_, lr.intercept_)[[-0.4037798 -0.57620209 -0.66280298 -1.01290277 -0.73168947]][-2.16155132] 계수들과 절편을 볼 수 있다. 따라서 이 로지스틱 회귀 모델이 학습한 방정식은 다음과 같다.z = -0.404 x 무게 + -0.576 x 길이 + ··· + -2.161 z값과 시그노이드 함수의 값 또한 볼 수 있다. 12345decisions = lr.decision_function(train_bream_smelt[:5])print(decisions)from scipy.special import expitprint(expit(decisions)) 1[-6.02927744 3.57123907 -5.26568906 -4.24321775 -6.0607117 ] [0.00240145 0.97264817 0.00513928 0.01415798 0.00232731] 로지스틱 회귀로 다중 분류 수행하기LogisticRegression 클래스는 반복적인 알고리즘을 사용하며, max_iter 매개변수에서 반복값을 지정하며 기본값은 100 이다. 또한 릿지 회귀와 같이 계수의 제곱을 규제하며, L2 규제라고도 불린다. 릿지회귀에서 alpha로 규제의 양을 조절한 것과 달리, C 매개변수로 조절한다. C의 기본값은 1이며 작을수록 규제가 커진다. 123456lr = LogisticRegression(C=20, max_iter=1000)lr.fit(train_scaled, train_target)print(lr.score(train_scaled, train_target))print(lr.score(test_scaled, test_target))print(lr.predict(test_scaled[:5])) # 샘플 5개의 종류 예측 1230.93277310924369750.925['Perch' 'Smelt' 'Pike' 'Roach' 'Perch'] 과대적합이나 과소적합이 되지 않았다. 5개 샘플에 대한 예측 샘플도 볼 수 있다. 123print(lr.classes_)proba = lr.predict_proba(test_scaled[:5])print(np.round(proba, decimals=3)) 123456['Bream' 'Parkki' 'Perch' 'Pike' 'Roach' 'Smelt' 'Whitefish'][[0. 0.014 0.841 0. 0.136 0.007 0.003] [0. 0.003 0.044 0. 0.007 0.946 0. ] [0. 0. 0.034 0.935 0.015 0.016 0. ] [0.011 0.034 0.306 0.007 0.567 0. 0.076] [0. 0. 0.904 0.002 0.089 0.002 0.001]] 클래스 정보와 클래스 예측 확률을 볼 수 있다. 다중 분류에서의 예측 확률은 소프트맥스 함수를 사용하여 7개의 z값을 확률로 변환한다. 소프트맥스 함수소프트 맥스 함수는 다중 분류에서 여러 선형 방정식의 출력 결과를 정규화하여 합이 1이 되도록 만든다. 수식 넣을 방법 필요…","link":"/2024/03/28/Logistic-Regression/"},{"title":"개체명 인식(Named Entity Recognition)","text":"NER(Named Entity Recognition)은 자연어 처리(Natural Language Processing, NLP)의 한 영역으로, 텍스트에서 사람, 조직, 위치, 날짜, 시간, 통화, 비율과 같은 명명된 엔티티(명사)를 식별하고 분류하는 기술이다. NER 시스템은 주어진 문서에서 중요한 정보를 추출하는 데 사용되며, 정보 검색, 질의 응답 시스템, 콘텐츠 요약, 고객 지원 시스템, 그리고 감정 분석 등 다양한 NLP 응용 프로그램에 활용된다. NER의 주요 기능: 엔티티 식별: 텍스트 데이터에서 사람의 이름, 기업 이름, 지리적 위치 등과 같은 엔티티를 식별합니다. 엔티티 분류: 식별된 엔티티를 사전 정의된 카테고리(예: 인물, 조직, 위치 등)로 분류합니다. 관계 추출: 텍스트 내에서 엔티티 간의 관계를 파악하고 추출합니다. 이는 텍스트 내에서 엔티티 간의 상호 작용을 이해하는 데 도움이 됩니다. NER의 활용 사례: 정보 추출: 뉴스 기사, 소셜 미디어 포스트, 문서에서 중요한 정보를 추출하여 구조화된 데이터로 변환합니다. 문서 분류: 엔티티 정보를 기반으로 문서를 자동으로 분류하고 정리합니다. 지능형 검색 엔진: 특정 엔티티에 관한 정보를 효율적으로 검색하기 위해 사용됩니다. 챗봇과 가상 비서: 사용자 질문에서 중요한 엔티티를 식별하여 보다 정확한 답변을 제공합니다. 감성 분석: 제품, 서비스, 브랜드에 대한 특정 엔티티의 언급을 분석하여 고객의 의견과 태도를 이해합니다.NER 기술의 발전은 딥러닝과 인공 신경망 모델의 진보 덕분에 크게 향상되었습니다. 이러한 모델들은 맥락과 의미를 더 잘 이해할 수 있게 해주며, NER의 정확도와 효율성을 높여줍니다.","link":"/2024/04/03/NER/"},{"title":"비지도학습","text":"지도 학습과는 달리 정답 라벨이 없는 데이터를 비슷한 특징끼리 군집화하여 새로운 데이터에 대한 결과를 예측하는 방법을 비지도학습이라고 한다.라벨링 되어있지 않은 데이터로부터 패턴이나 형태를 찾아야 하기 때문에 지도학습보다는 조금 더 난이도가 있다고 할 수 있다.실제로 지도 학습에서 적절한 피처를 찾아내기 위한 전처리 방법으로 비지도 학습을 이용하기도 한다. [출처 : 혼자 공부하는 머신러닝+딥러닝 6장. 비지도 학습] 비지도학습비지도학습의 대표적인 종류는 클러스터링(Clustering)이 있다. 이 외에도 Dimentionality Reduction, Hidden Markov Model이 있다.예를 들어 여러 과일의 사진이 있고 이 사진이 어떤 과일의 사진인지 정답이 없는 데이터에 대해 색깔이 무엇인지, 모양이 어떠한지 등에 대한 피러르 토대로 바나나다, 사과다 등으로 군집화 하는 것이다. 지도/비지도 학습 모델(Semi-Supervised Learning)을 섞어서 사용할 수도 있다. 소량의 분류된 데이터를 사용해 분류되지 않은 더 큰 데이터 세트를 보강하는 방법으로 활용할 수도 있다. 최근 각광받고 있는 GAN(generative Adversarial Network) 모델도 비지도 학습에 해당한다. 과일 분류하기 예시1!wget https://bit.ly/fruits_300_data -O fruits_300.npy 코랩의 코드 셀에서 ‘!’ 문자로 시작하면 코랩은 이후 명령을 파이썬 코드가 아니라 리눅스 쉘 명령으로 이해한다. wget 명령은 원격 주소에서 데이터를 다운로드하여 저장한다. 1234import numpy as npimport matplotlib.pyplot as pltfruits = np.load('fruits_300.npy') npy 파일을 load() 메서드를 이용하여 로드한다. 12&gt;&gt;&gt; print(fruits.shape)(300, 100, 100) 첫 번째 차원(300)은 샘플의 개수 두 번째 차원(100)은 이미지 높이, 세 번째 차원(100)은 이미지 너비 1234567&gt;&gt;&gt; print(fruits[0, 0, :])[ 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 2 2 1 1 1 1 1 1 1 1 2 3 2 1 2 1 1 1 1 2 1 3 2 1 3 1 4 1 2 5 5 5 19 148 192 117 28 1 1 2 1 4 1 1 3 1 1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 첫 번째 해에 있는 픽셀 100개에 들어 있는 값을 출력하면 위와 같다.이 넘파이 배열은 흑백 사진을 담고 있으므로 0~255까지의 정숫값을 가진다. 이 첫 번째 이미지를 배열과 비교하기 위해 그림으로 그리면 아래와 같다. 12plt.imshow(fruits[0], cmap='gray')plt.show() cmap : 사용할 컬러의 스케일을 지정해줄 수 있음 우리가 보는 것과 컴퓨터가 처리하는 방식이 다르기 때문에 위와 같이 흑백 이미지를 반전하여 사용한다.cmap 매개변수를 ‘gray_r’로 지정하면 다시 반전하여 우리 눈에 보기 좋게 출력 가능하다. 12plt.imshow(fruits[0], cmap='gray_r')plt.show() 이 그림에서 밝은 부분은 0에 가깝고 짙은부분은 255에 가깝다. 픽셀값 분석하기로드 한 데이터의 처음 100개는 사과, 그다음 100개는 파인애플, 마지막 100개는 바나나이다.각 과일 사진의 평균을 내서 차이를 확인해보겠다.사용하기 쉽게 fruits 데이터를 사과, 파인애플, 바나나로 각각 나눠 보겠다. 123apple = fruits[0:100].reshape(-1, 100*100)pineapple = fruits[100:200].reshape(-1, 100*100)banana = fruits[200:300].reshape(-1, 100*100) reshape() 메서드를 사용해 두 번째 차원(100)과 세 번째 차원(100)을 10,000으로 합친다. 첫 번째 차원을 -1로 지정하면 자동으로 남은 차원을 할당한다. 이제 apple, pineapple, banana 배열의 크기는 100, 10000)이다. 각 배열에 들어 있는 샘플의 픽셀 평균값을 계산하기 위해 mean() 메서드를 사용하겠다.샘플마다 픽셀의 평균값을 계산해야 하므로 mean() 메서드가 평균을 계산할 축을 지정해야 한다.axis=0으로 하면 첫 번째 축인 행을 따라 계산한다.axis=1로 지정하면 두 번째 축인 열을 따라 계산한다. 1234567891011121314&gt;&gt;&gt; print(apple.mean(axis=1))[ 88.3346 97.9249 87.3709 98.3703 92.8705 82.6439 94.4244 95.5999 90.681 81.6226 87.0578 95.0745 93.8416 87.017 97.5078 87.2019 88.9827 100.9158 92.7823 100.9184 104.9854 88.674 99.5643 97.2495 94.1179 92.1935 95.1671 93.3322 102.8967 94.6695 90.5285 89.0744 97.7641 97.2938 100.7564 90.5236 100.2542 85.8452 96.4615 97.1492 90.711 102.3193 87.1629 89.8751 86.7327 86.3991 95.2865 89.1709 96.8163 91.6604 96.1065 99.6829 94.9718 87.4812 89.2596 89.5268 93.799 97.3983 87.151 97.825 103.22 94.4239 83.6657 83.5159 102.8453 87.0379 91.2742 100.4848 93.8388 90.8568 97.4616 97.5022 82.446 87.1789 96.9206 90.3135 90.565 97.6538 98.0919 93.6252 87.3867 84.7073 89.1135 86.7646 88.7301 86.643 96.7323 97.2604 81.9424 87.1687 97.2066 83.4712 95.9781 91.8096 98.4086 100.7823 101.556 100.7027 91.6098 88.8976] 사과 샘플 100개에 대한 픽셀 평균값을 계산한 것이다. 히스토그램을 그려보면 평균값이 어떻게 분포되어 있는지 한눈에 볼 수 있다. 히스토그램이란히스토그램은 값이 발생한 빈도를 그래프로 표시한 것이다. 보통 x축이 값의 구간(계급)이고, y축은 발생 빈도(도수)이다. 12345plt.hist(np.mean(apple, axis=1), alpha=0.8)plt.hist(np.mean(pineapple, axis=1), alpha=0.8)plt.hist(np.mean(banana, axis=1), alpha=0.8)plt.legend(['apple', 'pineapple', 'banana'])plt.show() 사과와 파인애플은 90~100 사이에 많이 모여있다. 바나나는 픽셀 평균값만으로 사과나 파인애플과 확실히 구분된다. 사과와 파인애플은 많이 겹쳐져있어서 픽셀값만으로는 구분하기 쉽지 않다. 해결책으로 샘플의 평균값이 아니라 픽셀별 평균값을 비교하는 방법이 있다.전체 샘플에 대해 각 픽셀의 평균을 계산하는 것이다.픽셀의 평균을 계산하는 것은 axis=0으로 지정하면 된다. 12345fig, axs = plt.subplots(1, 3, figsize=(20, 5))axs[0].bar(range(10000), np.mean(apple, axis=0))axs[1].bar(range(10000), np.mean(pineapple, axis=0))axs[2].bar(range(10000), np.mean(banana, axis=0))plt.show() 순서대로 사과, 파인애플, 바나나 그래프이다. 각 과일마다 값이 높은 구간이 다르다. 픽셀 평균값을 100*100 크기로 바꿔서 이미지처럼 출력하여 위 그래프와 비교하면 더 좋다.픽셀을 평균 낸 이미지를 모든 사진을 합쳐 놓은 대표 이미지로 생각할 수 있다. 123456789apple_mean = np.mean(apple, axis=0).reshape(100, 100)pineapple_mean = np.mean(pineapple, axis=0).reshape(100, 100)banana_mean = np.mean(banana, axis=0).reshape(100, 100)fig, axs = plt.subplots(1, 3, figsize=(20, 5))axs[0].imshow(apple_mean, cmap='gray_r')axs[1].imshow(pineapple_mean, cmap='gray_r')axs[2].imshow(banana_mean, cmap='gray_r')plt.show() 세 과일은 픽셀 위치에 따라 값의 크기가 차이난다.이 대표 이미지와 가까운 사진을 골라낸다면 사과, 파인애플, 바나나를 구분할 수 있을 것이다. 이처럼 흑백 사진에 있는 픽셀값을 사용해 과일 사진을 모으는 작업을 해 보았다. 이렇게 비슷한 샘플끼리 그룹으로 모으는 작업을 군집(clustering)이라고 한다. 군집은 대표적인 비지도 학습 작업 중 하나이고, 군집 알고리즘에서 만든 그룹을 클러스터(cluster)라고 부른다. k-means앞에서는 사과, 파인애플, 바나나에 있는 각 픽셀의 평균값을 구해서 가장 가까운 사진을 골랐다. 이 경우에는 사과, 파인애플, 바나나 사진임을 미리 알고 있었기 때문에 각 과일의 평균을 구할 수 있었다. 하지만 진짜 비지도 학습에서는 사진에 어떤 과일이 들어 있는지 알지 못한다.이런 경우 어떻게 평균값을 구할 수 있을까? 바로 k-평균(k-means) 군집 알고리즘이 평균값을 자동으로 찾아준다. k-means 알고리즘 작동방식 무작위로 k개의 클러스터 중심을 정한다. 각 샘플에서 가장 가까운 클러스터 중심을 찾아 해당 클러스터의 샘플로 지정한다. 클러스터에서 속한 샘플의 평균값으로 클러스터 중심을 변경한다. 클러스터 중심에 변화가 없을 때까지 2번으로 돌아가 반복한다. k-means 모델 만들기1. 데이터 준비하기1234!wget https://bit.ly/fruits_300_data -O fruits_300.npyimport numpy as npfruits = np.load('fruits_300.npy')fruits_2d = fruits.reshape(-1, 100*100) 준비된 넘파이 배열을 100*10000 크기로 재배열한다. 2. k-means 알고리즘으로 모델 학습하기사이킷런의 k-평균 알고리즘은 sklearn.cluster 모듈에 KMeans 클래스가 구현되어 있다.n-cluster 매개변수로 클러스터 갯수를 지정할 수 있다.3개로 지정 후 모델을 훈련시킨다. 123from sklearn.cluster import KMeanskm = KMeans(n_clusters=3, random_state=42)km.fit(fruites_2d) 군집된 결과는 KMeans 객체의 labels_ 속성된 결과에 저장된다.클러스터 갯수가 3이기 때문에 배열의 값은 0,1,2 중 하나이다.단, 레이블값과 순서에 의미는 없다. 12345678910&gt;&gt;&gt; print(km.labels_)[2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 0 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 0 0 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1] 이를 통해 각 클러스터의 샘플의 갯수를 알 수 있다. 12&gt;&gt;&gt; print(np.unique(km.labels_, return_counts=True))(array([0, 1, 2], dtype=int32), array([111, 98, 91])) 3. 각 클러스터의 그림 출력하기각 클러스터가 어떤 이미지를 나타냈는지 그림으로 출력하기 위해 간단한 유틸리티 함수를 만들어본다. 12345678910111213141516import matplotlib.pyplot as pltdef draw_fruits(arr, ratio=1): n = len(arr) # n은 샘플 개수입니다 # 한 줄에 10개씩 이미지를 그립니다. 샘플 개수를 10으로 나누어 전체 행 개수를 계산합니다. rows = int(np.ceil(n/10)) # 행이 1개 이면 열 개수는 샘플 개수입니다. 그렇지 않으면 10개입니다. cols = n if rows &lt; 2 else 10 fig, axs = plt.subplots(rows, cols, figsize=(cols*ratio, rows*ratio), squeeze=False) for i in range(rows): for j in range(cols): if i*10 + j &lt; n: # n 개까지만 그립니다. axs[i, j].imshow(arr[i*10 + j], cmap='gray_r') axs[i, j].axis('off') plt.show() draw_fruits()는 (샘플갯수, 너비, 높이)의 3차원 배열을 받아 가로로 10개의 이미지를 출력하는 함수이다. figsize는 ratio 매개변수에 비례하여 커진다. draw_fruits()에 fruits 배열을 불리언 인덱싱을 통해 넣어준다. 1draw_fruits(fruits[km.labels_==0]) 레이블 0에는 파인애플과 바나나, 사과가 섞여있는 것을 볼 수 있다. k-means 알고리즘이 이 샘플들을 완벽하게 분류하진 못했지만, 비슷한 샘플을 잘 모은 것을 볼 수 있다. 사과를 완벽하게 분류 했다. 4. 클러스터 중심KMeans 클래스가 최종적으로 찾은 클러스터 중심은 cluser_centers_ 속성에 저장되어 있다.이를 그림으로 표현해보면 아래와 같다. 1draw_fruits(km.cluster_centers_.reshape(-1, 100, 100), ratio=3) 이전에 각 과일의 평균 픽셀값을 출력했던 것과 비슷함을 확인할 수 있다. 훈련 데이터 샘플에서 클러스터 중심까지 거리로 변환해주는 transfor() 메서드와, 데이터를 예측하는 predict() 메서드가 있다.클러스터 중심이 가장 가까운 것이 예측 클래스로 출력된다. 12345&gt;&gt;&gt; print(km.transform(fruits_2d[100:101]))[[3393.8136117 8837.37750892 5267.70439881]]&gt;&gt;&gt; print(km.predict(fruits_2d[100:101]))[0] k-means 알고리즘은 클러스터 중심을 옮기면서 최적의 클러스터를 찾는 과정을 반복하는데, 알고리즘이 반복한 횟수는 n_iter_에 저장된다. 12&gt;&gt;&gt; print(km.n_iter_)4 최적의 k 찾기 : 엘보우 방법k-means 알고리즘의 단점 중 하나는, 클러스터 갯수를 사전에 지정해야 한다는 것이다.군집 알고리즘에서 적절한 k값을 찾는 완벽한 방법은 없다. 저마다 장단점이 있지만 가장 대표적인 엘보우 방법을 알아보겠다. k-means 알고리즘은 클러스터 중심과 클러스터에 속한 샘플 사이의 거리를 잴 수 있는데, 이것의 제곱합을 이너셔(inertia)라고 한다. 이너셔는 클러스터에 속한 샘플이 얼마나 가깝게 모여있는지를 나타내는 값인데, 클러스터 개수가 늘어나면 이너셔도 줄어든다. 엘보우 방법은 클러스터 갯수를 늘려가면서 이너셔의 변화를 관찰하여 최적의 클러스터를 찾는 방법이다. 클러스터 갯수에 대한 이너셔를 그래프로 그리면 꺽이는 지점이 있는데, 그 지점이 바로 적절한 클러스터 갯수이다.KMeans 클래스는 자동으로 이너셔를 계산해서 inertia_ 속성으로 제공한다. 123456789inertia = []for k in range(2, 7): km = KMeans(n_clusters=k, random_state=42) km.fit(fruits_2d) inertia.append(km.inertia_)plt.plot(range(2, 7), inertia)plt.xlabel('k')plt.ylabel('inertia')plt.show() 주성분 분석차원 축소주성분 분석설명된 분산","link":"/2024/04/01/Unsupervised-Learning/"},{"title":"인공 신경망 (Artificial Neural Network)","text":"인공 신경망(Artificial Neural Network, ANN)은 인간의 뇌 구조와 기능을 모방하여 만들어진 컴퓨팅 시스템이다. 인공 신경망은 데이터를 처리하고 학습하는데 사용되며, 주로 패턴 인식, 분류, 예측 등과 같은 다양한 머신러닝과 딥러닝 문제를 해결하는 데 활용한다. 생물학적 뉴런에서 영감 받아 만든 머신러닝 알고리즘이지만, 실제 우리 뇌를 모델링한 것은 아니다. 신경망은 기존 머신러닝 알고리즘으로 다루기 어려웠던 이미지, 음성, 텍스트 분야에서 뛰어난 성능을 발휘하면서 크게 주목받고 있다. 인공 신경망 알고리즘을 종종 딥러닝이라고 부른다. [출처 : 혼자 공부하는 머신러닝+딥러닝 7장. 인공 신경망] 1. 패션 MNIST딥러닝을 배울 때 많이 쓰는 데이터셋이 MNIST 데이터셋이다.10종류의 패션 아이템으로 구성되어있다. keras.datasets.fashion_mnist모듈 아래 load_data() 함수로 훈련 데이터와 테스트 데이터를 얻을 수 있다. 12345from tensorflow import keras(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()print(train_input.shape, traion_taret.shape)print(test_input.shape, test_target.shape) 12(60000, 28, 28) (60000,)(10000, 28, 28) (10000,) 28*28 사이즈의 이미지가 훈련세트에는 6만개, 테스트세트에는 1만개 들어있다. 훈련 데이터에서 몇 개의 샘플을 그림으로 표현해본다. 123456import matplotlib.pyplot as pltfig, axs = plt.subplots(1, 10, figsize=(10,10))for i in range(10): axs[i].imshow(train_input[i], cmap='gray_r') axs[i].axis('off')plt.show() 12import numpy as npprint(np.unique(train_target, return_counts=True)) 1(array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8), array([6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000, 6000])) 12각 레이블 :0 티셔츠, 1 바지, 2 스웨터, 3 드레스, 4 코트, 5 샌달, 6 셔츠, 7 스니커즈, 8 가방, 9 앵클부츠 각 레이블마다 6000개의 샘플이 들어있다. 2. 로지스틱 회귀로 패션 아이템 분류하기이 훈련 샘플은 60,000개나 되기 때문에, 전체 데이터를 한꺼번에 사용하여 모델을 훈련시키기보다, 샘플을 하나씩 꺼내서 모델을 훈련하는 방법이 효율적이다. 이 상황에서 가장 잘 맞는 방법은 확률적 경사 하강법이다.SGDClassifier 클래스의 매개변수 loss='log'로 지정하여 확률적 경사 하강법 모델을 만든다. 2-1. 데이터를 표준화 전처리하기확률적 경사 하강법은 여러 특성 중 기울기가 가장 가파른 방향을 따라 이동한다. 특성마다 값의 범위가 다르다면 올바르게 손실 함수의 경사를 내려올 수 없다. 이 데이터셋의 각 픽셀은 0255 사이의 정수값을 가지기 때문에, 255로 나누어 01 사이의 값으로 정규화한다. 12train_scaled = train_input / 255.0train_scaled = train_scaled.reshape(-1, 28*28) SGDClassifier는 2차원 입력을 다루지 못하기 때문에 각 샘플을 1차원 배열로 만들어야 한다. reshape() 메서드의 두 번째 매개변수를 28*28 이미지 크기에 맞게 지정하면 척 번째 차원은 변하지 않고, 원본 데이터의 두 번째, 세 번째 차원이 1차원으로 합쳐진다. 1print(train_scaled.shape) 1(60000, 784) 2-2. 모델을 만들고 교차검증하기SGDClassifier 클래스와 cross_validate()로 교차검증을 진행한다. 123456from sklearn.model_selection import cross_validatefrom sklearn.linear_model import SGDClassifiersc = SGDClassifier(loss='log_loss', max_iter=5, random_state=42)scores = cross_validate(sc, train_scaled, train_traget, n_jobs=-1)print(np.mean(scores['test_score'])) 10.8196000000000001 2-3. 로지스틱 회귀 공식 복습앞서 배운 로지스틱 회귀 공식은 이렇다.z = a * weight + b * length + ... + f 이를 MNIST 데이터셋에 적용하면 이렇다.z_티셔츠 = w1 * 픽셀1 + w2 * 픽셀2 + ... + w784 * 픽셀784 + bz_바지 = w1' * 픽셀1 + w2' * 픽셀2 + ... + w784' * 픽셀784 + b' 이와 같이 10개의 클래스에 대한 선형 방정식을 구한 뒤, 소프트맥스 함수를 통과하여 각 클래스에 대한 확률을 얻을 수 있다. 2-4. SGDClassifier 복습SGDClassifier는 사이킷런(Scikit-learn) 라이브러리에서 제공하는 확률적 경사 하강법(Stochastic Gradient Descent, SGD)을 사용한 선형 분류기이다. “SGD”는 반복적으로 주어진 데이터셋을 작은 배치로 나누어 모델의 가중치를 업데이트하는 최적화 알고리즘을 말하며, 이를 통해 분류 또는 회귀 문제를 해결할 수 있다. SGDClassifier는 특히 대규모 데이터셋에 대한 선형 분류 문제에 효과적인 방법으로 널리 사용된다. 이 클래스는 다양한 선형 모델, 예를 들어 로지스틱 회귀(Logistic Regression), 선형 서포트 벡터 머신(Linear Support Vector Machine) 등을 구현할 수 있도록 지원한다. 주요 기능 및 특징: 효율성: 대규모 데이터셋에 대해 효율적인 학습이 가능하다. 유연성: 다양한 손실 함수(Loss Function)를 지원하며, 이를 통해 다양한 선형 분류 문제를 해결할 수 있다. 예를 들어, loss=&quot;hinge&quot;는 선형 SVM을, loss=&quot;log&quot;는 로지스틱 회귀를 의미합니다. 정규화: l2, l1, elasticnet과 같은 정규화 옵션을 제공하여 모델의 복잡도를 조절하고, 과적합을 방지할 수 있다. 온라인 학습 지원: partial_fit 메서드를 사용하여 데이터가 순차적으로 도착할 때 모델을 점진적으로 업데이트할 수 있다. 사용 예시:123456789101112from sklearn.linear_model import SGDClassifierfrom sklearn.datasets import make_classification# 예제 데이터 생성X, y = make_classification(n_samples=1000, n_features=20, random_state=42)# SGDClassifier 인스턴스 생성 및 학습clf = SGDClassifier(loss=&quot;hinge&quot;, penalty=&quot;l2&quot;, max_iter=1000)clf.fit(X, y)# 예측predictions = clf.predict(X[:5]) 이 코드는 먼저 가상의 분류 데이터셋을 생성하고, SGDClassifier를 사용하여 선형 SVM 모델 (loss-&quot;hinge&quot;)을 학습한 뒤, 몇 개의 샘플에 대해 예측을 수행한다.SGDClassifier는 특히 대규모 데이터셋을 다룰 때 그 장점이 두드러지며, 다양한 선형 분류 문제에 적용될 수 있다. 3. 인공신경망으로 모델 만들기3-1. 인공 신경망 가장 간단한 인공 신경망은 출력층 하나가 있는 인공 신경망이다.확률적 경사 하강법을 사용하는 로지스틱 회귀와 같다.보통 인공 신경망을 이야기 할 때나 딥러닝을 이야기 할 때는 출력층 하나가 아니라 더 많은 층이 있는 경우를 이야기한다. 입력층 : 픽셀값 자체이고, 특별한 계산을 수행하지 않는다. 출력층 : z1 ~ z10을 계산하고 이를 바탕으로 클래스를 예측 뉴런 : z값을 계산하는 단위, 뉴런에서 일어나는 일은 선형 계산이 전부이다. 이제는 뉴런이란 표현 대신 유닛(unit)이라고 부르는 사람이 더 많다. 3-2. 텐서플로우와 케라스 텐서플로우 : 구글이 2015년 11월 오픈소스로 공개한 딥러닝 라이브러리케라스 : 텐서플로의 고수준 API 딥러닝 라이브러리가 다른 머신러닝 라이브러리와 다른 점 중 하나는 그래픽 처리 장치인 GPU를 사용하여 인공 신경망을 훈련한다는 것이다. GPU는 벡터와 행렬 연산에 매우 최적화되어 있기 때문에 곱셈과 덧셈이 많이 수행되는 인공 신경망에 큰 도움이 된다. 케라스 라이브러리는 직접 GPU 연산을 수행하지 않는다. 대신 GPU 연산을 수행하는 다른 라이브러리를 백엔드로 사용한다. 예를 들면 텐서플로가 케라스의 백엔드 중 하나이다. 이런 케라스를 멀티-백엔드 케라스라고 부른다. 케라스 API만 익히면 다양한 딥러닝 라이브러리를 입맛대로 골라서 쓸 수 있다. 3-3. 케라스 모델 만들기인공신경망에서는 교차 검증을 잘 사용하지 않고, 검증 세트를 별도로 덜어 내어 사용한다. 데이터셋이 충분히 크고, 교차검증에는 오랜시간이 걸리기 때문이다. 12345from sklearn.model_selection import train_test_splittrain_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)print(train_scaled.shape, train_target.shape) #훈련세트print(val_scaled.shape, val_target.shape) #검증세트 12(48000, 784) (48000,)(12000, 784) (12000,) 가장 기본이 되는 층인 밀집층을 만들어본다.입력층은 784개의 뉴런으로 구성되며, 출력층은 10개의 뉴런으로 구성된다.밀집층은 각 뉴런이 모두 연결되어야 하기 때문에, 784*10 = 7840개의 선이 포함된다.이를 완전 연결층이라고도 부른다. 12dense = keras.layers.Dense(10, activation='softmax', input_shape=(784, ))model = keras.Sequential(dense) # 밀집층을 가진 신경망 모델 뉴런의 갯수를 10으로 지정하고, 10개의 뉴런에서 출력되는 값을 확률로 바꾸기 위해 소프트맥스 함수를 이용한다. 만약 이진분류라면 activation='sigmoid'로 입력한다.이후 밀집층을 가진 신경망 모델을 만들기 위해 Sequential클래스를 사용한다. 소프트맥스와 같이 뉴런의 선형방정식 계산 결과에 적용되는 함수를 활성화 함수라고 부른다. 케라스 모델은 훈련하기 전 설정 단계가 있다. 1model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy') loss매개변수에 이진 분류라면 binary_crossentropy, 다중 분류라면 categorical_crossentropy를 사용한다. 정수로 된 타깃값을 원-핫 인코딩으로 바꾸지 않고 사용하려면 loss='sparse_categorical_crossentropy',타깃값을 원-핫 인코딩으로 준비했다면 loss='categorical_crossentropy'으로 지정한다. metrics 매개변수는 accuracy를 지정하면 에포크마다 정확도를 함께 출력해준다. 1model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 3s 1ms/step - loss: 0.6125 - accuracy: 0.7901Epoch 2/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4786 - accuracy: 0.8392Epoch 3/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4556 - accuracy: 0.8475Epoch 4/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4452 - accuracy: 0.8512Epoch 5/51500/1500 [==============================] - 2s 1ms/step - loss: 0.4376 - accuracy: 0.8547 epochs 매개변수로 에포크 횟수를 지정할 수 있다.evaluate() 메서드로 모델의 성능을 평가해본다. 1model.evaluate(val_scaled, val_target) 12375/375 [==============================] - 1s 3ms/step - loss: 0.4630 - accuracy: 0.8458[0.46303632855415344, 0.8458333611488342] fit() 메서드와 비슷한 출력을 보여준다.","link":"/2024/04/02/Artificial-Neural-Network/"},{"title":"심층 신경망(Deep Neural Network)","text":"심층 신경망(Deep Neural Network, DNN)은 여러 개의 은닉층을 포함하는 인공 신경망의 한 종류이다. 인공 신경망은 입력층(input layer), 하나 이상의 은닉층(hidden layers), 그리고 출력층(output layer)으로 구성되며, 이 중에서 은닉층이 여러 개인 경우를 심층 신경망이라고 한다. 심층 신경망은 복잡한 데이터에서 높은 수준의 추상화와 패턴 인식을 수행할 수 있으며, 이미지 인식, 자연어 처리, 음성 인식 등 다양한 분야에서 광범위하게 활용된다. [출처 : 혼자 공부하는 머신러닝+딥러닝 7장. 인공 신경망] 심층 신경망의 특징: 다층 구조: 심층 신경망은 두 개 이상의 은닉층을 가진다. 은닉층의 수가 많을수록 네트워크는 더 복잡한 패턴과 관계를 학습할 수 있다. 비선형성: 심층 신경망은 비선형 활성화 함수를 사용하여 입력 데이터의 비선형 특성을 모델링한다. 이를 통해 선형 모델로는 표현할 수 없는 복잡한 패턴을 학습할 수 있다. 자동 특성 추출: 심층 신경망은 데이터로부터 중요한 특성을 자동으로 학습하고 추출할 수 있다. 이는 수동으로 특성을 설계하는 작업을 줄여준다. 범용 근사자: 이론적으로 심층 신경망은 어떤 함수도 근사할 수 있는 범용 함수 근사자(universal function approximator)로 간주된다. 활용 분야: 컴퓨터 비전: 이미지 분류, 객체 탐지, 이미지 생성 등에 활용된다. 자연어 처리: 기계 번역, 감성 분석, 텍스트 요약 등의 작업에 사용된다. 음성 인식: 음성을 텍스트로 변환하거나, 음성 명령을 인식하는 데 사용된다. 게임 및 로봇 공학: 자율 주행, 게임 AI, 로봇의 의사 결정 등에 활용된다. 도전 과제: 과적합(Overfitting): 모델이 훈련 데이터에 지나치게 최적화되어 새로운 데이터에 대한 일반화 능력이 떨어질 수 있다. 해석성(Interpretability): 심층 신경망의 결정 과정이 “블랙 박스”처럼 보일 수 있어, 모델의 예측을 해석하기 어려울 수 있다. 계산 비용: 심층 신경망의 학습은 대량의 데이터와 고성능의 컴퓨팅 자원을 요구한다. 이제 여러 개의 층을 추가하여 다층 인공 신경망, 즉 심층 신경망을 만들고, 은닉층에 사용하는 활성화 함수인 렐루 함수, 가중치와 절편을 학습하기 위한 옵티마이저를 알아본다. 1. 데이터 준비데이터를 표준화 전처리하고 훈련세트와 검증세트로 나눈다. 123456789from tensorflow import kerasfrom tensorflow import kerasfrom sklearn.model_selection import train_test_split(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()train_scaled = train_input / 255.0train_scaled = train_scaled.reshape(-1, 28*28)train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42) 2. 시그모이드 함수로 밀집층 추가하기인공 신경망과 달리, 입력층과 출력층 사이에 밀집층을 추가한다. 이를 은닉층이라고 한다. 2-1. 은닉층의 활성화 함수 : 시그모이드인공 신경망에서 출력층에 적용했던 소프트맥스 함수도 활성화 함수이다.단 출력층에서는 보통 이진 분류에서는 시그모이드 함수, 다중 분류에서는 소프트맥스를 사용한다.은닉층에도 활성화 함수가 적용되는데, 대표적으로 시그모이드 함수와 볼 렐루 함수가 있다. 은닉층 활성화 함수를 적용하는 이유는 선형 계산을 비선형으로 비틀어 주어 다음 층의 계산과 합쳐지지 않고 역할을 수행할 수 있기 때문이다. 아래 그림은 시그모이드 그래프이다. 이 함수는 뉴런의 출력 z값을 0과 1사이로 압축한다. 이를 사용해 은닉층을 만든다. 2-2. 시그모이드 활성화 함수로 심층 신경망 생성하기1234dense1 = keras.layers.Dense(100, activation='sigmoid', input_shape=(784,))#출력층에서 10개의 클래스를 분류하므로 10개의 뉴런, 소프트맥스 활성화함수dense2 = keras.layers.Dense(10, activation='softmax') activation='sigmoid'로 활성화 함수를 시그모이드로 지정할 수 있다.은닉층에서 100개의 뉴런을 지정했는데, 이는 특별한 기준이 없지만, 출력층의 뉴런보다는 많이 만들어야한다. 이제 위의 두 개층을 Sequential 클래스에 추가하여 심층 신경망을 만든다.두 개의 층을 리스트로 Sequential 클래스에 전달한다. 12model = keras.Sequential([dense1, dense2])model.summary() 1234567891011Model: &quot;sequential_2&quot;_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= dense (Dense) (None, 100) 78500 dense_1 (Dense) (None, 10) 1010 =================================================================Total params: 79,510Trainable params: 79,510Non-trainable params: 0_________________________________________________________________ summary() 메서드로 층에 대한 정보를 얻는다. dense의 출력 크기를 보면 (None, 100)으로, 첫번째 차원은 샘플 크기를 나타낸다. 샘플 갯수가 아직 정의되지 않아 None 이며, 후에 fit() 매서드에 훈련 데이터를 주입하면 미니배치 경사 하강법을 사용한다.케라스의 기본 미니베치 크기는 32개이며, fit() 메서드에서 batch_size 매개변수로 바꿀 수 있다. 두번째 100개 출력은, 784개의 특성이 은닉층을 통과하며 100개의 특성으로 압축됨을 뜻한다. 모델 파라미터 갯수는 입력픽셀 784개와 100개의 모든 조합에 대한 가중치, 100개의 절편이 있어 784*100 + 199 = 78500개 이다. 두번째 층의 파라미터 또한 100*10 + 10 = 1010개 이다. 2-3. 층을 추가하는 다른 방법Sequential 클래스의 생성자 안에서 바로 Dense 클래스의 객체를 만드는 방법이 있다. 1234model = keras.Sequential([ keras.layers.Dense(100, activation='sigmoid', input_shape=(784,), name='hidden'), keras.layers.Dense(10, activation='softmax', name=&quot;output&quot;)], name='패션 MNIST모델') 너무 많은 층을 추가하려면 생성자가 매우 길어지기 때문에, add() 메서드도 사용한다. 123model = keras.Sequential()model.add(keras.layers.Dense(100, activation='sigmoid', input_shape=(784,)))model.add(keras.layers.Dense(10, activation='softmax')) 2-4. 심층 신경망 모델 훈련12model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 5s 3ms/step - loss: 0.5596 - accuracy: 0.8103Epoch 2/51500/1500 [==============================] - 4s 3ms/step - loss: 0.4054 - accuracy: 0.8556Epoch 3/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3716 - accuracy: 0.8658Epoch 4/51500/1500 [==============================] - 4s 3ms/step - loss: 0.3489 - accuracy: 0.8735Epoch 5/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3320 - accuracy: 0.8801 추가된 층이 성능을 항상시켰다는 것을 알 수 있다. 3. 렐루 활성화 함수초창기 인공 신경망의 은닉층에 많이 사용된 활성화 함수는 시그모이드 함수였다.다만 이 함수는 오른쪽과 왼쪽 끝으로 갈수록 그래프가 누워있기 때문에 올바른 출력을 만드는데 신속하게 대응하지 못한다는 단점이 있다. 이는 층이 많은 신경망일수록 효과가 누적되어 학습을 어렵게 한다. 이를 개선하기 위해 렐루함수가 사용된다. 렐루 함수는 max(0,z)로 쓸 수 있다. 이는 특히 이미지 처리에 좋은 성능을 낸다. 3-1. 입력 차원을 일렬로 펼치는 Flatten 층렐루 함수를 적용하기 전, 입력차원을 일렬로 펼치는 Flatten 층을 알아본다.앞에서 reshape() 메서드를 사용하여 사진 데이터를 일렬로 펼쳤지만, 이를 입력층과 은닉층 사이에 추가할 수 있다. 1model.add(keras.layers.Flatten(input_shape=(28, 28))) 3-2. 렐루 함수를 이용한 밀집층 추가123456model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28, 28)))model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dense(10, activation='softmax'))model.summary() 123456789101112Model: &quot;sequential_4&quot;_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= flatten (Flatten) (None, 784) 0 dense_4 (Dense) (None, 100) 78500 dense_5 (Dense) (None, 10) 1010 =================================================================Total params: 79,510Trainable params: 79,510Non-trainable params: 0_________________________________________________________________ Flatten 층을 신경망 모델에 추가하면 입력값의 차원을 짐작할 수 있다. 3-3. 훈련 데이터로 모델 훈련123456(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()train_scaled = train_input / 255.0train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42)model.compile(loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 4s 2ms/step - loss: 0.5249 - accuracy: 0.8142Epoch 2/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3924 - accuracy: 0.8590Epoch 3/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3553 - accuracy: 0.8712Epoch 4/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3310 - accuracy: 0.8810Epoch 5/51500/1500 [==============================] - 4s 2ms/step - loss: 0.3179 - accuracy: 0.8868 검증 세트로 모델을 평가해본다. 1model.evaluate(val_scaled, val_target) 12375/375 [==============================] - 1s 2ms/step - loss: 0.3948 - accuracy: 0.8674[0.39478805661201477, 0.8674166798591614] 은닉층을 추가하지 않은 모델보다 성능이 몇 퍼센트 더 상승했다. 4. 옵티마이저 : 다양한 경사 하강 알고리즘신경망에는 모델이 학습되지 않아 사람이 지정해주어야 하는 하이퍼파라미터가 많다. 다양한 종류의 경사 하강법 알고리즘도 지정할 수 있는데, 이를 옵티마저라고 한다. 4-1. SGD : 확률적 경사 하강법compile() 메서드에서 케라스의 기본 경사 하강법 알고리즘은 RMSprop을 사용했다.확률적 경사 하강법인 SGD를 사용할 수 있는데, 이 역시 미니배치를 사용한다. 1model.compile(optimizer='sgd', loss='sparse_categorical_crossentropy', metrics='accuracy') SGD 객체를 생성하여 옵티마저로 적용할 수 있다. 12sgd = keras.optimizers.SGD()model.compile(optimizer=sgd, loss='sparse_categorical_crossentropy', metrics='accuracy') SGD 클래스의 학습률 기본값은 0.01이며, learning_rate 매개변수에 학습률을 지정할 수 있다. 1sgd = keras.optimizers.SGD(learning_rate=0.1) momentum 매개변수의 기본값은 0이고 0보다 큰 값으로 지정하면 그레디언트를 가속도처럼 사용하는 모멘텀 최적화를 사용할 수 있다. 보통 0.9 이상을 지정한다.nesterov 매개변수를 True로 바꾸면 네스테로프 모멘텀 최적화를 사용한다. 1sgd = keras.optimizers(momentum=0.9, nesterov=True) 네스테로프 모멘텀은 모멘텀 최적화를 두번 반복하여 구현한다. 대부분 기본 확률적 경사 하강법보다 나은 성능을 제공한다. 4-2. Adagrad, RMSprop : 적응적 학습률 사용모델이 최적점에 가까이 갈수록 학습률을 낮출 수 있으며, 이를 통해 안정적으로 최적점에 수렴할 가능성이 높다. 12345adagrad = keras.optimizers.Adagrad()model.compile(optimizer=adagrad, loss='sparse_categorical_crossentropy', metrics='accuracy')rmsprop = keras.optimizers.RMSprop()model.compile(optimizer=rmsprop, loss='sparse_categorical_crossentropy', metrics='accurac') 모멘텀 최적화와 RMSprop 장점을 접목한 것이 Adam이다. 4-3. Adam : 모멘텀 최적화와 RMSprop의 장점 접목Adam 클래스의 매개변수 기본값을 사용해 모델을 훈련한다. 12345678model = keras.Sequential()model.add(keras.layers.Flatten(input_shape=(28,28)))model.add(keras.layers.Dense(100, activation='relu'))model.add(keras.layers.Dense(10, activation='softmax'))# 옵티마이저를 Adam으로 훈련model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')model.fit(train_scaled, train_target, epochs=5) 12345678910Epoch 1/51500/1500 [==============================] - 4s 2ms/step - loss: 0.5218 - accuracy: 0.8183Epoch 2/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3916 - accuracy: 0.8586Epoch 3/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3544 - accuracy: 0.8711Epoch 4/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3248 - accuracy: 0.8809Epoch 5/51500/1500 [==============================] - 3s 2ms/step - loss: 0.3058 - accuracy: 0.8880 검증 세트에서의 성능도 확인해본다. 1model.evaluate(val_scaled, val_target) 12375/375 [==============================] - 1s 2ms/step - loss: 0.3426 - accuracy: 0.8767[0.3426271080970764, 0.8767499923706055]","link":"/2024/04/02/Deep-Neural-Network/"},{"title":"합성곱 신경망의 구성요소와 이미지 분류","text":"합성곱 신경망(Convolutional Neural Network, CNN)은 주로 이미지 인식, 영상 처리, 컴퓨터 비전 분야에서 사용되는 심층 신경망의 한 종류이다. CNN은 이미지로부터 패턴을 인식하고 이해하는 데 특화되어 있으며, 이를 위해 합성곱 계층(convolutional layer)과 풀링 계층(pooling layer)을 포함한 특별한 구조를 가진다. [출처 : 혼자 공부하는 머신러닝+딥러닝 8장. 이미지를 위한 인공신경망] CNN의 주요 구성 요소: 합성곱 계층(Convolutional Layer): 이 계층은 이미지로부터 특성을 추출하는 데 사용된다. 여러 개의 필터(또는 커널)를 사용하여 이미지를 스캔하고, 이 과정에서 생성된 특성 맵(feature map)을 통해 이미지의 중요한 정보를 추출한다. 활성화 함수(Activation Function): 대부분의 CNN에서는 ReLU(Rectified Linear Unit) 함수가 활성화 함수로 사용된다. 이 함수는 비선형 변환을 제공하여 네트워크가 복잡한 패턴을 학습할 수 있게 한다. 풀링 계층(Pooling Layer): 특성 맵의 크기를 줄이거나 요약하여 계산량을 감소시키고, 과적합을 방지하는 역할을 한다. 최대 풀링(Max Pooling)과 평균 풀링(Average Pooling)이 일반적으로 사용된다. 완전 연결 계층(Fully Connected Layer): CNN의 마지막 부분에 위치하며, 앞서 추출된 특성을 바탕으로 최종적인 분류나 예측을 수행한다. CNN의 특징 및 장점: 공간적 계층 구조: CNN은 이미지의 공간적 계층 구조를 이해할 수 있으며, 이를 통해 이미지의 로컬 패턴(예: 가장자리, 질감 등)부터 복잡한 객체까지 인식할 수 있다. 매개변수의 공유: 합성곱 필터는 이미지 전체에 걸쳐 공유되므로, 전통적인 심층 신경망에 비해 훨씬 적은 수의 매개변수를 사용한다. 이동 불변성(Translation Invariance): CNN은 이미지 내 객체의 위치가 변해도 동일한 객체를 인식할 수 있다. 활용 분야: 이미지 분류: 사진 속 객체를 분류한다(예: 강아지, 고양이 분류). 객체 탐지: 이미지 내에서 객체의 위치와 종류를 탐지한다. 시맨틱 분할: 이미지를 픽셀 수준에서 분류하여, 각 픽셀이 어떤 객체에 속하는지 결정한다. 얼굴 인식, 자율 주행 자동차, 의료 영상 분석 등 다양한 분야에서 광범위하게 활용된다. 이번에는 텐서플로 케라스 API를 이용해 패션 MNIST 데이터를 합성곱 신경망 (Convolutional Neural Network, CNN)으로 분류한다. 모델을 만들며 합성곱, 패딩, 스트라이드, 풀링의 개념도 같이 알아볼 것이다. 1. 데이터 준비패션 MNIST 데이터를 불러오고 표준화 전처리 후 훈련세트와 검증세트로 나눈다.이때, 합성곱 신경망은 2차원 이미지를 그대로 사용하기 때문에 일렬로 펼치지 않는다.흑백 이미지이기 때문에 1차원 채널이 추가되며, 컬러 이미지는 3차원이 추가된다. 12345678from tensorflow import kerasfrom sklearn.model_selection import train_test_split(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()# 흑백 이미지에 채널 추가train_scaled = train_input.reshape(-1, 28, 28, 1) / 255.0train_scaled, val_scaled, train_target, val_target = train_test_split(train_scaled, train_target, test_size=0.2, random_state=42) 2. 합성곱 신경망 만들기2-1. 합성곱 층 추가하기합성곱 신경망의 구조는 합성곱 층에서 이미지의 특징을 감지한 후 밀집층에서 클래스에 따른 분류 확률을 계산한다. 먼저 Sequential 클래스 객체를 만들고, 첫 번째 합성곱 층인 Conv2D를 추가한다. Conv2D() 매개변수로 커널의 개수, 커널 사이즈, 활성화 함수, 패딩, 입력 데이터 크기가 필요하다. 123# 32개 필터, 커널 크기 3x3, 렐루 함수, 세임패딩model = keras.Sequential()model.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu', padding='same', input_shape=(28,28,1))) 커널의 개수를 32개로 지정하고, 커널 사이즈를 3으로 놓으면 (3, 3) 크기가 된다.렐루 함수를 활성화 함수로 지정하고, 세임패딩 적용, 인풋 데이터 크기를 지정한다. 세임 패딩과 밸리드 패딩패딩이란 입력 배열 주위를 가상의 원소로 채우는 것을 의미한다. 예로, (4, 4) 크기의 입력에 0을 1개 패딩하면 (6, 6)크기의 입력이 된다. 세임 패딩 : 합성곱 층의 출력 크기를 입력과 동일하게 만들기 위해 입력에 패딩을 추가하는 것이다. 밸리드 패딩 : 패딩 없이 순수한 입력 배열에서만 합성곱을 하여 특성 맵을 만드는 것, 특성 맵의 크기가 줄어든다. 만약 패딩이 없다면 원소들이 2번 이상 커널과 계산되는 것과 달리, 네 모서리에 있는 4개의 값은 커널에 한번만 계산되게 된다. 만약 이 입력이 이미지라면 모서리에 있는 중요한 정보가 특성 맵에 잘 전달되지 않을 가능성이 높다. 반대로 가운데 있는 정보는 잘 표현된다. 2-2. 풀링 층 추가하기풀링과 스트라이드 풀링 : 합성곱 층에서 만든 특성 맵의 가로세로 크기를 줄이는 역할을 수행한다. 특성 맵의 개수는 줄이지 않는다. 최대 풀링 : 커널 영역에서 가장 큰 값을 고른다. 평균 풀링 : 커널 영역의 값을 평균화한다. 스트라이드 : 합성곱 층에서 필터가 입력 위를 이동하는 크기 예를 들어 (2,2,3) 크기의 특성 맵에 스트라이드가 1인 풀링을 적용하면 (1,1,3) 크기의 특성 맵이 된다. 많은 경우 평균 풀링보다 최대 풀링을 사용하는데, 평균 풀링은 특성 맵의 중요한 정보를 평균화하여 희석시킬 수 있기 때문이다. 케라스는 최대 풀링과 평균 풀링을 MaxPooling2D, AveragePooling2D 로 제공한다. 그 중에서 최대풀링을 사용하며, 풀링 크기를 (2,2)로 지정한다. 1model.add(keras.layers.MaxPooling2D(2)) 패선 MNIST 이미지가 (28,28) 크기에 세임 패딩을 적용하여 합성곱 층에서 출력된 특성 맵의 가로세로 크기는 입력과 동일하다. 이후 (2,2) 풀링을 적용하여 특성 맵의 크기는 절반으로 줄어들고, 합성곱 층에서 32개의 필터를 사용하여 최대 풀링을 통과한 특성 맵의 크기는 (14,14,32) 이다. 이제 두 번째 합성곱-풀링 층을 추가한다. 첫번째와 동일하지만, 필터 개수를 64개로 늘렸다. 12model.add(keras.layers.Conv2D(64, kernel_size=3, activation='relu', padding='same'))model.add(keras.layers.MaxPooling2D(2)) 이 층을 통과하면 특성 맵의 크기는 (7,7,64)가 된다. 2-3. Flatten, 은닉층, Drop, 출력층 구성하기이제, 마지막에 10개의 뉴런을 가진 출력층에서 확률을 계산하기 위해 3차원 특성 맵을 펼쳐야 한다. Flatten 층을 만들고, Dense은닉층, Dropout , Dense출력층 순서대로 층을 구성한다. 1234model.add(keras.layers.Flatten())model.add(keras.layers.Dense(100, activation='relu')) #은닉층model.add(keras.layers.Dropout(0.4)) # 40% 드롭아웃model.add(keras.layers.Dense(10, activation='softmax')) #출력층 은닉층과 출력층 사이에 드롭아웃을 넣어 은닉층의 과대적합을 막아 성능을 개선할 수 있다.은닉층에 100개의 뉴런을 사용하고 렐루 활성화 함수를 사용한다.클래스 10개를 분류할 다중 분류 문제이기 때문에 출력층의 활성화 함수는 소프트맥스 함수를 사용한다. 3. 모델 구조 확인하기summary() 메서드로 모델 구조를 확인할 수 있다. 1model.summary() 1234567891011121314151617Model: &quot;sequential&quot;_________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 28, 28, 32) 320 max_pooling2d (MaxPooling2D) (None, 14, 14, 32) 0 conv2d_1 (Conv2D) (None, 14, 14, 64) 18496 max_pooling2d_1 (MaxPooling2D) (None, 7, 7, 64) 0 flatten (Flatten) (None, 3136) 0 dense (Dense) (None, 100) 313700 dropout (Dropout) (None, 100) 0 dense_1 (Dense) (None, 10) 1010 =================================================================Total params: 333,526Trainable params: 333,526Non-trainable params: 0_________________________________________________________________ 각 층의 파라미터의 개수를 계산할 수 있다. 첫 번째 합성곱 층은 32개의 필터를 가지고 있고 크기가 (3,3), 깊이가 1이다. 또 필터마다 하나의 절편이 있다. 3x3x1x32+32 = 320개의 파라미터가 있다. 두 번째 합성곱 층은 64개의 필터, 크기 (3,3), 깊이 32이다. 필터마다 하나의 절편이 존지하므로 3x3x32x64+64 = 18496개의 파라미터가 있다. Flatten 층에서 (7,7,64) 크기의 특성 맵을 1차원으로 펼치면 (3136,)이며, 은닉층에서는 3136개가 100개의 뉴런과 연결되어야 하고, 100개의 절편이 있으므로 3136x100+100 = 313700개의 파라미터가 있다. 마지막 출력층은 100개의 특성이 10개의 뉴런과 연결되고, 10개의 절편이 있으므로 100x10+10 = 1010개의 파라미터가 있다. keras.utils 패키지의 plot_model() 으로 층의 구성을 그림으로 볼 수 있다. 1keras.utils.plot_model(model, show_shapes=True, to_file='cnn-architecture.png', dpi=300) show_shapes=True 로, 입력과 출력의 크기가 표시되며, to_file 매개변수는 출력한 이미지를 파일로 저장한다. dpi 매개변수는 해상도를 지정한다. 4. 모델 컴파일과 훈련Adam 옵티마이저를 사용하고, ModelCheckpoint, EarlyStopping 콜백을 사용하여 조기 종료 법을 구현한다. 1234567model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics='accuracy')checkpoint_cb = keras.callbacks.ModelCheckpoint('best-cnn-model.h5')early_stopping_cb = keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)history = model.fit(train_scaled, train_target, epochs=20, validation_data=(val_scaled, val_target), callbacks=[checkpoint_cb, early_stopping_cb]) 123456789101112131415161718Epoch 1/201500/1500 [==============================] - 19s 7ms/step - loss: 0.5307 - accuracy: 0.8096 - val_loss: 0.3467 - val_accuracy: 0.8756Epoch 2/201500/1500 [==============================] - 10s 7ms/step - loss: 0.3584 - accuracy: 0.8720 - val_loss: 0.3089 - val_accuracy: 0.8859Epoch 3/201500/1500 [==============================] - 11s 7ms/step - loss: 0.3119 - accuracy: 0.8876 - val_loss: 0.2708 - val_accuracy: 0.8968Epoch 4/201500/1500 [==============================] - 10s 7ms/step - loss: 0.2770 - accuracy: 0.8992 - val_loss: 0.2580 - val_accuracy: 0.9047Epoch 5/201500/1500 [==============================] - 10s 7ms/step - loss: 0.2544 - accuracy: 0.9065 - val_loss: 0.2518 - val_accuracy: 0.9101Epoch 6/201500/1500 [==============================] - 11s 7ms/step - loss: 0.2322 - accuracy: 0.9149 - val_loss: 0.2452 - val_accuracy: 0.9122Epoch 7/201500/1500 [==============================] - 10s 7ms/step - loss: 0.2171 - accuracy: 0.9195 - val_loss: 0.2294 - val_accuracy: 0.9176Epoch 8/201500/1500 [==============================] - 11s 7ms/step - loss: 0.2013 - accuracy: 0.9257 - val_loss: 0.2560 - val_accuracy: 0.9134Epoch 9/201500/1500 [==============================] - 10s 7ms/step - loss: 0.1881 - accuracy: 0.9299 - val_loss: 0.2308 - val_accuracy: 0.9186 이전보다 정확도가 훨씬 좋아진 것을 확인할 수 있다. 손실 그래프를 그려, 조기 종료가 잘 이루어졌는지 확인한다. 1234567import matplotlib.pyplot as pltplt.plot(history.history['loss'])plt.plot(history.history['val_loss'])plt.xlabel('epoch')plt.ylabel('loss')plt.legend(['train', 'val'])plt.show() 일곱 번째 에포크가 최적임을 알 수 있다. predict() 메서드를 사용하여 데이터에 대한 예측을 만들어 본다. 123456import numpy as npclasses = ['티셔츠','바지','스웨터','드레스','코트','샌달','셔츠','스니커즈','가방','앵클 부츠']preds = model.predict(val_scaled[0:1])print(classes[np.argmax(preds)]) 1가방 테스트 세트로 합성곱 신경망의 일반화 성능을 가늠해본다. 12test_scaled = test_input.reshape(-1, 28, 28, 1) / 255.0model.evaluate(test_scaled, test_target) 12313/313 [==============================] - 2s 7ms/step - loss: 0.2460 - accuracy: 0.9108[0.24599412083625793, 0.9107999801635742] 약 91% 정도의 성능을 기대할 수 있다.","link":"/2024/04/03/Convolutional-Neural-Network/"},{"title":"신경망 모델 훈련","text":"인공 신경망과 심층 신경망을 구성하고 다양한 옵티마이저를 통해 성능을 향상시킬 수 있는 방법에 대해 알아보았다.이번에는 과대적합을 막기 위해 신경망에서 사용하는 규제방법인 드롭아웃, 최상의 훈련 모델을 자동으로 저장하고 유지하는 콜백과 조기종료를 알아보겠다. [출처 : 혼자 공부하는 머신러닝+딥러닝 7장. 인공 신경망]","link":"/2024/04/02/Train-Neural-Network-Model/"},{"title":"합성곱 신경망의 시각화","text":"이번에는 저번 편에서 저장한 합성곱 신경망 모델을 읽어 들인 후 모델의 가중치와 특성 맵을 시각화해본다. 또한 케라스의 함수형 API를 사용하여 모델의 조합을 자유롭게 구성해본다. [출처 : 혼자 공부하는 머신러닝+딥러닝 8장. 이미지를 위한 인공신경망] 1. 데이터 준비이전에 훈련한 합성곱 신경망 모델을 불러온다. 12from tensorflow import kerasmodel = keras.models.load_model('best-cnn-model.h5') 케라스 모델에 추가한 층은 layers 속성에 저장되어 있다. 1model.layers 12345678[&lt;keras.layers.convolutional.Conv2D at 0x7f803f8dad90&gt;, &lt;keras.layers.pooling.MaxPooling2D at 0x7f7fc61459d0&gt;, &lt;keras.layers.convolutional.Conv2D at 0x7f7fc60eed10&gt;, &lt;keras.layers.pooling.MaxPooling2D at 0x7f7fc60eee90&gt;, &lt;keras.layers.core.flatten.Flatten at 0x7f7fc6085c10&gt;, &lt;keras.layers.core.dense.Dense at 0x7f7fc60eeb90&gt;, &lt;keras.layers.core.dropout.Dropout at 0x7f7fc608a450&gt;, &lt;keras.layers.core.dense.Dense at 0x7f7fc607c990&gt;] 첫 번째 합성곱 층의 가중치를 조사해 본다. 층의 가중치와 절편은 층의 weights 속성에 저장되어 있다. 12conv = model.layers[0]print(conv.weights[0].shape, conv.weights[1].shape) 1(3, 3, 1, 32) (32,) 커널 크기가 (3,3,1)이며 필터 개수가 32개이므로 첫 번째 원소의 가중치의 크기는 (3,3,1,32)이다.필터마다 1개의 절편이 있으므로 두 번째 원소의 크기는 (32,0)이다. 한편, weights 속성은 텐서플로의 다차원 배열인 Tensor 클래스의 객체이다.numpy() 메서드로 넘파이 배열로 변환한 후 가중치 배열의 평균과 표준편차를 구해본다. 1234conv_weights = conv.weights[0].numpy()# 평균, 표준편차print(conv_weights.mean(), conv_weights.std()) 1출력 -0.019439656 0.23001778 이 가중치의 평균값은 0에 가깝고, 표준편차는 0.23 정도이다. 나중에 이 값을 훈련하기 전의 가중치와 비교해본다. 1. 가중치 시각화하기1-1. 가중치 분포 히스토그램으로 나타내기먼저 이 가중치의 분포를 히스토그램으로 그려본다. 12345import matplotlib.pyplot as pltplt.hist(conv_weights.reshape(-1,1))plt.xlabel('weight')plt.ylabel('count')plt.show() 0을 중심으로 종 모양으로 분포됨을 확인할 수 있다. 1-2. 커널 그림으로 나타내기이번에는 32개의 커널을 16개씩 두 줄에 출력해 본다. 123456789fig, axs = plt.subplots(2, 16, figsize=(15,2))for i in range(2) : for j in range(16) : # 32개의 커널을 16개씩 두 줄에 출력 axs[i, j].imshow(conv_weights[:,:,0,i*16+j], vmin=-0.5, vmax=0.5) axs[i, j].axis('off')plt.show() vmin과 vmax 파라미터로 픽셀의 최댓값과 최솟값을 지정하여 컬러맵으로 표현할 범위를 지정한다.결과를 보면, 이 가중치 값이 무작위가 아닌 어떠한 패턴이 나타난 것을 볼 수 있다. 픽셀의 특정 부분이 밝다거나 하는 식이다. 1-3. 빈 합성곱 신경망의 가중치위와 같은 방법으로, 훈련되지 않은 합성곱 신경망의 가중치를 조사한다. 12345no_training_model = keras.Sequential()no_training_model.add(keras.layers.Conv2D(32, kernel_size=3, activation='relu', padding='same', input_shape=(28,28,1)))no_training_conv = no_training_model.layers[0]print(no_training_conv.weights[0].shape) 1(3, 3, 1, 32) 3x3 커널을 32개 사용했다. 12no_training_weights = no_training_conv.weights[0].numpy()print(no_training_weights.mean(), no_training_weights.std()) 1-0.0010081615 0.07870515 위의 훈련된 합성곱 신경망과 비교하여 평균은 비슷하지만 표준편차는 매우 작은 것을 알 수 있다. 1234plt.hist(no_training_weights.reshape(-1,1))plt.xlabel('weights')plt.ylabel('count')plt.show() 대부분의 가중치가 -0.15 ~ 0.15 사이에 있고, 고른 분포를 보이는 것을 알 수 있다.텐서플로가 신경망의 가중치를 처음 초기화할 때, 균등 분포에서 랜덤하게 값을 선택하기에 이런 분포를 보인다. 이를 그림으로 시각화한다. 12345678fig, axs = plt.subplots(2, 16, figsize=(15,2))for i in range(2) : for j in range(16) : axs[i, j].imshow(no_training_weights[:,:,0,i*16+j], vmin=-0.5, vmax=0.5) axs[i, j].axis('off')plt.show() 위와 달리 가중치가 밋밋하게 초기화된 것을 볼 수 있다. 이를 통해 합성곱 신경망이 데이터셋의 분류 정확도를 높이기 위해 유용한 패턴을 학습했다는 사실을 알 수 있다. 2. 함수형 API지금까지 신경망 모델을 만들 때 케라스 Sequential 클래스를 사용했다. 이는 층을 차례대로 쌓은 모델을 만드는데, 딥러닝에서는 좀 더 복잡한 모델이 많이 있어 이런 경우는 Sequential 클래스를 사용하기 어렵다. 대신 함수형 API를 사용한다. 함수형 API는 케라스의 Model 클래스를 사용하여 모델을 만든다.그 예로, Dense층 2개로 만들어진 완전 연결 신경망을 함수형 API로 구현해본다. 123456789inputs = keras.Input(shape=(784,))dense1 = keras.layers.Dense(100, activation='signoid')dense2 = keras.layers.Dense(10, activation='softmax')hidden = dense1(inputs)outputs = dense2(hidden)model = keras.Model(inputs, outputs) 케라스는 InputLayer 클래스 객체를 쉽게 다룰 수 있게 Input() 메서드를 별도로 제공한다. 두 개의 층 dense를 만든 뒤, inputs를 dense1에 통과시켜 출력값 hidden을 만들고, 이를 다시 입력값으로 dense2에 통과시켜 출력값을 만들어 이를 모델화한다. 한편, 특성 맵을 시각화하기 위해서는 첫 번째 층인 Conv2D의 출력이 필요하고, 이는 Conv2D 객체의 output 속성에서 얻을 수 있다.모델 객체의 input 속성으로 모델의 입력 또한 얻을 수 있다. 이것을 이용하여 model.input과 model.layers[0].output 을 연결하는 새로운 conv_acti 모델을 만들 수 있다. 1conv_acti = keras.Model(model.input, model.layers[0].output) model 객체의 predict() 메서드를 호출하면 입력부터 마지막 층까지의 계산을 수행한 후 최종 출력을 반환하므로, conv_acti의 predict() 메서드를 호출하여 Conv2D의 출력을 반환할 수 있다. 이를 통해 특성 맵을 시각화해 본다. 3. 특성 맵 시각화케라스 패션 MNIST 데이터셋으로 훈련 세트의 첫 번째 샘플을 그려본다. 123(train_input, train_target), (test_input, test_target) = keras.datasets.fashion_mnist.load_data()plt.imshow(train_input[0], cmap='gray_r')plt.show() 이 샘플을 conv_acti 모델에 주입하여 Conv2D층이 만드는 특성 맵을 출력한다.입력 차원을 reshape()하고, 255로 나누어 표준화한다. 1234inputs = train_input[0:1].reshape(-1, 28, 28, 1) /255.0feature_maps = conv_acti.predict(inputs)print(feature_maps.shape) 1(1, 28, 28, 32) 28x28 크기의 필터 32개로 구성되어 있다. 이를 시각화한다. 12345678fig, axs = plt.subplots(4, 8, figsize=(15,8))for i in range(4) : for j in range(8) : axs[i, j].imshow(feature_maps[0,:,:,i*8+j]) axs[i, j].axis('off')plt.show() 이 특성 맵은 32개의 필터로 인해 입력 이미지에서 강하게 활성화된 부분을 보여 준다.이전 가중치 시각화와 비교하여 어떤 부분이 크게 활성화되었는지 파악할 수 있다. 두 번째 합성곱 층이 만든 특성 맵도 같은 방식으로 시각화한다. 12345678910111213conv2_acti = keras.Model(model.input, model.layers[2].output)inputs = train_input[0:1].reshape(-1, 28, 28, 1) /255.0feature_maps = conv2_acti.predict(inputs)fig, axs = plt.subplots(8, 8, figsize=(12,12))for i in range(8) : for j in range(8) : axs[i, j].imshow(feature_maps[0,:,:,i*8+j]) axs[i, j].axis('off')plt.show() 이 특성 맵은 시각적으로 이해하기 어렵다. 이를 통해, 처음의 합성곱 층은 이미지의 시각적인 정보를 감지하고, 뒤쪽에 있는 합성곱 층은 앞쪽에서 감지한 시각적인 정보를 바탕으로 추상적인 정보를 학습한다고 볼 수 있다.","link":"/2024/04/04/Visualization-Of-CNN/"},{"title":"순차데이터와 순환신경망","text":"RNN(Recurrent Neural Network, 순환 신경망)은 시퀀스 데이터를 처리하기 위해 설계된 인공 신경망의 한 종류이다. RNN은 시간에 따라 정보를 전달할 수 있는 내부 메모리를 가지고 있어, 시퀀스의 길이에 상관없이 입력데이터 사이의 장기 의존성을 학습할 수 있다. 이러한 특성으로 인해 RNN은 자연어 처리(NLP), 음성 인식, 시계열 예측 등 시퀀스 데이터를 다루는 다양한 분야에서 활용된다. 출처 : 혼자 공부하는 머신러닝+딥러닝 9장. 텍트를 위ㄴ 인공 신경망] 순차 데이터(Sequential Data)순서가 있는 데이터를 말한다. 이러한 데이터는 특정 순서대로 나ㅕㄹ되어 있으며, 각 데이터 포인트 사이에는 시간적 또는 공간적 연관성이 존재한다. 순차 데이터의 한 요소는 그 전후의 요소와 관련이 있으며, 이러한 연속성 때문에 데이터 전체를 통해 패턴이나 관계를 찾아낼 수 있다.ex) 글, 대화, 일자별 날씨, 일자별 판매 실적 순환 신경망일반적인 완전 연결 신경망과 거의 비슷하나, 이전 데이터의 처리 흐름을 순환하는 고리 하나가 추가된다. 뉴런의 출력이 다시 자기 자신으로 전달되는데, 즉 어떤 샘플을 처리할 때 바로 이전에 사용했던 데이터를 재사용하는 것이다. 이렇게 샘플을 처리하는 한 단계를 타임스텝이라고 부르며, 순환신경망은 이전 타임스텝의 샘플을 기억하지만, 타임스텝이 오래될수록 순환되는 정보는 희미해진다.순환 신경망에서는 특별히 층을 셀이라고 부른다. 한 셀에는 여러개의 뉴런이 있지만, 뉴런을 모두 표시하지 않고 하나의 셀로 층을 표현한다. 또 셀의 출력을 은닉 상태라고 부른다.입력에 어떤 가중치를 곱하고, 활성화 함수를 통과시켜 다음층으로 보내는 구조는 합성곱 신경망과 같으나, 층의 출력을 다음 타임 스텝에 재사용하는 것이 다르다.은닉층의 활성화 함수로는 하이퍼볼릭 탄젠트(tanh)를 사용한다. 시그모이드 함수와는 달리 -1 ~ 1 사이의 범위를 가진다. RNN의 주요 특징 순환 구조 : RNN은 네트워크 내에서 정보를 순환시키는 구조를 가지고 있어, 이전의 계산 결과를 현재의 계산에 활용할 수 있다. 이를 통해 시퀀스 내의 정보를 시간적으로 연결하여 처리한다. 변동하는 시퀀스 길이 처리 : RNN은 입력 시퀀스의 길이가 변동적인 데이터를 처리할 수 있다. 이는 고정된 크기의 입력을 다루는 다른 신경망 모델과는 차별되는 특징이다. 파라미터 공유 : 시퀀스의 각 지점(time step)마다 동일한 가중치를 사용함으로써, 모델의 파라미터 수를 효율적으로 관리한다. 활용 분야 자연어 처리 : 문장, 문서분류, 기계 번역, 감성 분석 등 NLP의 여러 작업에서 사용된다. 음성 인식 : 오디오 시퀀스에서 음성을 텍스트로 변환하는 작업에 사용된다. 시계역 예측 : 주식 가격, 기상 상태 등 시간에 따라 변하는 데이터의 미래 값을 예측하는 데 사용된다. 한계 장기 의존성 문제 : RNN은 이론적으로는 시퀀스의 장기 의존성을 학습할 수 있지만, 실제로는 그레디언트 소실(vanishing gradient) 또는 폭발(exploding gradient) 문제로 인해 학습이 어려울 수 있다. 계산 비용 : 순환 구조로 인해 병렬 처리가 어렵고, 긴 시퀀스를 처리할 때 계산 비용이 높아질 수 있다. 이러한 한계를 극복하기 위해 LSTM(Long Short-Term Memory)이나 GRU(Gated Recurrent Unit)와 같은 고급 RNN 구조가 개발되었다. 이들은 장기 의존성을 더 효과적으로 학습할 수 있는 메커니즘을 제공한다. IMDB 리뷰 분류1. 데이터 준비하기IMDB 리뷰 데이터셋을 적재한다. 리뷰를 감상평에 따라 긍정과 부정으로 분류해 놓은 데이터셋인데, 총 50,000개의 샘플로 이루어져 있고 훈련 데이터와 테스트 데이터에 25,000개씩 나누어져 있다. 실제 IMDB 리뷰 데이터셋은 영어로 된 문장이지만, 텐서플로에는 이미 정수로 바꾼 데이터가 포함되어 있다. 여기에는 가장 자주 등장하는 단어 500개만 사용한다. 12345from tensorflow.keras.datasets import imdb(train_input, train_target), (test_input, test_target) = imdb.load_data(num_words=500)print(len(train_input[0]))print(len(train_input[1])) 123(25000, ) (25000, )218189 첫 번째 리뷰의 길이는 218개의 토큰, 두 번째는 189개의 토큰으로 이루어져있다. 1print(train_input[0]) 12[1, 14, 22, 16, 43, 2, 2, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 2, 112, 50, 2, 2, 9, 35, 480, ... ] 텐서플로의 IMDB 리뷰 데이터는 정수로 변환되어 있다. num_words=500으로 지정했기 때문에 어휘 사전에 없는 단어는 모두 2로 표시된다. 1print(train_target[:20]) 1[1 0 0 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 1] 타깃 데이터는 0(부정)과 1(긍정)으로 나누어진다. 12from sklearn.model_selection import train_test_splittrain_input, val_input, train_target, val_target = train_test_split(train_input, train_target, test_size=0.2, random_state=42) 훈련 데이터의 20% 정도를 검증세트로 떼어 놓는다. 2. 데이터 분석과 패딩평균적인 리뷰, 가장 짧은 리뷰, 가장 긴 리뷰의 길이를 확인하기 위해 먼저 각 리뷰의 길이를 계산해 넘파이 배열에 담아 그래프로 표현한다. 123456789import numpy as npimport matplotlib.pyplot as pltlengths = np.array([len(x) for x in train_input])plt.hist(lengths)plt.xlabel('length')plt.ylabel('frequency')plt.show() 대부분 리뷰 길이는 300개 미만인 것을 볼 수 있다. 리뷰는 대부분 짧기 때문에 이 예제에서는 100개의 단어만 사용하기로 한다.이 리뷰들의 길이를 맞추기 위해 패딩이 필요하다. pad_sequences()함수를 통해 시퀀스 데이터의 길이를 맞출 수 있다.짧은 리뷰는 앞에서부터 0토큰을 채우고, 긴 리뷰는 잘라내는데, 만약 pad_sequences()의 매개변수 padding을 기본값인 pre에서 post로 바꾸면 샘플의 뒷부분으로 패딩할 수 있다. 123456from tensorflow.keras.preprocessing.sequence import pad_sequencestrain_seq = pad_sequences(train_input, maxlen=100)val_seq = pad_sequences(val_input, maxlen=100)print(train_seq.shape) 1(20000, 100) train_seq는 이제 (20000, 100) 크기의 2차원 배열임을 알 수 있다. 3. 원-핫 인코딩으로 데이터 바꾸기케라스는 여러 종류의 순환층 클래스를 제공하는데, 가장 간단한 것은 SimpleRNN 클래스이다. 이 문제는 이진 분류이므로 마지막 출력층은 1개의 뉴런을 가지고 시그모이드 활성화 함수를 사용한다. 1234from tensorflow import kerasmodel = keras.Sequential()model.add(keras.layers.SimpleRNN(8, input_shape=(100, 500)))model.add(keras.layers.Dense(1, activation='sigmoid')) # 이진분류 뉴런 갯수를 8개로 지정하고, 샘플의 길이가 100이고 500개의 단어만 사용하도록 설정했기 때문에 input_Shape를 (100, 500)으로 둔다. 순환층도 활성화 함수를 사용하는데 기본 매개변수 activation의 의 기본값은 tanh로, 하이퍼볼릭 탄젠트 함수를 사용한다. 그러나 토큰을 정수로 변환한 데이터를 신경망에 주입하면, 큰 정수가 큰 활성화 출력을 만들게 된다. 이 정수들 사이에는 어떤 관련이 없기 때문에 정수값에는 있는 크기 속성을 없애고 각 정수를 고유하게 표현하기 위해 원-핫 인코딩을 사용한다. keras.utils 패키지의 to_categorical() 함수를 사용하여 훈련세트와 검증 세트를 원-핫 인코딩으로 바꾸어준다. 123train_oh = keras.utils.to_categorical(train_seq)val_oh = keras.utils.to_categorical(val_seq)print(train_oh.shape) 1(20000, 100, 500) 정수 하나마다 500차원의 배열로 변경되었다. 12print(train_oh[0][0][:12])print(np.sum(train_oh[0][0])) 12[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]1.0 첫 리뷰의 첫 단어를 원-핫 인코딩시킨 결과이다.모든 원소의 값을 더하면 1임을 알 수 있다. 4. 순환 신경망 훈련하기RMSprop의 기본 학습률 0.001을 사용하지 않기 위해 별도의 RMSprop 객체를 만들어 학습률을 0.0001로 지정한다.","link":"/2024/04/08/Recurrent-Neural-Network/"},{"title":"API Gateway Pattern에는 API Gateway가 없다.","text":"Microservices Architecture에 관해 이야기하면 왠지 API Gateway를 꼭 사용해야 할 것 같은 느낌이 든다. 과연 그럴까?왜 함부로 API Gateway를 사용하면 안 되는지, 그래도 사용해야 한다면 언제 사용할지에 대해 다뤄보려고 한다. 우아한형제들의 서버개발그룹장분이 잘 설명해주셔서, 두고두고 보려고 기록해두는 글이다. 다룰 내용 MSA에서 필수적인 API Gateway Pattern이 API Gateway Framework와 무관한 이유 API Gateway Pattern 이란? MSA의 API 애플리케이션의 역할 구분 하나씩 알아보는 API Gateway Framework을 사용하면 안되는 이유들 그래도 API Gateway Framework를 사용해도 되는 경우들 API Gateway Pattern 이란?API Gateway Pattern은 어디 나오나? 크리스 리처드슨은 본인이 생각한 MSA의 여러가지 패턴을 마이크로 서비스 패턴이라는 책으로 정리하였다.그 외에도 마이크로소프트 홈페이지에서도 API Gateway 패턴을 다루고 있다. API 게이트의 패턴은 여러 마이크로 서비스로 나뉜 것을 하나로 묶어 클라이언트가 호출하게 해주는 것으로 MSA에서는 필수적인 요소이다. 그렇다면 API 게이트웨이 패턴의 정의를 그림 한장으로 나타내면 무엇일까? 모바일 앱이나 웹브라우저 혹은 외부 애플리케이션 서버 등 외부의 클라이언트가 API 게이트웨이를 통해서 내부망에 있는 api를 그대로 호출한다.이때, API Gateway가 인증을 하고 클라이언트가 호출 가능한 api들만 공개함으로써 안전하게 내부 서비스를 보호해주게 된다고 생각할것이다.외부 개발자는 내부망의 모든 api 호출 엔드포인트를 알 필요가 없기 때문에 API 게이트웨이 엔드포인트 하나만 확인하고 편하게 호출할 수 있는 좋은 패턴이라고 생각될 것이다. 위의 구조는 API Gateway의 프레임워크를 통해 설정 정보만으로도 구성이 가능하다.안타깝게도 이것은 API Gateway 패턴이 전혀 아니다.그리고 계속 생각할것이다라고 말한 이유는 사실은 그렇지 못하기 때문이다. Microservice.io 혹은 마이크로서비스 패턴 책에 나오는 API Gateway 패턴의 정의는 위의 도식과 같다.이 API Gateway 패턴은 클라이언트의 특정 비즈니스 요청을 받아주는 서버 애플리케이션을 개발자가 직접 작성하고 내부 api들을 조합해서 호출한 뒤 응답을 클라이언트에게 필요한 것만 정리해서 내보내는 방식이다. 앞의 것은 설정만으로도 가능하고, 뒤의 실제 API Gateway 패턴은 개발자가 명백하게 코드로 작성해야만 하는 것이다.코드 작성은 쉽다. 자바 개발자라면 흔한 웹프레임워크인 스프링 웹 MVC나 Webflux를 사용하면 된다.다만 리액티브한 웹플럭스를 권한다. 1차 결론 외부에 API를 제공하기 위한 API Gateway Pattern은 Spring Cloud Gateway나 Netflix Zuul 같은 API Gateway Framework를 사용하는 것과는 무관 API Gateway Pattern은 클라이언트의 요청을 받아서 내부 마이크로서비스를 호출하는 로직을 직접 작성하고 응답 내용도 취사 선택하여 필요한 것만 내보내느 것 인증과 권한 기능을 붙였는지 여부와 무관하게 외부에 API를 제공해줄 용도로 API Gateway Framework를 사용한 요청/응답 Routing은 하면 안됨 특히 규모가 작다면 실제 MSA를 하면서 API Gateway Framework를 사용할 일은 사실 많지 않음 API Gateway FrameworkSpring Cloud Gateway, Netflix Zuul, AWS API Gateway 등을 이 글에서 부르는 용어 Gateway Routing PatternMS에서 정의한 API Gateway Framework를 통한 단순 요청/응답 라우팅에 관한 패턴 BFF - Backend For FrontendAPI Gateway Pattern과 유사하게 직접 구현하는 로직을 작성하는 방법과 API Gateway 애플리케이션을 클라이언트 기기 단위로 구분하고 소유권을 정하는 방식 조합 API Gateway 라는 단어의 의미는 현재 매우 심각하게 오염되어 있다. 이 단어를 듣는 순간 대부분 사람들이 떠올리는 그림은 위 두 그림 중 하나일 텐데 아마도 대부분 사람들은 첫 번째 보여 드린 요청응답을 단순 라우팅하는 그림을 떠올렸을 것이다. 바로 이 점 때문에 의사소통과 실제 구현에서 다양한 문제들이 발생하고 있다. 그래서 필자는 우리가 보통 API 게이트웨이라고 부르는 것들 즉 스프링클라우드 게이트의 넷플릭스 Zuul과 같은 것을 지금부터 API Gateway Framework 라고 부르겠다. 그리고 마이크로소프트 홈페이지에서는 이런 API Gateway Framework를 통한 단순 요청응답으로 구성하는 것을 게이트웨이 라우팅 패턴이라고 불렀다. 그래서 필자도 앞으로 Gateway Routing Pattern이라는 용어로 API Gateway Framework를 사용하는 단순 라우팅을 지칭하도록 하겠다. 왜냐하면 최근 API Gateway Framework는 단순 라우팅뿐만 아니라 코딩을 통해 API Gateway Pattern에서 정의한대로의 역할도 수행할 수 있기 때문이다. API Gateway Framework로도 API Gateway Pattern의 구현이 가능하지만, 필자는 권하지 않는다. 참고로 API Gateway Pattern이 API Gateway라는 용어의 경우 일부 다른 사람들은 엣지 마이크로 서비스 클라이언트 어댑터 등의 다른 용어로 부르는 것도 많이 보인다. 최근에는 BFF라는 용어도 대세가 되어가는 것 같다. MSA 자체의 역사가 길지 않다보니 많은 것들이 정리되지 못하고 다양한 오해를 일으키고 있어 보인다. 사실상 필자가 하고 싶은 말의 핵심은 여기서 끝이다. 위 두 글미을 보는 순간 딱 하고 그래 그렇지 않고 뭔가 떠오른 사람 혹은 이미 알고 있는 사람이라면 더 이상 이 글을 보지 않아도 좋을거 같다. 하지만 어쨋든 왜 API Gateway Pattern이라는게 어째서 단순히 API Gateway Framework를 통한 단순 라우팅을 하면 안되는 것인지 혹은 왜 비록 올바른 API Gateway의 구현이 가능하다 하더라도 필자가 API Gateway Framework의 사용을 권장하지 않는지 살펴보도록 하겠다. Monolithic Architecture : 기본으로 돌아가 모놀리식 아키텍처를 살펴보자. 일반적인 스프링 웹 MVC 기반 Monolithic 애플리케이션은 이 그림과 비슷한 형태를 띤다.애플리케이션은 크게 두 가지 계층으로 나눈다. 하나는 실제 비즈니스 로직을 처리하는 비즈니스 계층이다. 여기서 저장소에 접근하고 도메인 로직을 수행한다. 서비스 계층이라고 부르기도 한다. 그리고 프레젠테이션 계층이 외부 클라이언트의 요청을 받아서 비즈니스 계층을 호출하고 그 결과를 클라이언트가 사용하기 적합하게 변환해서 응답하게 된다. 프레젠테이션 계층에는 필터, 인터셉터가 있다. 비즈니스 계층에는 AOP가 있다. 이 둘은 역할이 비슷하다. 필터 혹은 인터셉터는 여러 컨트롤러의 공통 적용된 로직을 실행하고 컨트롤러 코드를 호출한다. AOP도 마찬가지인데 AOP는 보통 서비스나 그 외의 비즈니스 로직이 공통으로 적용할 로직을 수행한다. 둘이 하는 일을 횡단 관심사 처리라고 한다. 영어로는 Cross Cutting Concerns라고 한다. 프레젠테이션 계층에는 Facade라는 것도 보이는데 클라이언트의 요청이 하나의 비즈니스 로직이 아니라 여러 비즈니스 로직의 조합일 경우 작성한다. 이 Facade라는 용어를 모르고있더라도 혹은 별도의 클래스를 분리하지 않고 컨트롤러에 직접 Facade 코드를 작성하더라도 어찌됐든 여러분은 자기도 모르는 사이에 Facade를 작성하고 있을 것이다.계층형 아키텍처 혹은 계층형이 아니더라도 소프트웨어 개발에서 가장 기본 중 하나는 하위 모듈이 상위 모듈을 호출할 수 없고 하위 계층이 상위 계층을 호출할 수 없다는 것이다. 여기서 하위 계층은 비즈니스 계층이고 상위 계층은 프레젠테이션 게층이다. 따라서 비즈니스 계층은 절대로 프레젠테이션 계층을 호출하면 안된다. 쉽게 말해 서비스 클래스는 컨트롤러 클래스를 호출해서는 안된다. MSA를 이런 계층형 아키텍처로 매핑하면 어떤 모양이 될까? 우리가 보통 마이크로서비스라고 부르는 그것들은 비즈니스 계층에 속한다. 그래서 마이크로 서비스 아키텍처라고 부르는 것 같다. 그리고 API Gateway Framework가 아닌 API Gateway는 프레젠테이션 계층에 속하게 된다. 몇몇 분들은 아닌데 내가 만든 마이크로서비스는 프레젠테이션 계층인데라고 하는 사람이 있을 수 있는데 이것은 조금 이따가 알아보도록 하곘다. 혹시 이 그림을 보는 순간 어째서 API Gateway Framework로 라우팅하는 것이 문제가 되는지 곧바로 파악이 되는가?여기서 API Gateway를 API Gateway Framework를 통한 라우팅 기반으로 변경하면 어떤 형태가 되는 걸까? 바로 이렇게 컨트롤러 코드와 Facade 코드가 존재하지 않고 횡단 관심사만 처리하는 필터/인터셉터만 남은 프레젠테이션 계층이 되어버리는 것이다. 컨트롤러와 Facade 코드가 존재하지 않는 프레젠테이션 계층은 아주 심각한 문제들을 일으킨다. 2차 결론 API Gateway Framework를 통해 단순 Gateway Routing Pattern으로 내부 서비스 API를 외부로 노출시키는 경우에는 필터/인터셉터 역할만 하기 때문에 컨트롤러/Facade가 존재하지 않는 상태가 되어 많은 문제를 일으킴 프레젠테이션 계층 구성은 꼭 컨트롤러와 퍼사드 코드를 직접 개발자가 작성하는 API Gateway Pattern을 따라야 함. 다시 말하면 비즈니스 계층 마이크로서비스 API들을 프레젠테이션 계층으로 변환하는 용도로 API Gateway Framework를 사용해서는 안된다. 반대로 Filter/Interceptor/AOP 같은 횡단 관심사를 처리하는 데는 API Gateway Framework를 사용할 수 있다는 의미 - 추천하지는 않음 필자는 사실 API Gateway Framework를 사용할 일은 정말 드물다고 생각한다. 그것이 비록 횡단 관심사 처리 일지라도 말이다.사실상 이글의 핵심은 여기서 다 나왔다고 봐도 된다. 그래도 도대체 왜 컨트롤러와 퍼사드 코드를 직접 작성하지 않으면 문제가 되는지 어째서 횡단 관심사 처리에도 API Gateway Framework를 사용하지 말라는 것인지 궁금할 것이다. API Gateway Framework Gateway Routing 방식의 문제점 보안 취약성 Client 개발자 혹은 특정 Micro Service에게 Business 로직 전가 Business 계층과 Client 간의 강결합과 그로인한 보안 취약성 성능 저하 내부 Service들 간의 protocol 자유도 하락 계층형 아키텍처의 일반적인 원칙 위반 사례 횡단 관심사 처리 관점에서도 사용하지 않아야 하는 이유 Single Point Of Failure 성능 저하 관리 부담 문서화 AGF문제점 : 보안 취약성 인증 : 사용자의 로그인 여부 - 누구세요? 권한 : 당신은 이것을 사용할 수 있고 저것은 사용할 수 없습니다. “이것” 이란? 특정 API 해당 API가 처리하는 객체 API Gateway Framework를 사용하면 보안이 좋아진다는 착각이 많이 있다. 하지만 사실이 아니다.인증을 한다는 것과 특정 API 호출 권한을 제어한다는 것인 보안을 강화하는 것은 아니다. 그것은 프레젠테이션 계층이 당연히 해야할 일이다. 인증은 사용자의 로그인 여부를 체크해서 누구인지 확인한다. 여기서 문제는 권한에 대해 많이들 오해한다는 점이다.권한에는 특정 api를 호출해도 된다라는게 있다. 그리고 그 특정 api가 처리하는 객체에 대한 접근 가능 여부도 권한에 들어간다.Gateway Routing 방식으로 사용하면 위의 빨간색으로 된 객체 관련 권한 처리를 누락하게 된다. GET /users/{userId} : 허용 PUT /users/{userId} : 허용 POST /users : 금지 Login 사용자 1번이라면 GET /users/1 : 허용 PUT /users/1 : 허용 POST /users : 금지 GET /users/2 : 허용 GET /users/3 : 허용 GET /users/... : 허용 PUT /users/2 : 허용 PUT /users/3 : 허용 PUT /users/... : 허용 단순한 예로 로그인 사용자에게 API Gateway Framework에서 GET /users/id와 PUT은 허용하고 POST는 금지했다고 해보자.보안이 강화된 것처럼 보인다. 하지만 로그인 사용자 1번은 해당 api에서 1번 사용자의 개체만을 다루어야 한다. 그런데 2번 3번 객체를 다루는 것은 막지 못하고 있다.권한을 지정할 때 http 메소드와 url 패턴이 일치하면 허용했지 패턴에 들어가 있는 값에 대해서는 확인하지 않았기 때문이다. 다른사용자의 데이터를 조회하는 것뿐만 아니라 마음대로 수정까지 가능하다. 심각한 보안 위협이다. 이 문제를 해결하는 방법은 직접 컨트롤러 코드를 작성하는 수밖에 없다.지금 예제는 간단하다. 로그인 유저 아이디는 세션 역할의 쿠키에 저장되어 있고 이를 프레임워크가 알고 있으니까 왠지 단순하게 유저 id값과 로그인 사용자의 유저 아이디를 비교해서 호출하지 못하게 필터를 구성하면 될 것 같다. 하지만 문제가 해결된 것은 아니다. 오더나 리뷰처럼 이와 같이 오더 id와의 관계를 알 수 없는 수많은 api 호출들이 존재하기 때문이다. 이런 경우에도 로그인 사용자가 명백히 소유한 주문이 아니면 조회나 수정이 불가능해야 한다. 그러면 아래처럼 모든 api 호출에 대해 소유 관계를 넣으면 일부 문제는 해소해준다. 특정 프레젠테이션 계층에서는 이렇게 하면 성능도 좋아진다.하지만 사실 아주 많은 경우에 저렇게 소유 관계만으로 객체를 조회할 수 없다. 멀리 갈 것도 없이 오더 마이크로 서비스 입장에서는 본인들이 가지고 있지도 않은 유저에 대한 정보를 API url에 넣는 것 자체가 부담일 것이고 일반적인 비즈니스 로직에서는 오더 아이디만으로 조회해야 하는 경우가 태반일텐데 그렇제 못해서 오는 불편도 상당하다. 그리고 그런 것을 넘어서 가장 중요하게 이 방식이 안 되는 이유가 있다. 많은 착각 중에 하나가 프레젠테이션 계층이 하나일 거라는 것이다. 대부분의 엔터프라이즈 애플리케이션은 프레젠테이션 계층이 여러 개이다. 요즘 많이 회자되고 있는 어떤 아키텍처가 떠오를 것이다. Admin은 모든 사용자의 주문 정보에 접근이 가능해야 한다. 가가 업주는 본인의 가게에서 일어난 주문에 대해서는 그 사용자가 누구이든 조회할 수 있어야 한다. 필자는 Batch Job이나 이벤트 컨슈머도 프레젠테이션 계층으로 보는데 이것들은 시스템으로서 무제한의 데이터 접근권을 가진 프레젠테이션 계층에 속한다. 또한 타사의 서버 애플리케이션 호출 지점도 프레젠테이션 계층이다. 이 모든 경우를 미세하게 API Gateway Framework로 제어하는 것은 필자가 보기엔 불가능하다. 따라서 비즈니스 계층은 프레젠테이션 계층의 접근자가 누구인지 구분하지 않고 요청을 받아주고 프레젠테이션 계층에서 각자 코딩을 통해 권한을 미세하게 조정해야 한다. 아마도 비즈니스 계층 자체에 저 모든 처리르 넣고 API Gateway Framework를 붙이려는 욕구가 생길 수 있는데 비즈니스 계층 서비스가 100개이고 처음에는 두 개 정도의 프리젠테이션 계층으로 시작했는데 나중에 두 개의 또 다른 프리젠테이션 계층이 더 추가됐다고 상상해 보길 바란다… 당장 100개는 아니더라도 수많은 마이크로서비스에 자기들은 관심도 없는 프리젠테이션 계층의 권한 로직을 추가해야 하는 부담을 지게된다. 과연 또 다른 프리젠테이션 계층이 추가될 것 같은가? 지금 당장 생각나는 것은 배달원 프리젠테이션 계층 고객센터 직원 프리젠테이션 계층 고객사 직원 계층이 생각난다. 또 얼마나 많이 추가될지 상상도 안된다. 애초에 저런 권한 로직을 비즈니스 계층에 넣는 것 자체가 비즈니스 계층과 프리젠테이션 계층을 모호하게 만드는 잘못된 행동으로 보인다. AGF문제점 : 보안 취약성 IDOR IDOR : Insecure Direct Object Reference 그 중에서도 “수평적 권한상승” 개발자가 꼭 알아야 할 애플리케이션 보안 : 입문부터 놓치면 안될 트렌드까지?! (8월 우아한테크세미나 - 권현준) 수평적 권한상승쉽게 말해, 동일한 권한을 가진 다른 사용자의 객체에 접근할 수 있을때를 수평적 권한상승이라고 함 수직적 권한상승본인이 지닌 권한을 넘어서는 기능을 수행할 수 있을때를 수직적 권한상승이라 함 필자는 이 취약점이 최근에 급부상한 이유가 마이크로 서비스 아키텍처가 급부상하면서 함께 증가한 API Gateway Framework 사용 때문이라고 추측하고 있다.방금은 수평적 권한 획득의 예였고 잘못 설계된 마이크로 서비스 아키텍처에는 수직적 권한 상승 문제도 마찬가지로 발생한다. 그 예는 추후에 포스팅 하도록 하겠다. AGF 문제점 : 잘못된 로직 구현 전가정말로 어떻게 해서 절대로 보안 문제가 없는 경우라고 가정해도 단순 라우팅을 하게되면 Facade가 필요한 경우에 엉뚱한 곳에 로직을 전가하게 된다. 클라이언트(Mobile App, Web Browser)가 직접 스스로 비즈니스 계층의 로직을 다양하게 조합해서 호출하게 됨 혹은 반대로 Order Service가 엉뚱하게 Product Service를 호출해서 클라이언트에 필요한 정보를 조합해 내려주는 것처럼 본인의 비즈니스가 아닌 것에 대해 구현하고 의존하게 됨 이게 뭐가 문제일까? AFG 문제점 : Client에 구현 전가 일단 클라이언트 개발자가 UI 처리에 집중하지 못하게 됨 비즈니스 로직은 서버개발자와 클라이언트 개발자 둘 다 밀도 높게 파악해야 함 비즈니스 로직 하나하나는 무슨 수르 ㄹ써서 든 보안 요구를 충족할지라도 그것들을 자유롭게 조합할 권한을 해커에게 내준 것이라서 그 조합이 어떤 문제를 일으킬지 확신할 수 없음 버그가 존재할 경우 Mobile App은 각종 앱스토어 인증 절차에 걸리는 시간과 사용자들이 자발적으로 앱을 업데이트 할 때 까지의 시간동안 버그 해결이 불가능해짐 클라이언트 개발자에게 전가하기가 불가능 경우가 있거나 혹은 위에서 말한 문제들 떄문에 이 Facade 구현을 서버측에 전가하면 퍼사드의 역할을 결국은 누가 됐든 특정 마이크로서비스에서 처리해야한다. 그때도 문제가 심각해진다. AFG 문제점 : 각 마이크로서비스에 전가 특정 서비스 개발자들은 알지도 못하는 로직 구현 책임을 맡게되고 해당 서비스는 본 서비스가 아닌 다른 역할을 맡음으로써 불필요하게 복잡도 증가와 타 서비스들과의 결합도 증가 본 역할도 아닌 타 API Network 호출 증가로 인해 정작 본 서비스의 처리에 장애 유발 요인 증가 특정 프리젠테이션 계층 API Gateway에서 문제가 생겼다면 해당 API Gateway만 문제가 되지만 비즈니스 계층 Microservice에서 장애가 발생하면 이를 호출하는 다른 모든 계층에서 문제가 될 수 있음 - 장애 여파 훨씬 커지게 됨 이런식으로 전혀 엉뚱한 역할을 전가받은 서비스의 개발자들의 시선은 복잡해 개발만족도는 갈수록 떨어지게 된다. 그리고 애초에 마이크로 서비스로 나눈 이유 자체에도 반하게 되는 결정이 된다. 마이크로 서비스 패턴 책을 보면 API Gateway Pattern을 구현하는 방법을 스프링 웹 MVC, Webflux처럼 직접 웹플럭스처럼 직접 웹프레임워크로 개발하기와 스프링클라우드 게이트웨이처럼 라우팅과 직접 구현을 동시에 지원하는 API Gateway Framework를 사용하는 방법을 추천하고 있다. 실제 예제로도 스프링클라우드 게이트웨이를 사용한다. 즉 일부는 단순 라우팅, 나머지는 직접 구현하는 것을 섞어서 하고 있다. 하지만 필자는 앞서 API Gateway Framework를 통해 비록 API Gateway Pattern이 구현가능하고 일부 단순한 api는 간편하게 라우팅 처리를 할 수 있다 하더라도 그렇게 해서는 안된다고 말했다.이제 그 이유에 대해 다뤄보겠다. AFG 문제점 : Client / Business 계층간 강결합이제부터 Business 계층 개발자는 항상 Client의 변경사항을 주시해야 한다. 1234567{ &quot;userId&quot; : 1, &quot;name&quot; : &quot;강백호&quot;, &quot;address&quot; : &quot;서울시 동작구 ...&quot;, &quot;phone&quot; : &quot;010-1111-2222&quot;, &quot;ssn&quot; : &quot;221212-1234567&quot;} 배달원 앱용 Presentation AFG에서 User Microservice의 /users/{userId} API를 단순 라우팅 했고 그 응답 객체도 단순 반환했는데 나중에 User Microservice의 개발자가 별 생각없이 /users/{userId}의 응답에 고객의 전화번호와 주민번호를 추가한다면? -&gt; 배달원 앱을 통해 고객의 전화번호와 주민등록번호가 그대로 노출 이를 막으려면 User Microservice 측에서는 모든 Presentation 계층별로 서로 다른 사용자 정보 조회 API를 만드는 방법 밖에 없게 됨 Business 계층이 Presentation 계층에 의해서 계속 변화하게 됨 단순 요청/응답 라우팅은 어떠한 경우에도 사용하지 말고 항상 응답 내용을 프리젠테이션 게층에서 Client에 필요한 것만 정제해서 내보내기 배달앱에서는 어차피 새로 추가된 데이터를 보여주지 않을테니까 괜찮다는 식의 안심은 금물인 것은 당연히 알 것이다. 보다시피 게이트웨이 라우팅 패턴이 스며드는 순간 비즈니스 계층은 프리젠테이션 계층의 속박에서 벗어날 길이 없게 된다. AGF 문제점 : client 성능 저하Client는 느리다 여러 api 호출을 조합해야 하므로 network 호출이 여러번 일어나게 되고 api 호출 결과가 불필요한 데이터까지 포함하여 과하게 크거나 필요한 데이터가 없어서 추가 api 호출을 하게 됨 API Gateway Pattern을 Non Blocking IO / Reactive 하게 구현하여 성능 향상 추천 (Spring WebFlux 등) 이 그림은 넷플릭스 블로그에 나온 API 게이트웨이 예이다. 여기서 넷플릭스 api가 API GW라고 보면 되고 API GW의 서버가 다른 비즈니스 계층 서비스를 패러럴하게 호출하고 있다. 클라이언트는 단 한 번만 호출하고 있다. GraphQL로 이 문제를 해소 가능하지만, 비즈니스 계층 Micfoservice에 GraphQL을 구축하고 이를 그대로 프리젠테이션 계층까지 노출하면 앞서 말했던 보안 문제들이 그대로 발생함. GraphQL도 API Gateway Pattern에 따라 프리젠테이션 계층 서버 애플리케이션으로 적절한 보안 처리를 해서 별도 구축 AGF 문제점 : 내부 서비스의 프로토콜 제약 API Gateway Framework로 프리젠테이션 계층을 구현한다는 것은 곧 내부 비즈니스 계층 서비스가 모두 HTTP API(REST?)로 고정된다는 의미 내부 비즈니스 계층을 한 번 밖으로 노출하면 프로토콜 변경은 어려워짐 내부적으로 성능 향상과 다양한 요구 사항만족을 위해 gRPC, Message Queue, HTTP API, 기타 등등을 사용할 수 있는 자유도를 포기하지 말기 (출처 : https://microservices.io/patterns/apigateway.html) 계층형 아키텍처의 일반적인 원칙 무시 사례 계층형 아키텍처의 일반적인 원칙을 무시한 사례 중 마이크로 서비스를 나눌 때 비즈니스 계층으로 나눈게 아니라 애초에 프리젠테이션 계층으로 만들어 버리는 사례이다.예를 들어 주문 마이크로 서비스를 만드는데 애초에 앱클라이언트의 요청을 받게 만들어 버리는 것이다.인증도 붙이고 권한처리도 한다. 그리고 내부적으로 비즈니스 계층에서 비즈니스 로직을 처리하고 저장소 처리까지 합한다.사실 여기는까지는 괜찮다. 다만 많은 경우에 바로 이런 도식과 같은 형태를 띄게 버리는 것이다. 프리젠테이션 계층 api가 다른 프리젠테이션 계층 api를 호출하고 비즈니스 계층 api가 없다보니 프리젠테이션은 계층의 비즈니스 처리용 api를 만들어서 호출해버리는 것이다.이는 모놀리식 아키텍처에서 컨트롤러가 컨트롤러를 호출하고 서비스가 컨트롤러를 호출하는 형국이다.비즈니스 계층에 속하는 상품 서비스가 주문 프리젠테이션 계층을 호출하려면 필연적으로 주문 프리젠테이션 계층에서 인증을 풀어주거나 단순화한 형태의 api를 제공해줘야 한다.이는 IDOR의 수직적 권한 상승에 취약해지는 결과를 낳는다.Private 망의 상품 서비스가 Public 망의 api를 호출하려면 외부망으로 접속이 연결되면서 연결 지연이 발생한다.웹 방화벽에 의해 Private 망의 모든 호출이 단일 ip에서 오는 DDOS 공격으로 간주되어 차단될 수도 있다. 수직적 권한 상승에 취약해진다는것은 잘못하면 해커가 무제한의 권한을 가진 api를 알아낼 수 있다는 의미이다.계층형 아키텍처의 기본을 어김으로써 나타나는 문제들을 차치하고서라도 보안 문제와 네트워크 인프라적으로도 심각한 문제가 발생하는 것을 볼 수 있다. 하나의 메소드, 하나의 역할만 하듯 하나의 마이크로 서비스도 하나의 계층 역할만 해야한다.의존 관계는 항상 프리젠테이션 계층에서 비즈니스 계층으로 흐르거나 비즈니스 계층간에만 이루어져야 한다.비즈니스 계층간의 호출에 있어서도 Circular 호출이 일어나서는 안 됨 (A -&gt; B -&gt; A) 마이크로 서비스로 분할된 애플리케이션들간의 호출에 있어서도 우리가 보편적으로 코드를 짤 때 지켜야 하는 규칙을 철저히 지켜야한다. AGF를 횡단 관심사에서도 사용하지 말기진짜 진짜 필요한게 아니라면… Single Point Of Failure(단일 장애 지점)가 된다. 불필요하게 서버 관리 부담이 늘어나고 비용도 증가 Client가 하나의 end point만 바라본다고 해서 좋아질 것은 크게 없음 API Gateway pattern을 구현하게 되면 실제 client 개발자가 바라보는 API endpoint 개수는 현저하게 줄어들게 됨. ktNaviService.메소드수천개() : 정말 좋은가? 진짜 중요한 것은 필요한 API를 빠르게 찾아볼 수 있게 문서들을 잘 모아두기 API Gateway Framework를 사용할 만한 경우사실 대부분의 규모의 서비스에서 API Gateway Framework가 별로 필요 없다. 출처 : Announcing Zuul: Edge Service in the Cloud 이 그림은 넷플릭스가 2014년 Zuul API Gateway Framework를 발표한 내용을 가져온것이다.넷플릭스는 여기서 프리젠테이션 계층 API Gateway에 해당하는 부분을 클라이언트 어댑터라고 부르고 있으며 코드를 직접 짜서 구현하고 있다.넷플릭스는 Zuul을 비즈니스 계층 api를 프리젠테이션 계층으로 노출하는 용도로 사용한 것이 아니라 명백하게 각각의 프리젠테이션 계층에 대해서 횡단 관심사 처리를 위해 사용한 것이다.여기서 다양한 횡단 관심사를 보여주는데 흔히 생각하듯 인증 정적응답 처리 Canary testing, Stress testing, 동적 라우티을 위해서 사용하고 있다.만들 Micro Service에서 프리젠테이션 계층이 저정도 용도가 필요하다면 사용해도 좋을 것 같다. 참고로 넷플릭스는 위의 도식에 해당하는 아키텍처를 버린듯한 내용의 포스팅이 최근에 올라와있다.전반적으로 프리젠테이션 계층을 GraphQL로 전환하고 있는 것으로 보인다.자세한 것은 아래 참조 문서를 참고바란다. How Netflix Scales its API with GraphQL Federation (Part 1) 비즈니스 계층 횡단 관심사 처리 위 그림은 비즈니스 계층에 속하는 마이크로 서비스 API 서버에 API Gateway Framework를 적용한 경우로 이를 호출하는 프리젠테이션 계층 API Gateway들이 꼭 필요로 하는 API들만을 호출 가능하게 제약하고, 호출에 대한 로그를 남기는 API Gateway Framework를 두었다. 사실 실제로 이렇게까지 사용하는 경우는 드물다. 이것을 다시 모놀리식으로 비유하자면 서비스 코드에다가 AOP로, 어느 컨트롤러는 이 서비스 클래스의 메소드를 호출할 수 있고 어느 클래스는 호출 못하고 이런 것을 설정을 안할 것이다. 그래서 이런 API Gateway Framework도 별로 필요하지 않다고 말하는 것이다. 하지만 그래도 특정 프리젠테이션 계층 API Gateway에 대해 늘 철저히 보안 처리를 하고 싶다면 사용해도 좋을 것 같다. 여기서도 보다시피 절대로 API Gateay Framework는 비즈니스 계층을 프리젠테이션 계층으로 격상하는 용도로 사용한 것이 아니다. AOP처럼 횡단 관심사 처리를 위해 사용되었을 뿐이다. 여기서 내가 생각하거나 혹은 이미 마이크로서비스 패턴 책 등에 나와 있는 API Gateway Framework의 문제점들을 다양하게 제시했다. 마무리로 첨언하자면 여기서 다룬 모든 문제들이 모두 해결 가능하거나, 그런 문제들에도 불구하고 이를 사용해서 얻는 이득이 훨씬 큰 경우가 있다면 당연히 API Gateway Framework를 사용해도 괜찮을 것이다.","link":"/2024/04/14/API-Gateway/"},{"title":"도쿄 여행을 위한 사전조사 1편","text":"도쿄는 런던, 뉴욕과 함께 세계 3대 도시에 속한다고들 한다.또한 일본의 정치, 경제, 사회, 문화 등 모든 면에서 일본을 대표하는 최대 규모의 도시이다.과거와 현재가 공존하는 도쿄에서는 천년의 역사를 지닌 사찰부터 최신식 고층 빌딩까지 다채롭게 펼쳐지는 건축물들을 보는 즐거움이 있고 세련된 마천루 사이사이를 거닐거나 트렌디한 감성의 캐주얼한 골목에서 최신 유행 제품들을 둘러보는 재미도 있다고 한다. 매년 미쉐린 가이드 세스토랑이 가장 많이 선정되는 도시 중 하나로 저렴하면서도 맛있는 식당, 대를 이어 영업하는 노포부터 최고급 요리 전문점까지 아무리 까다로운 미식가라도 만족시킬 수 있는 다양한 옵션들이 존재하는 곳이다. 어쩌고 저쩌고 암튼 도쿄 여행에 대한 사전 조사를 시작하겠다. 첫번째로 도쿄역, 긴자, 신바시, 롯폰기에 대해 알아보겠다. 출처 : https://www.youtube.com/watch?v=ZzqN8lkNQ-I 도쿄 들어가기인천 - 나리타 소요시간은 약 2시간 20분이다.나리타에서 도쿄 도심까지는 최소 1시간이상 걸린다. 나리타에서는 케이세이 나리타 스카이 엑세스, 나리타 익스프레스, 스카이 라이너 등의 옵션이 있다.또한 최근 저가 리무진 버스도 노선을 확대하고 있다. 시내 교통시내 교통의 경우 이동하는 지역이 많은 여행계획을 가지고 있는 경우 도쿄 메트로와 은행잎 모양의 토에이선 등 13개 노선, 250여개 정류장을 무제한 이용할 수 있는 도쿄 메트로 패스 24/48/72 시간권이 가장 좋다고 한다.만약 핵심지역 몇 군데만 다닐것이다 하는 경우 도쿄, 시부야, 신주쿠 이케부쿠로, 아키하바라 등 핵심 관광명소를 지나는 JR 야카노테센을 이용하면 된다. JR 메트로 패스로는 이용이 불가한 점을 유념해야 한다. 마루노우치 니혼바시일왕이 현재 거주하는 도쿄 고쿄, 100년이 넘는 역사를 지닌 도쿄역이 위치한 마루노우치 지역일대는 관광지로서 엄청난 볼거리가 있다기보다는 도쿄의 심장과 같은 곳을 의미가 크다고 한다. 도쿄역을 중심으로 서쪽 마루노우치 방향 출구와 동쪽 야에스 방향 출구가 나뉘어 진다.마루토우치 방향으로 가면 고쿄(황거)를, 야에스 방향으로 가면 백화점거리인 주오도리와 니혼바시를 만날 수 있다.모두 도보로 이동할 수 있는 거리이다. 도쿄역 도쿄역 서편의 건물들 사이 마루노우치 광장은 생각보다 세련되고 예쁜 뷰를 자랑한다고 한다. 도쿄역 옆에 위치한 킷테 쇼핑몰 6층에서 조망 가능 중앙 우체국 건물을 리모델링하여 조성한 복합 쇼핑몰 구 우체국장실인 4층 레터룸 6층 킷테 가든 독특한 스팟 나카도오리 에비뉴마루노우치 광장에서 마루노우치 빌딩을 지나면 나카도오리 에비뉴가 나온다. 이곳은 마루노우치의 가로수길로 거리 양 옆의 초록 가로수길들과 다양한 브랜드 숍, 레스토랑들이 모여있다고 한다.천천히 산책하듯 걸으며 쇼핑하고 거리에서 커피 한 잔 즐기기 좋을것 같다. 고쿄내부 입장은 무료지만 반드시 투어를 통해 들어가야하고, 입장 인원 제한이 있다.그렇기에 인터넷으로 미리 예약하고 가는것이 권장된다. 일제강점기 우리나라 의사들의 의거지로도 유명한 니주바시 다리. 니주바시 다리와 이중교는 같은곳이다. 니혼바시 도쿄역에서 야에스 방면으로 나가면 다이마루 백화점을 시작으로 과거 에도시대에 상업의 중심지로 번창했던 니혼바시지역이 나온다. 이곳의 지명이 유래된 다리, 니혼바시를 기점으로 남북으로 뻗은거리를 주오거리라고 부른다.다이마루, 다카시마야, 미츠코시처럼 전통있는 박화점들이 줄지어 있다. 미츠코시 백화점은 외관도 멋지고 내부도 멋지다고 한다. 긴자 : 도쿄의 세련미 상징여전히 명실상부 도쿄 최고의 부촌 가운데 하나이다. 간자역 중심으로 펼쳐진 다양한 레스토랑, 카페, 상점들을 비롯해, 시계탑으로 유명한 와코백화점, 긴자식스, 도큐 플라자 긴자, 도쿄 미드타운 히비야 등이 볼만하다. 와코 백화점2차 세계대전 공습에도 살아남은 건물로, 역사가 느껴지는 우아함을 간직한 곳.이곳을 기점으로 펼쳐지는 사거리에서, 화려한 긴자 거리 전체를 조망할 수도 있다. 길을 건너 두 블럭 정도 걸어가면 긴자에서 가장 큰 쇼핑몰인 긴자식스가 있다. 긴자식스패션위주의 쇼핑몰답게 감각적인 내부 디자인이 돋보이는 건물이다. 인테리어 그 자체가 하나의 전시물을 감상하는 것처럼 느껴질정도로 보는 즐거움이 있고, 다양한 맛집과 함께 6층에는 츠타야 서점과 스타벅스가 있다. 도큐 플라자 긴자 긴자식스와 함께 긴자의 대표 쇼핑몰로 꼽히는 도큐 플라자 긴자는 컷팅유리로 이루어진 독특한 건물 외관을 지니고 있다. 내부는 알록달록한 차 매장(TWG)과 눈을 사로잡는 톡톡 튀는 패션 아이템들을 만나볼 수 있다. 도쿄 미드타운 히비야 2018년에 지어진 비교적 최신 쇼핑몰이지만생각보다 규모는 그리 크지 않다. 패션보단 맛집이 많은 것으로 유명하다.날씨가 좋을때는 쇼핑몰 앞 광장에서 문화 행사를 진행한다고 한다. 신바시신바시 부근 도심지역은 신주쿠, 시부야, 롯폰기에 등에 비해서는 여행자들 사이에서 인지도가 떨어지는 편이라고한다. 관광지로서 북적이는 곳이 아닌, 현지인들로 붐비는 곳에 가보고 싶다면 이곳을 추천한다. 시오도메 시오도메는 과거 화물역 부지를 재개발한 곳으로 현재 유수의 대기업과 미디어 방송사 등이 다수 입주해있는 활기찬 직장인 거리이다. 빌딩들이 밀집해있는 지역을 시오도메 시오사이트라고 부르는데, 사이트 내 빌딩을 연결하는 고가 육교를 걷다보면 뜻밖의 아름다운 전망을 감상할수도 있다. 미야자키 하야오의 시계 시오도메의 상징과도 같은 닛폰 텔레비전 건물의 커다란 시계인 미야자키 하야오의 시계도 볼만하다.세계에서 가장 큰 태엽 시계라고도 한다. 카렛타 시오도메근처의 카렛타 시오도메 쇼핑몰에서는 일본 내에서도 손꼽히는 일루미네이션을 볼 수 있고, 46층에는 무료 전망대가 있어서 야경을 감상하기에 좋다. 신바시시오도메와 인접한 신바시지역은 일본 현지인 샐러리맨들이 많은 지역이다. 신바시역에 내리면 서쪽 출구를 따라 1940년대 증기 기관차가 있는 신바시 만남의 장소, 니시구치 광장 거리가 있다. 광장에서 아래쪽으로 신바시 역사 주변, 교각 아래 다양한 식당과 이자카야 등이 줄지어 있는 거리가 있다. 이곳은 퇴근시간 이후 근처 직장인들이 회식이나 퇴근 후 스트레스를 풀며 한 잔할 장소로 많이 찾는다. 굴다리 아래 종기종기 모여있는 가게들이 독특한 풍경을 자아내는데, 일본인들의 일상을 엿볼 수 있는 일본스러운 밤거리 풍경을 볼 수 있다. 토라노몬 힐즈신바시에서 도보로 15분 거리에는 토라노몬 지역이 있는데, 대규모 복합시설인 토라노몬 힐즈가 있다. 토라노몬힐즈는 몰리타워를 중심으로 비즈니스 타워, 레지덴셜 타워, 스테이션 타워가 차례로 오픈하며 총 4개 타워가 있다. 도시속의 도시라는 지향점에 걸맞게 미래 도시에 와있는 듯한 느낌이 들게하는 곳이다. 여기서 남쪽으로 15분거리에는 도쿄타워가 있다. 도쿄타워도쿄 타워가 위치한 시바코엔의 탁 트인 잔디에서 타워를 볼 수 있다. 그 옆의 조죠d지 절을 배경으로 타워가 우뚝 선 모습도 멋지다. 오렌지색으로 반짝이는 타워와 야경이 어우러진 모습을 보려면 어두워진 후 전망대나 타워 근처로 이동하는게 좋지만, 낮 시간대에 시바공원, 조죠지 절과 함께 즐기는 것도 좋다. 롯폰기 아자부주반도쿄를 대표하는 번화가이자 긴자 못지않게 고급 상권가가 형성되어 있지만, 분위기를 비교하자면 좀 더 트렌디하고 힙한 느낌이 강한곳이다. 크게 롯폰기역 북쪽의 도쿄 미드타운 롯폰기, 남쪽의 롯폰기 힐스, 그리고 더 아래로 대사관들이 많아 한남동과 비슷한 분위기의 아자부주반이 있다. 롯폰기 힐스롯폰기는 지금의 롯폰기를 있게한 곳이다.20년 전 탄생했을 당시 기존에 보지 못했던 성격의 대규모 복합타운이었고, 그러다보니 등장 자체로 화제였다고 한다.쇼핑몰뿐만 아니라 호텔, 영화관, 방송국, 일반 주거지 등 무려 10여개의 건물이 모여 이루어져있고, 모리타워 전망대는 도쿄타워가 가장 아름답게 보이는 곳으로 도쿄 최고의 전망대 중 하나로 손꼽힌다. 롯폰기 힐즈의 성공으로 제2의 롯폰기 힐즈들이 연이어 생겨나게 되었는데, 그 중 하나가 롯폰기 랜드마크의 양대산맥인 도쿄 미드타운 롯폰기이다. 총 6개의 건물로 이루어진 미드타운은 호텔, 오피스, 미술관, 쇼핑센터, 주거공간 등이 부지의 40%를 차지하는 녹지와 어우러져 도심 한가운데 있으면서도 뭔가 힐링이 되는 휴식공간처럼 느껴진다. 출처 : https://m.blog.naver.com/ksm11015/222958660955 유리벽을 통해 자연 채광과 외부 조망이 가능한 가든테라스 식당가, 유리지붕과 나무로 이루어진 중앙광장 등이 대표적이고, 대나무숲으로 자연 친화적이고 따뜻한 느낌을 주는 인기 쇼핑몰 갤러리아와 산토리 미술관도 둘러보면 좋다. 이 롯폰기 일대에는 고급 맛집에서부터 전통있는 카페, 기념품 전문점, 캐주얼한 선술집, 그리고 가볍게 즐기는 베이커리나 브런치 식당에 이르기까지 다양한 종류의 가게들이 있으니 여유가 있다면 천천히 돌아보아도 좋다. 아자부주반아자부주반은 롯폰기와 도쿄타워 사이의 한적한 거리이자 고급주택가로 주변에 대사관들도 많아 외교관, 외국인들에게 인기 있는 지역이기도 하다. 롯폰기와 인접해있지만 화려함보다는 조용한 분위기이고 에도시대부터 번창해온 아자부주반 상점가를 비롯해 전통적인 상점, 국제적이며 고급스러운 카페, 레스토랑등이 구석구석 숨어있다. 사라시나호라이 - 냉소바 위치","link":"/2024/05/05/Tokyo-Travel-00/"},{"title":"GPU 서버에 도커 이미지로 JupyterLab 배포하기","text":"","link":"/2024/05/09/Deploy-JupyterLab-on-Gpu/"}],"tags":[{"name":"CLI","slug":"CLI","link":"/tags/CLI/"},{"name":"MachineLearning","slug":"MachineLearning","link":"/tags/MachineLearning/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"DeepLearning","slug":"DeepLearning","link":"/tags/DeepLearning/"},{"name":"Backend","slug":"Backend","link":"/tags/Backend/"},{"name":"Travel","slug":"Travel","link":"/tags/Travel/"},{"name":"Docker","slug":"Docker","link":"/tags/Docker/"},{"name":"GPU","slug":"GPU","link":"/tags/GPU/"},{"name":"JupyterLab","slug":"JupyterLab","link":"/tags/JupyterLab/"}],"categories":[{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"CLI","slug":"Linux/CLI","link":"/categories/Linux/CLI/"},{"name":"AI","slug":"AI","link":"/categories/AI/"},{"name":"MachineLearning","slug":"AI/MachineLearning","link":"/categories/AI/MachineLearning/"},{"name":"NLP","slug":"AI/NLP","link":"/categories/AI/NLP/"},{"name":"DeepLearning","slug":"AI/DeepLearning","link":"/categories/AI/DeepLearning/"},{"name":"Backend","slug":"Backend","link":"/categories/Backend/"},{"name":"AWS","slug":"Backend/AWS","link":"/categories/Backend/AWS/"},{"name":"Travel","slug":"Travel","link":"/categories/Travel/"},{"name":"Japan","slug":"Travel/Japan","link":"/categories/Travel/Japan/"},{"name":"JupyterLab","slug":"AI/JupyterLab","link":"/categories/AI/JupyterLab/"}],"pages":[]}